{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":748156,"sourceType":"datasetVersion","datasetId":239},{"sourceId":9465681,"sourceType":"datasetVersion","datasetId":5755469},{"sourceId":9465689,"sourceType":"datasetVersion","datasetId":5755473},{"sourceId":9465772,"sourceType":"datasetVersion","datasetId":5755541}],"dockerImageVersionId":30762,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-09-23T20:23:53.545892Z","iopub.execute_input":"2024-09-23T20:23:53.546343Z","iopub.status.idle":"2024-09-23T20:23:54.059738Z","shell.execute_reply.started":"2024-09-23T20:23:53.546305Z","shell.execute_reply":"2024-09-23T20:23:54.058674Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"/kaggle/input/3d-mnist/voxelgrid.py\n/kaggle/input/3d-mnist/train_point_clouds.h5\n/kaggle/input/3d-mnist/test_point_clouds.h5\n/kaggle/input/3d-mnist/plot3D.py\n/kaggle/input/3d-mnist/full_dataset_vectors.h5\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport torchvision.models","metadata":{"execution":{"iopub.status.busy":"2024-09-23T21:08:06.541703Z","iopub.execute_input":"2024-09-23T21:08:06.542676Z","iopub.status.idle":"2024-09-23T21:08:06.547202Z","shell.execute_reply.started":"2024-09-23T21:08:06.542573Z","shell.execute_reply":"2024-09-23T21:08:06.546076Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"class Encoder(torch.nn.Module):\n    def __init__(self):\n        super(Encoder, self).__init__()\n        \n        \n        # Layer Definition\n        vgg16_bn = torchvision.models.vgg16_bn(pretrained=True)\n        self.vgg = torch.nn.Sequential(*list(vgg16_bn.features.children()))[:27]\n        self.layer1 = torch.nn.Sequential(\n            torch.nn.Conv2d(512, 512, kernel_size=1),\n            torch.nn.BatchNorm2d(512),\n            torch.nn.ELU(),\n        )\n        self.layer2 = torch.nn.Sequential(\n            torch.nn.Conv2d(512, 256, kernel_size=3),\n            torch.nn.BatchNorm2d(256),\n            torch.nn.ELU(),\n            torch.nn.MaxPool2d(kernel_size=3)\n        )\n        self.layer3 = torch.nn.Sequential(\n            torch.nn.Conv2d(256, 128, kernel_size=3),\n            torch.nn.BatchNorm2d(128),\n            torch.nn.ELU()\n        )\n\n        # Don't update params in VGG16\n        for param in vgg16_bn.parameters():\n            param.requires_grad = False\n\n    def forward(self, rendering_images):\n        # print(rendering_images.size())  # torch.Size([batch_size, n_views, img_c, img_h, img_w])\n        rendering_images = rendering_images.permute(1, 0, 2, 3, 4).contiguous()\n        rendering_images = torch.split(rendering_images, 1, dim=0)\n        image_features = []\n\n        for img in rendering_images:\n# torch.Size([1, 512, 28, 28])\n# torch.Size([1, 512, 28, 28])\n# torch.Size([1, 256, 8, 8])\n# torch.Size([1, 128, 6, 6])\n# torch.Size([1, 1, 128, 6, 6])\n\n# torch.Size([1, 512, 28, 28])\n# torch.Size([1, 512, 26, 26])\n# torch.Size([1, 256, 8, 8])\n# torch.Size([1, 128, 8, 8])\n# torch.Size([1, 1, 128, 8, 8])\n            features = self.vgg(img.squeeze(dim=0))\n            print(features.size())    # torch.Size([batch_size, 512, 28, 28])\n            features = self.layer1(features)\n            print(features.size())    # torch.Size([batch_size, 512, 26, 26])\n            features = self.layer2(features)\n            print(features.size())    # torch.Size([batch_size, 512, 24, 24])\n            features = self.layer3(features)\n            print(features.size())    # torch.Size([batch_size, 256, 8, 8])\n            image_features.append(features)\n\n        image_features = torch.stack(image_features).permute(1, 0, 2, 3, 4).contiguous()\n        print(image_features.size())  # torch.Size([batch_size, n_views, 256, 8, 8])\n        return image_features","metadata":{"execution":{"iopub.status.busy":"2024-09-23T21:39:39.748297Z","iopub.execute_input":"2024-09-23T21:39:39.749407Z","iopub.status.idle":"2024-09-23T21:39:39.768489Z","shell.execute_reply.started":"2024-09-23T21:39:39.749342Z","shell.execute_reply":"2024-09-23T21:39:39.767160Z"},"trusted":true},"execution_count":121,"outputs":[]},{"cell_type":"code","source":"rand=torch.rand(1,1,256).rehsape(2048,2,2,2)","metadata":{"execution":{"iopub.status.busy":"2024-09-23T21:39:08.288164Z","iopub.execute_input":"2024-09-23T21:39:08.288920Z","iopub.status.idle":"2024-09-23T21:39:08.335223Z","shell.execute_reply.started":"2024-09-23T21:39:08.288873Z","shell.execute_reply":"2024-09-23T21:39:08.333822Z"},"trusted":true},"execution_count":119,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","Cell \u001b[0;32mIn[119], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m rand\u001b[38;5;241m=\u001b[39m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrand\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrehsape\u001b[49m(\u001b[38;5;241m2048\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m2\u001b[39m)\n","\u001b[0;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'rehsape'"],"ename":"AttributeError","evalue":"'Tensor' object has no attribute 'rehsape'","output_type":"error"}]},{"cell_type":"code","source":"class Decoder(torch.nn.Module):\n    def __init__(self):\n        super(Decoder, self).__init__()\n        \n\n        # Layer Definition\n        self.layer1 = torch.nn.Sequential(\n            torch.nn.ConvTranspose3d(256, 128, kernel_size=4, stride=2, bias=False, padding=1),\n            torch.nn.BatchNorm3d(128),\n            torch.nn.ReLU()\n        )\n        self.layer2 = torch.nn.Sequential(\n            torch.nn.ConvTranspose3d(128, 64, kernel_size=4, stride=2, bias=False, padding=1),\n            torch.nn.BatchNorm3d(64),\n            torch.nn.ReLU()\n        )\n        self.layer3 = torch.nn.Sequential(\n            torch.nn.ConvTranspose3d(64, 32, kernel_size=4, stride=2, bias=False, padding=1),\n            torch.nn.BatchNorm3d(32),\n            torch.nn.ReLU()\n        )\n        self.layer4 = torch.nn.Sequential(\n            torch.nn.ConvTranspose3d(32, 8, kernel_size=4, stride=2, bias=False, padding=1),\n            torch.nn.BatchNorm3d(8),\n            torch.nn.ReLU()\n        )\n        self.layer5 = torch.nn.Sequential(\n            torch.nn.ConvTranspose3d(8, 1, kernel_size=1, bias=False),\n            torch.nn.Sigmoid()\n        )\n\n    def forward(self, image_features):\n        # [1, 1, 128, 6, 6]\n        image_features = image_features.permute(1, 0, 2, 3, 4).contiguous()\n        image_features = torch.split(image_features, 1, dim=0)\n        gen_volumes = []\n        raw_features = []\n\n        for features in image_features:\n\n#             gen_volume = features.view(-1, 256, 2, 2, 2)\n            gen_volume = features.view(-1, 128, 6, 6, 1)\n            # print(gen_volume.size())   # torch.Size([batch_size, 156, 2, 2, 2])\n#             gen_volume = self.layer1(gen_volume)\n            # print(gen_volume.size())   # torch.Size([batch_size, 128, 4, 4, 4])\n            gen_volume = self.layer2(gen_volume)\n            # print(gen_volume.size())   # torch.Size([batch_size, 64, 8, 8, 8])\n            gen_volume = self.layer3(gen_volume)\n            # print(gen_volume.size())   # torch.Size([batch_size, 32, 16, 16, 16])\n            gen_volume = self.layer4(gen_volume)\n            raw_feature = gen_volume\n            # print(gen_volume.size())   # torch.Size([batch_size, 8, 32, 32, 32])\n            gen_volume = self.layer5(gen_volume)\n            # print(gen_volume.size())   # torch.Size([batch_size, 1, 32, 32, 32])\n            raw_feature = torch.cat((raw_feature, gen_volume), dim=1)\n            # print(raw_feature.size())  # torch.Size([batch_size, 9, 32, 32, 32])\n\n            gen_volumes.append(torch.squeeze(gen_volume, dim=1))\n            raw_features.append(raw_feature)\n\n        gen_volumes = torch.stack(gen_volumes).permute(1, 0, 2, 3, 4).contiguous()\n        raw_features = torch.stack(raw_features).permute(1, 0, 2, 3, 4, 5).contiguous()\n        # print(gen_volumes.size())      # torch.Size([batch_size, n_views, 32, 32, 32])\n        # print(raw_features.size())     # torch.Size([batch_size, n_views, 9, 32, 32, 32])\n        return raw_features, gen_volumes","metadata":{"execution":{"iopub.status.busy":"2024-09-23T21:53:43.724080Z","iopub.execute_input":"2024-09-23T21:53:43.724832Z","iopub.status.idle":"2024-09-23T21:53:43.741169Z","shell.execute_reply.started":"2024-09-23T21:53:43.724788Z","shell.execute_reply":"2024-09-23T21:53:43.739925Z"},"trusted":true},"execution_count":153,"outputs":[]},{"cell_type":"code","source":"rand=torch.rand([1, 1, 128, 6, 6])\nimage_features = out1.permute(1, 0, 2, 3, 4).contiguous()\nimage_features = torch.split(image_features, 1, dim=0)","metadata":{"execution":{"iopub.status.busy":"2024-09-23T21:42:04.142611Z","iopub.execute_input":"2024-09-23T21:42:04.143632Z","iopub.status.idle":"2024-09-23T21:42:04.151248Z","shell.execute_reply.started":"2024-09-23T21:42:04.143561Z","shell.execute_reply":"2024-09-23T21:42:04.150106Z"},"trusted":true},"execution_count":136,"outputs":[]},{"cell_type":"code","source":"type(image_features[0])","metadata":{"execution":{"iopub.status.busy":"2024-09-23T21:43:04.551739Z","iopub.execute_input":"2024-09-23T21:43:04.552154Z","iopub.status.idle":"2024-09-23T21:43:04.559091Z","shell.execute_reply.started":"2024-09-23T21:43:04.552114Z","shell.execute_reply":"2024-09-23T21:43:04.558061Z"},"trusted":true},"execution_count":143,"outputs":[{"execution_count":143,"output_type":"execute_result","data":{"text/plain":"torch.Tensor"},"metadata":{}}]},{"cell_type":"code","source":"encoder=Encoder()","metadata":{"execution":{"iopub.status.busy":"2024-09-23T22:05:36.639699Z","iopub.execute_input":"2024-09-23T22:05:36.640762Z","iopub.status.idle":"2024-09-23T22:05:38.369665Z","shell.execute_reply.started":"2024-09-23T22:05:36.640720Z","shell.execute_reply":"2024-09-23T22:05:38.368637Z"},"trusted":true},"execution_count":190,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_BN_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_BN_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n","output_type":"stream"}]},{"cell_type":"code","source":"decoder=Decoder()","metadata":{"execution":{"iopub.status.busy":"2024-09-23T22:05:38.371550Z","iopub.execute_input":"2024-09-23T22:05:38.372164Z","iopub.status.idle":"2024-09-23T22:05:38.402034Z","shell.execute_reply.started":"2024-09-23T22:05:38.372113Z","shell.execute_reply":"2024-09-23T22:05:38.401030Z"},"trusted":true},"execution_count":191,"outputs":[]},{"cell_type":"code","source":"import torch.nn as nn","metadata":{"execution":{"iopub.status.busy":"2024-09-23T21:09:12.823697Z","iopub.execute_input":"2024-09-23T21:09:12.824701Z","iopub.status.idle":"2024-09-23T21:09:12.829003Z","shell.execute_reply.started":"2024-09-23T21:09:12.824656Z","shell.execute_reply":"2024-09-23T21:09:12.827906Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"code","source":"class model(nn.Module):\n    def __init__(self,encoder,decoder):\n        super(model,self).__init__()\n        self.encoder=encoder\n        self.decoder=decoder\n    def forward(self,x):\n        x=self.encoder(x)\n        # x = x.view(-1, 256, 2, 2, 2)\n        x=self.decoder(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2024-09-23T21:09:13.053991Z","iopub.execute_input":"2024-09-23T21:09:13.054421Z","iopub.status.idle":"2024-09-23T21:09:13.061209Z","shell.execute_reply.started":"2024-09-23T21:09:13.054381Z","shell.execute_reply":"2024-09-23T21:09:13.059978Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"code","source":"checkpoint = torch.load(\"/kaggle/input/qqqqqq/Pix2Vox-F-ShapeNet.pth\")","metadata":{"execution":{"iopub.status.busy":"2024-09-23T22:05:43.943716Z","iopub.execute_input":"2024-09-23T22:05:43.944125Z","iopub.status.idle":"2024-09-23T22:05:44.008030Z","shell.execute_reply.started":"2024-09-23T22:05:43.944085Z","shell.execute_reply":"2024-09-23T22:05:44.006944Z"},"trusted":true},"execution_count":192,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_36/4220080922.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  checkpoint = torch.load(\"/kaggle/input/qqqqqq/Pix2Vox-F-ShapeNet.pth\")\n","output_type":"stream"}]},{"cell_type":"code","source":"epoch_idx = checkpoint['epoch_idx']","metadata":{"execution":{"iopub.status.busy":"2024-09-23T22:05:45.447684Z","iopub.execute_input":"2024-09-23T22:05:45.448091Z","iopub.status.idle":"2024-09-23T22:05:45.453078Z","shell.execute_reply.started":"2024-09-23T22:05:45.448055Z","shell.execute_reply":"2024-09-23T22:05:45.451852Z"},"trusted":true},"execution_count":193,"outputs":[]},{"cell_type":"code","source":"epoch_idx","metadata":{"execution":{"iopub.status.busy":"2024-09-23T22:05:45.669429Z","iopub.execute_input":"2024-09-23T22:05:45.669858Z","iopub.status.idle":"2024-09-23T22:05:45.676574Z","shell.execute_reply.started":"2024-09-23T22:05:45.669820Z","shell.execute_reply":"2024-09-23T22:05:45.675383Z"},"trusted":true},"execution_count":194,"outputs":[{"execution_count":194,"output_type":"execute_result","data":{"text/plain":"245"},"metadata":{}}]},{"cell_type":"code","source":"checkpoint['encoder_state_dict']","metadata":{"execution":{"iopub.status.busy":"2024-09-23T22:05:47.575991Z","iopub.execute_input":"2024-09-23T22:05:47.576411Z","iopub.status.idle":"2024-09-23T22:05:48.059884Z","shell.execute_reply.started":"2024-09-23T22:05:47.576367Z","shell.execute_reply":"2024-09-23T22:05:48.058725Z"},"trusted":true},"execution_count":195,"outputs":[{"execution_count":195,"output_type":"execute_result","data":{"text/plain":"OrderedDict([('module.vgg.0.weight',\n              tensor([[[[-1.3644e-01,  3.4100e-01, -4.8989e-01],\n                        [-7.6665e-02, -1.4044e-01,  1.4004e-01],\n                        [-4.0409e-01, -9.2861e-02,  7.8583e-02]],\n              \n                       [[-7.9438e-03,  4.6198e-02,  3.2642e-02],\n                        [-4.0754e-01, -4.5644e-02,  3.5134e-01],\n                        [-3.5688e-01,  2.6731e-02,  4.0426e-01]],\n              \n                       [[ 4.8771e-01,  2.1508e-01,  9.1495e-02],\n                        [ 2.8036e-01,  1.2694e-05, -2.4137e-01],\n                        [-1.9255e-02, -2.5905e-01, -9.8193e-02]]],\n              \n              \n                      [[[-6.9555e-02,  2.7304e-01,  4.1057e-01],\n                        [-7.8063e-02,  2.1053e-01,  2.4030e-01],\n                        [ 1.0975e-01, -2.7798e-01, -2.7805e-01]],\n              \n                       [[-7.1334e-01, -2.3772e-01, -3.9419e-01],\n                        [-1.9547e-01, -4.4369e-01,  9.7205e-02],\n                        [-4.9029e-01,  1.1014e-02, -8.9601e-02]],\n              \n                       [[ 7.0656e-02,  8.2740e-02, -6.9733e-02],\n                        [-2.4344e-01, -8.9780e-02, -2.4779e-01],\n                        [ 3.0069e-01, -1.3540e-01, -5.1901e-01]]],\n              \n              \n                      [[[-1.2672e-01, -1.3800e-01,  1.9079e-01],\n                        [ 2.0210e-01,  3.0203e-01, -5.6077e-03],\n                        [-6.3418e-02,  4.2199e-01,  1.8142e-02]],\n              \n                       [[-4.3600e-02,  2.3689e-01, -1.2035e-01],\n                        [-1.8235e-01,  2.8724e-01,  1.7670e-01],\n                        [-3.9079e-02, -5.3853e-02, -1.1217e-01]],\n              \n                       [[ 1.6024e-02,  1.3120e-01,  7.1452e-01],\n                        [ 2.3056e-01,  1.3841e-01,  2.2749e-01],\n                        [ 4.8108e-01,  5.5310e-01,  4.7310e-01]]],\n              \n              \n                      ...,\n              \n              \n                      [[[ 8.9513e-02, -2.1044e-01, -1.1292e-01],\n                        [-6.3786e-02,  3.9343e-01,  5.3532e-02],\n                        [-4.5160e-01,  5.1821e-02, -5.6180e-01]],\n              \n                       [[ 1.1714e-02, -3.0484e-01, -7.9153e-01],\n                        [-3.5085e-02,  9.4474e-02, -4.5982e-01],\n                        [-2.3419e-01, -2.9578e-02,  3.0313e-01]],\n              \n                       [[ 1.3872e-01, -3.3703e-02,  3.3764e-01],\n                        [ 2.3167e-01, -6.9399e-02, -7.6151e-02],\n                        [-1.4422e-01,  7.8335e-02, -1.9294e-01]]],\n              \n              \n                      [[[-1.5594e-01,  1.0174e-01,  1.6202e-01],\n                        [ 2.5045e-02,  6.8542e-02,  8.1108e-01],\n                        [ 2.3025e-02,  4.9858e-01,  1.4432e-01]],\n              \n                       [[ 7.5972e-02,  1.5063e-01, -2.6877e-01],\n                        [-2.7091e-01, -9.7008e-02, -8.1762e-02],\n                        [-2.1911e-01, -2.1134e-01, -2.4082e-02]],\n              \n                       [[ 1.7332e-01, -2.0053e-01,  1.6433e-01],\n                        [-1.5653e-01,  6.6198e-02, -7.3986e-02],\n                        [-2.4074e-02, -9.0675e-02, -1.6024e-01]]],\n              \n              \n                      [[[ 4.1328e-01, -2.1797e-01,  7.4566e-01],\n                        [ 5.5988e-01, -3.5852e-01,  8.6298e-03],\n                        [ 1.3717e-01, -1.9249e-01, -4.2603e-01]],\n              \n                       [[ 2.9428e-01,  1.5714e-01, -1.1876e-01],\n                        [-4.7643e-02, -4.2174e-01,  7.3050e-02],\n                        [-2.5054e-02, -1.0151e-01, -1.4005e-01]],\n              \n                       [[-1.5071e-01, -2.5260e-01, -4.1183e-01],\n                        [ 8.4454e-02, -6.2269e-02, -1.0637e-01],\n                        [ 1.4208e-01, -1.2629e-01,  1.9943e-01]]]], device='cuda:0')),\n             ('module.vgg.0.bias',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n                     device='cuda:0')),\n             ('module.vgg.1.weight',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], device='cuda:0')),\n             ('module.vgg.1.bias',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n                     device='cuda:0')),\n             ('module.vgg.1.running_mean',\n              tensor([-0.1961, -1.8495,  2.6365,  0.0355,  2.6703, -0.9565,  1.1936,  1.2203,\n                      -1.2630,  1.0604,  0.2029, -1.7058, -0.0487, -0.8351, -0.3133,  1.0409,\n                       0.5179, -0.1218, -0.3036, -0.4786, -1.4005,  0.6226, -1.2251,  0.8588,\n                      -0.0093, -0.0361,  0.7051, -1.7617,  2.2963, -0.1488, -0.2551, -0.9512,\n                       1.8039, -0.7527, -0.0190,  0.2951,  1.5756, -1.3395, -0.5184,  1.8997,\n                      -0.5860, -0.4448,  0.2168, -1.6303, -1.4963, -0.3246,  0.8567,  0.4950,\n                       2.5111, -0.8180, -0.2632,  0.6181, -0.9647, -0.2650,  0.1950, -0.5555,\n                       1.5335, -1.5863,  1.3099,  0.6600, -0.4316, -1.3376,  0.3064, -0.2305],\n                     device='cuda:0')),\n             ('module.vgg.1.running_var',\n              tensor([0.0761, 2.8218, 5.4241, 0.0272, 5.6448, 0.7593, 1.1525, 1.3316, 1.2793,\n                      0.9254, 0.0674, 2.3456, 0.0898, 0.6240, 0.1420, 0.9270, 0.2849, 0.0646,\n                      0.1586, 0.2755, 1.5833, 0.3636, 1.2396, 0.6018, 0.0112, 0.0547, 0.3982,\n                      2.4529, 4.2303, 0.0913, 0.0921, 0.7099, 2.6125, 0.5101, 0.0363, 0.1339,\n                      2.0579, 1.4064, 0.2739, 2.8333, 0.2921, 0.1691, 0.0643, 2.1570, 1.7869,\n                      0.1188, 0.6302, 0.2514, 4.9341, 0.5782, 0.1289, 0.3961, 0.7613, 0.0883,\n                      0.1133, 0.3446, 1.8403, 1.9991, 1.3791, 0.3724, 0.1937, 1.4635, 0.1263,\n                      0.1197], device='cuda:0')),\n             ('module.vgg.1.num_batches_tracked',\n              tensor(942776, device='cuda:0')),\n             ('module.vgg.3.weight',\n              tensor([[[[-0.0459,  0.0220, -0.0840],\n                        [-0.1224, -0.0463, -0.0552],\n                        [-0.0744,  0.1014, -0.0213]],\n              \n                       [[-0.0506,  0.0136, -0.0101],\n                        [ 0.0590, -0.0374, -0.0105],\n                        [-0.0377, -0.0833,  0.1058]],\n              \n                       [[ 0.0798, -0.1320, -0.0736],\n                        [ 0.0691, -0.0052, -0.0827],\n                        [-0.0476,  0.0778, -0.0193]],\n              \n                       ...,\n              \n                       [[-0.0205,  0.0512,  0.0721],\n                        [-0.0851,  0.0627, -0.0082],\n                        [-0.0390, -0.0110,  0.0954]],\n              \n                       [[-0.0460,  0.1279,  0.0422],\n                        [ 0.1516, -0.0452, -0.0009],\n                        [-0.0388, -0.0280,  0.0566]],\n              \n                       [[-0.0451, -0.0381,  0.0360],\n                        [-0.0110, -0.0145, -0.0318],\n                        [-0.0209,  0.0495, -0.0680]]],\n              \n              \n                      [[[-0.0002,  0.0568, -0.0318],\n                        [ 0.0831, -0.0525, -0.0656],\n                        [-0.0292,  0.0684, -0.1255]],\n              \n                       [[ 0.0250,  0.0192, -0.0314],\n                        [-0.1237,  0.0515, -0.0042],\n                        [-0.0632, -0.0550,  0.0250]],\n              \n                       [[ 0.0178, -0.0951,  0.0378],\n                        [-0.0120, -0.0363, -0.1050],\n                        [ 0.0589, -0.0161,  0.0442]],\n              \n                       ...,\n              \n                       [[ 0.1234, -0.0587,  0.0118],\n                        [ 0.0163,  0.0194,  0.0781],\n                        [-0.0211,  0.0114,  0.0286]],\n              \n                       [[ 0.0981,  0.0278, -0.0071],\n                        [ 0.0433,  0.0241, -0.0128],\n                        [-0.0253, -0.0356,  0.0043]],\n              \n                       [[-0.0250, -0.1581,  0.0738],\n                        [-0.0300,  0.1253,  0.0551],\n                        [-0.1122, -0.0071, -0.0246]]],\n              \n              \n                      [[[ 0.0061, -0.0485, -0.0679],\n                        [-0.0477, -0.1576, -0.0223],\n                        [ 0.0045,  0.1038, -0.1438]],\n              \n                       [[ 0.0292, -0.0543,  0.0233],\n                        [ 0.0138, -0.0717,  0.0306],\n                        [-0.0265,  0.0254, -0.0576]],\n              \n                       [[-0.0688, -0.0185, -0.0262],\n                        [-0.0228,  0.0783, -0.0584],\n                        [-0.0157,  0.0154, -0.0430]],\n              \n                       ...,\n              \n                       [[-0.0522,  0.0062,  0.0044],\n                        [ 0.0125, -0.0162,  0.0202],\n                        [ 0.0311, -0.0672,  0.0312]],\n              \n                       [[-0.0577,  0.1220,  0.0685],\n                        [ 0.0122, -0.0095,  0.0520],\n                        [-0.1088,  0.0397,  0.0089]],\n              \n                       [[ 0.0349, -0.0355,  0.1536],\n                        [-0.0554,  0.0240,  0.0898],\n                        [-0.0761, -0.0319,  0.0878]]],\n              \n              \n                      ...,\n              \n              \n                      [[[ 0.0778,  0.0422, -0.0249],\n                        [ 0.0464,  0.0452, -0.0397],\n                        [ 0.1057, -0.1030,  0.0362]],\n              \n                       [[ 0.0858, -0.0154, -0.0381],\n                        [ 0.2059, -0.0792,  0.0007],\n                        [-0.0932,  0.0435,  0.0325]],\n              \n                       [[-0.0563,  0.0252, -0.1491],\n                        [ 0.0920,  0.0019,  0.0069],\n                        [ 0.0350,  0.0428,  0.1601]],\n              \n                       ...,\n              \n                       [[ 0.0211,  0.0020, -0.0999],\n                        [ 0.0698,  0.0263, -0.0642],\n                        [ 0.0462, -0.0058, -0.1338]],\n              \n                       [[-0.0891, -0.0587, -0.0429],\n                        [ 0.0083, -0.0212,  0.0023],\n                        [ 0.0466, -0.0137,  0.0722]],\n              \n                       [[ 0.0138,  0.1354, -0.0436],\n                        [-0.1071,  0.0372,  0.1122],\n                        [-0.0011,  0.0861,  0.0149]]],\n              \n              \n                      [[[ 0.0108, -0.0588, -0.0836],\n                        [-0.1129,  0.0397, -0.0168],\n                        [-0.0696, -0.0231, -0.0486]],\n              \n                       [[ 0.0461, -0.0343,  0.0618],\n                        [ 0.0074, -0.0378,  0.0466],\n                        [ 0.0162, -0.0284,  0.0492]],\n              \n                       [[ 0.0112,  0.0003, -0.0470],\n                        [ 0.0319, -0.0281,  0.0249],\n                        [ 0.0003,  0.0582,  0.0725]],\n              \n                       ...,\n              \n                       [[-0.0382,  0.0107, -0.0398],\n                        [-0.0445,  0.0313,  0.0758],\n                        [-0.0141, -0.0341, -0.0510]],\n              \n                       [[-0.0359, -0.0390, -0.0212],\n                        [-0.0760, -0.0134, -0.0372],\n                        [-0.0080, -0.1155, -0.0726]],\n              \n                       [[-0.0366,  0.0966,  0.0088],\n                        [-0.0162, -0.1064, -0.0092],\n                        [-0.1569, -0.0431,  0.0179]]],\n              \n              \n                      [[[ 0.0820, -0.0742, -0.0081],\n                        [-0.0317,  0.0308,  0.0063],\n                        [ 0.0856, -0.1693, -0.0940]],\n              \n                       [[-0.0030,  0.1050,  0.0393],\n                        [ 0.0082, -0.0686,  0.1100],\n                        [ 0.0098, -0.1029,  0.0941]],\n              \n                       [[-0.1065,  0.0760, -0.0826],\n                        [-0.0620,  0.1228,  0.0425],\n                        [ 0.0247,  0.0575,  0.0328]],\n              \n                       ...,\n              \n                       [[-0.1193, -0.0281,  0.0571],\n                        [-0.0799,  0.0378,  0.0150],\n                        [ 0.0491, -0.0250, -0.0667]],\n              \n                       [[ 0.0222,  0.0069,  0.0324],\n                        [-0.0350, -0.0044, -0.0132],\n                        [-0.0046, -0.0859, -0.0053]],\n              \n                       [[-0.0005, -0.0405,  0.0117],\n                        [-0.0113,  0.0840,  0.0029],\n                        [-0.0324, -0.0899, -0.0678]]]], device='cuda:0')),\n             ('module.vgg.3.bias',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n                     device='cuda:0')),\n             ('module.vgg.4.weight',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], device='cuda:0')),\n             ('module.vgg.4.bias',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n                     device='cuda:0')),\n             ('module.vgg.4.running_mean',\n              tensor([-0.8428, -0.2590,  0.9274,  0.0301, -0.6332, -0.6192,  0.3434,  0.6933,\n                      -0.4762, -1.7035,  0.0475,  1.1480, -0.6173, -0.7430,  0.3199, -0.0607,\n                       0.2620, -0.0610, -0.1068, -0.0566, -0.3874, -0.8614, -0.3454,  1.1494,\n                       1.3995, -0.5019,  0.6740, -0.4370, -0.2893,  0.2191,  0.6327,  0.4843,\n                       0.1865,  0.8520, -0.1706, -0.2574, -0.2636,  0.4357, -0.3992, -0.5518,\n                      -1.8464,  0.0806, -0.2188,  0.3769, -0.8499, -0.4775,  0.2436,  0.4235,\n                       0.2151,  0.5317,  1.1857,  0.0735,  0.7632, -1.5147, -0.0838, -0.7904,\n                       0.4508, -1.0099,  0.7301, -0.1018, -0.1429,  0.6199, -1.2676, -0.1173],\n                     device='cuda:0')),\n             ('module.vgg.4.running_var',\n              tensor([1.8102, 0.3731, 0.8294, 0.1768, 0.4733, 0.4515, 0.3223, 0.4466, 0.4107,\n                      2.2017, 0.3968, 1.4068, 0.7798, 0.4380, 0.3695, 1.4495, 0.2034, 0.2972,\n                      0.3087, 0.2587, 1.0425, 0.9772, 2.2295, 1.2437, 1.5790, 1.0170, 0.7968,\n                      0.2778, 0.4658, 0.6661, 0.3613, 0.8923, 0.6197, 1.5364, 0.4386, 0.6722,\n                      0.8060, 1.0372, 0.6219, 0.4382, 1.8614, 2.6090, 0.3057, 0.2865, 0.7866,\n                      0.9153, 0.9621, 0.4292, 1.7622, 1.9054, 1.3574, 0.7562, 0.4368, 2.4743,\n                      0.9079, 0.5907, 0.5170, 0.8098, 0.4489, 0.4847, 0.4894, 0.6455, 1.4696,\n                      0.3786], device='cuda:0')),\n             ('module.vgg.4.num_batches_tracked',\n              tensor(942776, device='cuda:0')),\n             ('module.vgg.7.weight',\n              tensor([[[[ 0.0807, -0.0618, -0.0662],\n                        [-0.0702,  0.0241, -0.0160],\n                        [ 0.0892,  0.0833, -0.0846]],\n              \n                       [[ 0.0309, -0.0722, -0.0297],\n                        [ 0.0340, -0.0499, -0.0284],\n                        [ 0.0747, -0.0116,  0.0462]],\n              \n                       [[ 0.0194,  0.0525,  0.0419],\n                        [ 0.0628, -0.0524,  0.0217],\n                        [-0.0463,  0.0283,  0.0130]],\n              \n                       ...,\n              \n                       [[-0.0109, -0.1052, -0.0671],\n                        [-0.0647, -0.1070, -0.1044],\n                        [ 0.0451, -0.0515, -0.1141]],\n              \n                       [[-0.0973,  0.0660,  0.0363],\n                        [ 0.0006,  0.0782,  0.0099],\n                        [ 0.0544,  0.0026, -0.0074]],\n              \n                       [[-0.0394, -0.0743,  0.0170],\n                        [-0.0775,  0.1438,  0.0981],\n                        [-0.0724, -0.0068, -0.0172]]],\n              \n              \n                      [[[-0.0617,  0.0260,  0.1131],\n                        [ 0.0417, -0.0926, -0.0974],\n                        [-0.0457, -0.0109,  0.0529]],\n              \n                       [[-0.0085, -0.0183,  0.0396],\n                        [ 0.0813, -0.0692,  0.0390],\n                        [ 0.0361, -0.0237, -0.0095]],\n              \n                       [[-0.0225,  0.0020, -0.0348],\n                        [ 0.0214,  0.0763, -0.0601],\n                        [ 0.0386,  0.0121, -0.0175]],\n              \n                       ...,\n              \n                       [[-0.0341,  0.0045, -0.0592],\n                        [ 0.0966,  0.0603, -0.0180],\n                        [-0.0654, -0.0573, -0.1129]],\n              \n                       [[-0.0330,  0.0429, -0.0077],\n                        [-0.0082,  0.0164, -0.0214],\n                        [-0.0303, -0.0092,  0.0556]],\n              \n                       [[ 0.0194,  0.0023,  0.0288],\n                        [ 0.0520, -0.0369, -0.0188],\n                        [ 0.0482, -0.0368,  0.0725]]],\n              \n              \n                      [[[-0.0235, -0.0893,  0.0045],\n                        [-0.0465, -0.1098,  0.0536],\n                        [ 0.0361,  0.0104, -0.1327]],\n              \n                       [[ 0.0491,  0.1404, -0.0346],\n                        [-0.0409,  0.0313,  0.0152],\n                        [ 0.0914, -0.0178, -0.1295]],\n              \n                       [[-0.0075, -0.0435,  0.0144],\n                        [ 0.0661,  0.0109,  0.0948],\n                        [ 0.0630, -0.0343, -0.1024]],\n              \n                       ...,\n              \n                       [[-0.0462,  0.0012,  0.0072],\n                        [ 0.0387,  0.0879, -0.0282],\n                        [ 0.0138, -0.0296,  0.0240]],\n              \n                       [[ 0.0626,  0.0066, -0.0100],\n                        [-0.0048,  0.0594,  0.0563],\n                        [-0.1378,  0.0372, -0.0576]],\n              \n                       [[ 0.0181, -0.0799,  0.0640],\n                        [ 0.0591, -0.0747, -0.0687],\n                        [-0.0313,  0.0491,  0.0310]]],\n              \n              \n                      ...,\n              \n              \n                      [[[-0.0052,  0.0446,  0.0070],\n                        [ 0.0023,  0.0017, -0.0391],\n                        [-0.0066,  0.0188, -0.0373]],\n              \n                       [[ 0.0490, -0.0516, -0.0387],\n                        [-0.0227, -0.0118, -0.0122],\n                        [ 0.0367, -0.0338,  0.0043]],\n              \n                       [[ 0.0030, -0.0327, -0.0482],\n                        [ 0.0977, -0.0813, -0.0960],\n                        [ 0.1202,  0.1200, -0.0487]],\n              \n                       ...,\n              \n                       [[-0.1322,  0.0369, -0.0215],\n                        [-0.0872,  0.0140, -0.0237],\n                        [-0.0181, -0.0703, -0.0816]],\n              \n                       [[ 0.0096,  0.0289,  0.1190],\n                        [ 0.0144, -0.0013, -0.0833],\n                        [ 0.0240,  0.0893,  0.0232]],\n              \n                       [[-0.0537,  0.0728, -0.0645],\n                        [-0.0312, -0.0591,  0.0510],\n                        [-0.0506, -0.0729, -0.0249]]],\n              \n              \n                      [[[-0.0377, -0.0269,  0.0314],\n                        [-0.0480, -0.0136, -0.0112],\n                        [ 0.0970,  0.0158,  0.0558]],\n              \n                       [[-0.0359,  0.0098, -0.0105],\n                        [-0.0070,  0.0328, -0.0843],\n                        [-0.0138, -0.0400, -0.0057]],\n              \n                       [[ 0.0288,  0.0180, -0.0432],\n                        [ 0.0360,  0.1591, -0.0229],\n                        [ 0.0455, -0.0376,  0.0431]],\n              \n                       ...,\n              \n                       [[-0.1250,  0.0416, -0.0931],\n                        [-0.0200, -0.0180,  0.0242],\n                        [-0.1053,  0.0381, -0.0018]],\n              \n                       [[ 0.0982, -0.1048, -0.0184],\n                        [-0.0822, -0.0518, -0.0648],\n                        [ 0.0091, -0.0880, -0.0645]],\n              \n                       [[-0.0640, -0.0226, -0.0166],\n                        [ 0.0277,  0.0240, -0.0437],\n                        [ 0.0085, -0.0053, -0.0143]]],\n              \n              \n                      [[[-0.0063, -0.0629,  0.0039],\n                        [ 0.0025, -0.0622,  0.0118],\n                        [ 0.0789,  0.0399, -0.0346]],\n              \n                       [[ 0.0186,  0.0498, -0.0137],\n                        [ 0.0479,  0.0314,  0.0272],\n                        [ 0.0490,  0.0083,  0.0818]],\n              \n                       [[ 0.0210,  0.1145,  0.0791],\n                        [-0.0094,  0.1011,  0.0251],\n                        [ 0.0505, -0.0226,  0.0057]],\n              \n                       ...,\n              \n                       [[ 0.0110, -0.0305, -0.0097],\n                        [-0.0196,  0.0069,  0.0231],\n                        [-0.0488,  0.0597,  0.0592]],\n              \n                       [[ 0.1146, -0.0203, -0.1039],\n                        [-0.0051, -0.0263,  0.0176],\n                        [ 0.0436, -0.1943,  0.0640]],\n              \n                       [[-0.0058, -0.0267, -0.0500],\n                        [ 0.0367, -0.0771,  0.0812],\n                        [-0.0480, -0.0171,  0.0404]]]], device='cuda:0')),\n             ('module.vgg.7.bias',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n             ('module.vgg.8.weight',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1.], device='cuda:0')),\n             ('module.vgg.8.bias',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n             ('module.vgg.8.running_mean',\n              tensor([-0.0228, -0.3881,  0.4867,  0.2238,  0.8836, -0.4751,  0.6521, -0.0509,\n                       0.2108, -0.4185,  0.0270, -0.0933, -0.5296,  0.5557, -0.3221,  0.2586,\n                      -1.0431, -0.1558,  0.4846, -0.3024, -0.4281,  0.4877, -0.3358,  0.0464,\n                       0.8269, -0.5193, -0.4784, -0.5712,  0.8613, -0.2401,  0.0234,  0.6332,\n                       0.0745,  0.1262, -0.0945,  0.9767, -0.7938, -1.1192,  0.3205, -0.0969,\n                       0.3568, -0.6606,  0.0989, -0.2394, -1.0781, -0.2658,  0.2387, -1.0363,\n                      -0.5587,  0.5899, -0.5468,  0.6451, -0.0482, -0.1310,  0.3972, -0.1533,\n                      -0.3205, -0.9030,  0.1331, -0.1895, -1.1365, -0.2818,  0.5582,  1.0320,\n                       0.0409,  0.4772, -0.9919,  1.0242,  0.1702,  0.0464,  1.1386, -0.1737,\n                       0.5543, -0.5667,  1.1393,  0.5846, -0.0938, -0.0420,  0.5049,  1.2640,\n                       0.5724,  0.0575,  0.8994, -0.9654,  0.9186, -0.3499,  0.1799, -0.1653,\n                       0.4119,  0.1556, -0.2349,  0.0333, -0.3955, -1.5471,  0.0829,  0.1159,\n                      -0.3253, -0.1250, -0.2781,  0.0020,  0.6914, -0.0206, -0.0114,  0.5292,\n                       0.7173,  0.5150,  0.0866,  0.8822, -0.3062, -1.1581, -0.2916, -1.2894,\n                       0.1591, -0.1292,  0.2860,  0.9528, -0.9812, -0.4526, -0.4314, -0.5235,\n                      -0.4354,  0.4750,  1.2186,  0.2632,  0.1942,  0.3781, -1.8092, -0.5341],\n                     device='cuda:0')),\n             ('module.vgg.8.running_var',\n              tensor([0.7305, 0.8113, 0.4429, 1.1703, 2.0048, 0.5407, 0.7640, 0.8200, 0.7719,\n                      0.9804, 0.5409, 0.7330, 0.8960, 1.0790, 0.8162, 1.2009, 1.1519, 1.6728,\n                      0.9378, 1.3877, 0.5914, 1.1999, 1.3282, 0.5318, 1.2327, 1.1057, 0.4885,\n                      1.0076, 1.5774, 0.7178, 0.5593, 2.0158, 0.7532, 0.4649, 1.0072, 0.8519,\n                      0.9572, 1.4672, 1.4244, 0.7878, 0.3707, 0.4468, 1.2569, 0.6336, 3.6615,\n                      0.6623, 1.0921, 1.3250, 1.3531, 0.8102, 0.8214, 0.8032, 1.1599, 0.6587,\n                      0.7644, 0.7944, 0.6608, 1.0786, 0.4560, 0.8007, 1.5810, 0.7853, 1.4592,\n                      1.4225, 0.5991, 0.4584, 0.6898, 1.0079, 1.3165, 0.5671, 1.9818, 0.8822,\n                      1.1262, 0.8180, 0.6802, 0.8285, 0.4417, 0.6263, 0.5050, 2.3733, 0.8524,\n                      0.9709, 1.9461, 1.7894, 1.3798, 0.5394, 2.2547, 1.8519, 1.8977, 0.9589,\n                      0.4555, 0.6504, 0.6189, 1.0801, 0.5556, 0.6666, 1.0795, 0.7627, 0.6954,\n                      0.6956, 0.7066, 1.0491, 0.8211, 0.8330, 0.5970, 1.3945, 0.9804, 1.9361,\n                      0.3775, 2.0924, 0.5185, 0.8908, 0.6970, 0.9542, 1.7786, 1.9174, 2.7703,\n                      0.4694, 1.4194, 1.0840, 2.2804, 1.4301, 0.9844, 1.1810, 0.9759, 0.3946,\n                      2.1077, 0.4577], device='cuda:0')),\n             ('module.vgg.8.num_batches_tracked',\n              tensor(942776, device='cuda:0')),\n             ('module.vgg.10.weight',\n              tensor([[[[ 5.8908e-02,  7.5715e-02,  4.3470e-02],\n                        [-2.1313e-02, -2.4486e-02, -4.2075e-02],\n                        [-1.9764e-02,  2.6499e-02,  2.6008e-02]],\n              \n                       [[ 3.6023e-02, -1.7412e-02,  4.1672e-02],\n                        [-2.1339e-02, -4.5340e-02,  6.8001e-02],\n                        [ 4.0133e-02, -1.5510e-02,  3.3747e-02]],\n              \n                       [[ 2.6331e-02, -1.8140e-02,  3.9888e-02],\n                        [-3.4286e-02, -7.4478e-02, -1.6558e-02],\n                        [-4.4901e-02, -4.4963e-02, -9.5544e-03]],\n              \n                       ...,\n              \n                       [[-7.2687e-03,  6.0870e-03,  1.9781e-02],\n                        [ 1.2273e-01, -1.2000e-01, -3.3758e-03],\n                        [ 1.2785e-02, -2.7334e-02,  2.6206e-02]],\n              \n                       [[-2.9999e-02, -3.8287e-02,  3.1642e-02],\n                        [-3.6712e-03, -2.6696e-02, -7.1592e-02],\n                        [-3.3622e-02, -1.1552e-01, -3.8859e-02]],\n              \n                       [[ 1.4725e-04, -2.4760e-02,  1.2717e-03],\n                        [ 1.1096e-02,  3.7378e-02,  1.2774e-02],\n                        [-2.5841e-02, -9.3962e-03,  5.5199e-02]]],\n              \n              \n                      [[[-3.7377e-02, -3.8941e-02,  6.8299e-03],\n                        [-2.2567e-03,  2.1564e-02,  5.6265e-02],\n                        [-3.7101e-02,  5.1487e-02, -2.7972e-03]],\n              \n                       [[ 3.2556e-02, -7.6228e-02,  9.4565e-03],\n                        [ 1.5762e-02, -5.3026e-02,  7.3598e-04],\n                        [ 4.4685e-02, -1.6720e-02,  2.7798e-02]],\n              \n                       [[ 1.5268e-02, -2.4665e-03,  5.8565e-04],\n                        [-4.3200e-02,  3.8735e-02,  2.5434e-02],\n                        [ 1.1004e-01, -5.6112e-02,  3.0940e-02]],\n              \n                       ...,\n              \n                       [[ 9.3956e-03,  3.1080e-02,  1.1417e-02],\n                        [-4.2953e-02, -3.3031e-02, -9.2880e-03],\n                        [ 8.8860e-02,  5.9178e-02, -5.5930e-02]],\n              \n                       [[-7.8519e-02, -9.1955e-02,  2.9642e-02],\n                        [ 3.0393e-02, -7.1543e-02, -3.3810e-04],\n                        [-2.1020e-02, -1.5245e-02,  3.3568e-02]],\n              \n                       [[ 9.2841e-02,  8.0430e-02, -3.0871e-02],\n                        [-4.8230e-02,  7.4251e-03,  2.0057e-02],\n                        [-1.6764e-02,  6.6098e-03,  8.7562e-02]]],\n              \n              \n                      [[[-2.4328e-02, -6.5989e-02, -2.9467e-02],\n                        [ 4.1303e-02, -1.4781e-02,  4.4457e-02],\n                        [-2.4337e-02,  6.4068e-03,  8.3122e-02]],\n              \n                       [[ 2.7793e-02, -9.7655e-03, -1.8204e-02],\n                        [-7.7399e-02, -4.9420e-02,  1.7874e-03],\n                        [-2.7429e-02,  3.7995e-02, -4.8801e-04]],\n              \n                       [[-1.5612e-02, -7.0442e-02,  1.1743e-02],\n                        [-3.3639e-02,  1.4766e-02, -7.3509e-02],\n                        [ 3.8347e-02, -1.5258e-02,  8.8077e-03]],\n              \n                       ...,\n              \n                       [[ 4.5769e-02,  3.7659e-03, -1.9267e-03],\n                        [-2.5954e-02,  5.0565e-03,  2.2654e-02],\n                        [ 7.1615e-02,  2.0454e-02,  6.2582e-02]],\n              \n                       [[-4.0445e-02,  3.7551e-02,  9.3684e-03],\n                        [-2.2958e-02, -3.9154e-02,  8.4177e-02],\n                        [ 1.2854e-02, -6.6503e-02,  1.0073e-01]],\n              \n                       [[-1.9479e-02, -8.8165e-02, -6.7518e-03],\n                        [-1.3698e-02,  5.8441e-02, -1.9573e-02],\n                        [-2.3583e-02,  5.5089e-02,  1.5090e-03]]],\n              \n              \n                      ...,\n              \n              \n                      [[[-8.6787e-03,  8.2180e-02,  1.9975e-02],\n                        [-3.5823e-02,  4.9440e-02, -3.1212e-02],\n                        [ 3.8672e-03, -4.3367e-02,  3.3460e-02]],\n              \n                       [[-3.9914e-04, -1.0361e-02,  2.6077e-02],\n                        [-2.9583e-03,  2.3112e-02, -6.8016e-03],\n                        [ 4.6832e-02,  3.8177e-02,  1.7227e-02]],\n              \n                       [[-9.7322e-02, -4.1631e-02,  7.2349e-02],\n                        [ 2.3256e-03,  4.4387e-02,  2.7713e-02],\n                        [ 6.8681e-02,  1.8203e-02,  4.9293e-02]],\n              \n                       ...,\n              \n                       [[-4.6464e-03,  2.3657e-02,  3.6041e-02],\n                        [-2.8811e-02, -7.6996e-03,  6.5814e-02],\n                        [-5.2583e-03,  1.6693e-02,  5.7207e-02]],\n              \n                       [[ 1.4992e-02, -7.2087e-02,  2.9019e-02],\n                        [-4.8755e-02, -9.6165e-03, -5.5387e-02],\n                        [-8.2505e-03, -5.6505e-02, -1.6578e-02]],\n              \n                       [[ 8.3898e-02, -2.0650e-02,  2.4719e-02],\n                        [ 3.7077e-02, -5.8285e-02, -2.3063e-02],\n                        [ 5.3938e-02, -1.8561e-02, -9.8558e-02]]],\n              \n              \n                      [[[ 6.1678e-03, -8.7586e-02,  5.4433e-02],\n                        [-2.9657e-02, -3.6846e-02,  4.9225e-03],\n                        [-4.9681e-02, -5.5242e-02, -1.5318e-02]],\n              \n                       [[-1.0891e-01, -3.1289e-02,  6.4877e-03],\n                        [-4.4677e-02,  5.3482e-02, -1.2931e-03],\n                        [ 3.4342e-02,  7.0734e-02,  5.7711e-02]],\n              \n                       [[ 1.6962e-02,  1.0940e-02,  1.8784e-02],\n                        [ 1.1309e-02, -8.7075e-02,  4.3742e-02],\n                        [-6.2865e-02,  4.4056e-02,  9.3784e-02]],\n              \n                       ...,\n              \n                       [[ 2.1915e-02, -9.7131e-03, -3.6208e-03],\n                        [ 2.8795e-03,  5.1971e-02, -4.4773e-02],\n                        [-4.7134e-02,  2.4662e-02, -3.6512e-03]],\n              \n                       [[-1.0673e-03, -1.9664e-02, -2.1339e-02],\n                        [-5.0387e-02, -1.9340e-02, -2.8639e-02],\n                        [-1.6339e-02, -1.7320e-02, -6.7904e-02]],\n              \n                       [[ 3.3125e-04,  5.6337e-03,  1.1914e-02],\n                        [ 5.2848e-02, -4.2834e-03, -2.1075e-02],\n                        [-3.4452e-02, -2.0867e-02,  7.3585e-02]]],\n              \n              \n                      [[[ 2.0613e-02,  5.8438e-03,  4.4536e-02],\n                        [-4.8573e-02,  2.5317e-02, -9.2895e-02],\n                        [-2.0295e-02,  8.2097e-03, -2.2792e-03]],\n              \n                       [[-2.1464e-03, -4.1622e-02,  5.6389e-03],\n                        [-1.0454e-01, -3.1516e-02, -3.3251e-02],\n                        [ 1.9887e-02,  4.7038e-02, -1.3537e-02]],\n              \n                       [[-1.1224e-02, -5.4614e-02,  2.7594e-02],\n                        [ 4.3734e-02,  1.6979e-02, -7.6100e-02],\n                        [ 2.3629e-02,  7.3768e-02, -1.6914e-02]],\n              \n                       ...,\n              \n                       [[ 2.0307e-02, -1.0530e-01, -6.0879e-02],\n                        [-4.6390e-02,  3.5037e-02, -3.1312e-02],\n                        [ 4.6503e-02, -2.9954e-02,  2.8048e-02]],\n              \n                       [[-1.6958e-02, -2.8897e-04,  4.0557e-02],\n                        [ 2.8886e-02, -1.1887e-02,  3.1417e-02],\n                        [ 3.3848e-02,  3.8336e-02,  3.1633e-04]],\n              \n                       [[-8.1872e-05,  4.2811e-02, -1.5143e-02],\n                        [-2.1418e-02, -1.1689e-02,  1.4980e-02],\n                        [-1.8721e-02,  1.9927e-02,  3.3213e-02]]]], device='cuda:0')),\n             ('module.vgg.10.bias',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n             ('module.vgg.11.weight',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1.], device='cuda:0')),\n             ('module.vgg.11.bias',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n             ('module.vgg.11.running_mean',\n              tensor([ 0.2895,  0.4560,  0.0192,  0.2253, -0.1496,  0.0934,  0.7493, -0.1790,\n                      -0.1702,  0.3666, -0.5162,  0.1091, -0.5379, -0.4163,  0.2207, -0.2057,\n                      -0.3911, -0.7201,  0.4396, -0.6416,  0.2517,  0.2725, -0.0873,  0.1752,\n                       0.6243, -0.2033, -0.9650, -0.3639, -0.0133, -0.2781, -0.0070,  0.9751,\n                       0.4717,  0.8485, -0.0391, -0.5799,  0.2437,  0.2342, -1.0051,  0.1669,\n                      -0.7211,  0.3151, -0.0588, -0.0648,  0.0903,  0.0514,  0.0824, -0.2340,\n                      -0.0081,  0.2070,  0.0788,  0.2145,  0.6178, -0.2997, -0.5565,  0.3373,\n                      -0.1934,  0.0241, -0.1698,  0.4315,  0.0745,  0.1494,  1.2564, -0.1585,\n                       0.0450,  0.3455,  0.3552, -0.3077, -0.6443, -0.6141,  0.1954,  0.2990,\n                       0.0506, -1.0615, -1.4437,  0.2195, -0.4545, -0.1405, -0.5919, -0.4881,\n                       0.2007,  0.5314,  0.2578,  0.3264, -0.1370, -0.2357, -0.2372,  0.4250,\n                      -0.3942,  0.4178,  0.1993,  0.6162, -0.1394, -0.3171,  0.4663, -0.2055,\n                       0.5871,  0.2607,  0.0266, -0.2849,  0.3122,  0.1891,  0.2436, -0.6494,\n                      -0.3693, -0.6246, -0.1967,  0.8074, -0.4058, -0.7416, -0.1387,  0.8187,\n                      -0.3626, -0.3396, -0.6879, -1.5198,  0.2200,  0.2244,  0.2703,  0.3264,\n                       0.3673, -0.1420, -0.7954, -0.7590, -0.9526, -0.7909, -0.5653, -0.1613],\n                     device='cuda:0')),\n             ('module.vgg.11.running_var',\n              tensor([0.5723, 1.2432, 0.8884, 0.7875, 0.5706, 0.3877, 0.5843, 0.7918, 0.6019,\n                      1.3288, 0.5256, 1.0352, 0.9922, 1.0959, 0.8077, 0.7076, 0.8670, 0.7260,\n                      0.4197, 1.7783, 0.7731, 0.6038, 1.2318, 0.7269, 0.5453, 0.8961, 1.0992,\n                      0.5009, 0.6601, 0.9323, 0.4610, 0.8536, 1.0911, 0.7824, 0.5738, 0.6464,\n                      0.3987, 0.5784, 0.9771, 0.4769, 0.9401, 1.2594, 0.5262, 0.5373, 0.7375,\n                      0.5728, 0.4503, 0.7049, 0.5154, 0.5619, 0.5383, 0.6753, 0.8700, 0.4783,\n                      0.7103, 0.4196, 0.4308, 0.8590, 0.4757, 0.6431, 0.4331, 1.2831, 3.2419,\n                      0.8941, 0.4390, 0.5475, 0.7810, 0.5161, 0.8099, 0.6221, 0.5720, 0.4708,\n                      0.4717, 0.5921, 1.2847, 0.3421, 0.3700, 0.8458, 0.6521, 0.6961, 0.8919,\n                      0.6973, 0.4746, 0.7753, 0.6194, 0.4685, 0.8458, 0.5692, 0.8305, 0.4893,\n                      0.7532, 0.8594, 0.8220, 0.4769, 0.4809, 0.5306, 0.7995, 0.6674, 1.1105,\n                      0.5971, 0.8814, 0.4248, 0.4251, 0.8151, 0.7530, 0.4318, 1.0173, 0.4302,\n                      1.2473, 0.7757, 0.4490, 0.9049, 0.7703, 1.0446, 0.8944, 1.3179, 0.4891,\n                      0.4894, 0.5892, 0.8227, 0.3158, 0.6048, 1.0925, 1.4776, 1.0457, 1.5870,\n                      0.7005, 0.7768], device='cuda:0')),\n             ('module.vgg.11.num_batches_tracked',\n              tensor(942776, device='cuda:0')),\n             ('module.vgg.14.weight',\n              tensor([[[[-0.0857,  0.0484,  0.0405],\n                        [ 0.0543,  0.0055,  0.0105],\n                        [-0.0182,  0.0091, -0.0417]],\n              \n                       [[-0.0537, -0.1003, -0.0150],\n                        [ 0.0183, -0.0097, -0.0038],\n                        [ 0.0079,  0.0258,  0.0346]],\n              \n                       [[-0.0218,  0.0137, -0.0508],\n                        [-0.0216, -0.0119,  0.0016],\n                        [-0.0115, -0.0091, -0.0685]],\n              \n                       ...,\n              \n                       [[ 0.0302,  0.0588,  0.0822],\n                        [-0.0202,  0.0634, -0.0018],\n                        [ 0.0383, -0.0170,  0.0370]],\n              \n                       [[-0.0767,  0.0330, -0.0374],\n                        [ 0.0396, -0.0081, -0.0619],\n                        [ 0.0184, -0.0862,  0.0176]],\n              \n                       [[-0.0237, -0.0249,  0.0710],\n                        [-0.0162, -0.0337,  0.0289],\n                        [-0.0162, -0.0137,  0.0242]]],\n              \n              \n                      [[[-0.0312,  0.0250,  0.0182],\n                        [-0.0020, -0.0482,  0.0222],\n                        [-0.0183,  0.0029,  0.0846]],\n              \n                       [[ 0.0272,  0.0246,  0.0239],\n                        [ 0.0429, -0.0220, -0.0570],\n                        [ 0.0037, -0.0412,  0.0524]],\n              \n                       [[-0.0411,  0.0258,  0.0311],\n                        [ 0.0104, -0.0135, -0.0557],\n                        [-0.1110,  0.0136,  0.0236]],\n              \n                       ...,\n              \n                       [[-0.0434,  0.0230, -0.0132],\n                        [-0.0035,  0.0066, -0.0371],\n                        [ 0.0077, -0.0065,  0.0028]],\n              \n                       [[-0.0812,  0.0519, -0.0309],\n                        [ 0.0653,  0.0269,  0.0169],\n                        [ 0.0360,  0.0349, -0.0199]],\n              \n                       [[ 0.0106, -0.0034, -0.0286],\n                        [ 0.0725,  0.0285,  0.0043],\n                        [-0.0421, -0.0286,  0.0522]]],\n              \n              \n                      [[[ 0.0343, -0.0313,  0.0018],\n                        [-0.0652,  0.0417, -0.0359],\n                        [-0.0198,  0.0522, -0.0192]],\n              \n                       [[ 0.0159, -0.0384, -0.0003],\n                        [ 0.0294, -0.0731, -0.0879],\n                        [ 0.0233, -0.0446, -0.0150]],\n              \n                       [[ 0.0234,  0.0032,  0.0070],\n                        [-0.0272,  0.0078,  0.0445],\n                        [ 0.0006,  0.0360,  0.0173]],\n              \n                       ...,\n              \n                       [[-0.0263,  0.0598, -0.0275],\n                        [ 0.0206,  0.0420, -0.0197],\n                        [ 0.0458, -0.0680,  0.0369]],\n              \n                       [[ 0.0024,  0.0364,  0.0399],\n                        [ 0.0086, -0.0665, -0.0159],\n                        [ 0.0404,  0.0360, -0.0255]],\n              \n                       [[ 0.0059, -0.0501,  0.0284],\n                        [ 0.0088, -0.0079, -0.0232],\n                        [-0.0072,  0.0546,  0.0012]]],\n              \n              \n                      ...,\n              \n              \n                      [[[ 0.0142, -0.0408, -0.0064],\n                        [ 0.0209, -0.0929,  0.0221],\n                        [-0.0572,  0.0432, -0.0482]],\n              \n                       [[ 0.0072,  0.0204,  0.0192],\n                        [ 0.0619, -0.0651, -0.0260],\n                        [ 0.0137, -0.0503,  0.0022]],\n              \n                       [[-0.0503, -0.0455, -0.0196],\n                        [-0.0242,  0.0600, -0.0992],\n                        [-0.0058, -0.0410, -0.0248]],\n              \n                       ...,\n              \n                       [[-0.0021, -0.0616, -0.0072],\n                        [-0.0582,  0.0076,  0.0356],\n                        [ 0.0521, -0.0466, -0.0231]],\n              \n                       [[-0.0266, -0.0041,  0.0051],\n                        [-0.0165,  0.0053, -0.0777],\n                        [ 0.0484,  0.0786, -0.0528]],\n              \n                       [[-0.0193,  0.0001,  0.0190],\n                        [-0.0243,  0.0545, -0.0323],\n                        [-0.0178,  0.0300,  0.0530]]],\n              \n              \n                      [[[-0.0037, -0.0397,  0.0261],\n                        [-0.0238, -0.0012, -0.0181],\n                        [-0.0094, -0.0066,  0.0597]],\n              \n                       [[ 0.0102, -0.0659,  0.0349],\n                        [-0.0036, -0.0130, -0.0142],\n                        [ 0.0518, -0.0066, -0.0110]],\n              \n                       [[ 0.0053, -0.0159, -0.0118],\n                        [-0.0037,  0.1169, -0.0118],\n                        [-0.0149,  0.0839,  0.0401]],\n              \n                       ...,\n              \n                       [[-0.0458, -0.0723, -0.0071],\n                        [-0.0131,  0.0351,  0.0489],\n                        [ 0.0032, -0.0653, -0.0183]],\n              \n                       [[-0.0114, -0.0149, -0.0412],\n                        [-0.0284, -0.0547, -0.0283],\n                        [ 0.0152,  0.0008, -0.0111]],\n              \n                       [[-0.0263,  0.0352, -0.0504],\n                        [-0.0561, -0.1002, -0.0409],\n                        [-0.1064,  0.0317, -0.0160]]],\n              \n              \n                      [[[-0.0403, -0.0818, -0.0123],\n                        [ 0.0787,  0.0624, -0.0073],\n                        [-0.0228, -0.0281,  0.0065]],\n              \n                       [[ 0.0125, -0.0368, -0.0355],\n                        [ 0.0202, -0.0554, -0.0072],\n                        [ 0.0834,  0.0739, -0.0008]],\n              \n                       [[ 0.0808,  0.0448,  0.0362],\n                        [-0.0127, -0.0125, -0.0342],\n                        [-0.0422,  0.0013, -0.0018]],\n              \n                       ...,\n              \n                       [[-0.0116,  0.0424,  0.0682],\n                        [ 0.0387, -0.0209, -0.0015],\n                        [ 0.0625, -0.0199, -0.0148]],\n              \n                       [[ 0.0178,  0.0533,  0.0040],\n                        [ 0.0147, -0.0285,  0.0398],\n                        [-0.1070,  0.0264,  0.0037]],\n              \n                       [[-0.0675, -0.0459, -0.0031],\n                        [ 0.0636, -0.0822,  0.0070],\n                        [ 0.0067, -0.0199, -0.0080]]]], device='cuda:0')),\n             ('module.vgg.14.bias',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n                     device='cuda:0')),\n             ('module.vgg.15.weight',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1.], device='cuda:0')),\n             ('module.vgg.15.bias',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n                     device='cuda:0')),\n             ('module.vgg.15.running_mean',\n              tensor([ 0.1512,  0.2389, -0.4627, -0.5338, -0.0946,  0.0751, -0.2862, -1.0916,\n                      -0.3395,  0.1809, -0.7638, -1.1489,  0.7644, -0.7329, -0.3901, -0.6303,\n                       0.9644, -0.2108, -1.7637, -0.3887,  0.6681,  0.5241,  0.3869, -0.1216,\n                       0.6797, -0.7620, -0.1452,  0.9537,  0.5173,  0.0468,  1.2867, -0.0297,\n                       0.8739, -0.6295,  0.2483,  0.1110,  0.7086,  0.9377,  0.2838, -0.5766,\n                       0.3661, -0.1044, -0.5897,  0.7251,  0.1229, -0.0157, -0.5043,  0.4931,\n                      -0.9657,  1.2128, -0.0111, -0.8104,  0.0865, -1.1158,  0.2352, -0.4198,\n                      -0.3440,  0.4942,  0.7062, -0.1963, -0.1908,  0.8350, -0.6984,  0.7768,\n                      -0.1876, -1.1867,  0.2014,  0.4911,  0.2657,  0.0704,  0.3322,  1.0783,\n                      -0.1237,  0.2090,  0.1793,  0.7055, -1.2187, -0.2862,  0.7531,  0.5706,\n                       0.5888,  0.2191,  1.3360, -0.8122,  0.1731, -0.1031,  1.0231,  0.1083,\n                      -0.4232,  0.4504,  1.6657, -0.7545,  0.1973,  1.0480,  0.1882,  0.3401,\n                       0.8872, -0.9324,  0.5191, -0.7104, -0.9546, -0.8472, -0.5937,  0.0492,\n                       1.1515, -0.4033, -0.1814, -0.1203,  0.9170, -0.4833,  0.6162, -1.3638,\n                       0.4500, -0.2572, -0.2527, -0.1853, -0.1194, -0.5587,  0.0411, -0.2021,\n                      -0.3045, -0.0191,  0.3331,  0.2129,  0.2229,  0.4377,  0.5616, -0.6422,\n                       0.1277,  0.0387, -0.7072,  0.3131, -0.7816, -0.2690,  0.9362, -0.8129,\n                      -0.2885,  0.4990, -0.1274,  0.6608,  0.9330, -0.6189, -0.3415, -0.0679,\n                      -1.0006, -0.1625,  1.2377,  0.4957,  0.8441,  0.4489,  0.2114, -0.4921,\n                       0.3209, -0.1687,  0.9246,  0.2559, -0.6687, -0.4642, -0.1412, -0.3729,\n                       0.1938,  0.2591,  0.0641, -0.5710,  0.2329,  0.0631, -0.0300,  0.3450,\n                      -0.3676, -0.4013, -0.2470, -0.5009, -0.2045,  0.4607,  0.6341,  0.1013,\n                       0.6105, -0.2886,  0.1503,  0.7846,  0.4362, -0.9920,  0.1336,  0.4996,\n                       0.5315,  0.4034,  0.2605,  0.4262, -0.2320, -0.1422,  0.5998,  0.2310,\n                      -0.2709,  0.4720, -0.3956,  1.0187,  0.3104,  0.9963,  0.2077,  0.1450,\n                      -0.1656,  0.3950,  0.9848,  0.0132,  0.4857, -0.7273,  1.1264,  0.0071,\n                       0.0230, -0.0743, -0.9789, -0.6569,  0.9616,  0.5082, -0.7823, -0.9182,\n                       0.4215, -0.6258,  0.5816,  0.7995,  1.3573, -0.0646, -0.5535, -1.8216,\n                      -0.5692,  0.5892, -0.5052,  1.1822,  0.1260,  0.1426,  1.0876, -0.8845,\n                       0.5522, -0.1325, -0.3911,  0.1618, -0.5436,  0.9078,  0.0910,  0.7918,\n                      -0.4519, -0.9535,  0.0715, -0.4004, -0.1762,  0.6141,  0.0180, -0.2651,\n                       0.4677, -0.2348,  0.2355,  0.7129,  0.2950, -0.8196,  0.0409,  0.1447],\n                     device='cuda:0')),\n             ('module.vgg.15.running_var',\n              tensor([1.3839, 1.1776, 0.8822, 1.4596, 1.2469, 1.5374, 0.8204, 1.3046, 1.8229,\n                      0.9315, 1.6405, 1.8935, 1.2735, 0.9862, 1.0202, 1.4459, 2.1441, 0.9527,\n                      5.3865, 0.8712, 2.1119, 1.8803, 0.9654, 0.8025, 0.9151, 0.7847, 0.9754,\n                      2.0379, 1.0326, 1.1299, 1.6809, 0.9249, 1.5931, 0.9318, 0.6356, 0.5993,\n                      1.6948, 1.7557, 1.6267, 1.0264, 0.9256, 1.5237, 1.7066, 1.2020, 0.7072,\n                      0.6621, 1.0794, 0.9375, 1.8122, 2.6116, 1.0379, 1.4328, 0.9938, 4.0014,\n                      1.0697, 1.1462, 0.9933, 1.3848, 1.5516, 0.6593, 2.9891, 1.6396, 1.5572,\n                      2.1013, 0.7694, 3.0304, 1.1389, 0.9053, 1.4720, 0.5930, 1.0842, 1.8929,\n                      0.9762, 1.2568, 0.6883, 2.2024, 1.8026, 0.8958, 2.5672, 1.7848, 1.1363,\n                      0.8740, 3.7422, 1.4784, 0.6294, 0.7907, 1.9939, 1.0033, 0.9419, 2.6003,\n                      5.0671, 2.0703, 1.3298, 1.9128, 0.9474, 0.8818, 1.2465, 1.4889, 0.8727,\n                      0.8230, 1.7998, 1.3392, 2.3735, 1.9178, 0.6836, 0.9397, 0.6280, 1.4878,\n                      2.2801, 2.1743, 0.8697, 4.1352, 1.0255, 0.4943, 1.2030, 0.8147, 2.5633,\n                      1.4814, 0.9540, 2.1671, 1.4936, 1.1186, 1.9548, 1.4023, 1.0690, 0.6247,\n                      0.6478, 1.1126, 0.7420, 0.7090, 1.2629, 0.7619, 1.5739, 0.9756, 4.1937,\n                      2.8276, 0.7994, 0.9058, 1.1315, 2.1776, 1.7614, 1.5206, 0.9402, 0.6571,\n                      2.8941, 0.8888, 2.2710, 1.2451, 1.8547, 1.2160, 0.6884, 1.0981, 0.9043,\n                      1.0410, 2.3295, 0.9306, 0.8445, 1.0400, 0.5999, 1.1211, 1.2356, 0.8039,\n                      0.4574, 0.9054, 0.9935, 1.7277, 0.7590, 1.2503, 0.9180, 0.7926, 1.6549,\n                      1.3177, 0.6380, 0.7619, 0.6831, 0.9132, 1.7330, 0.7712, 0.9780, 1.8094,\n                      0.7028, 3.5918, 0.7555, 2.0387, 0.7783, 1.4269, 1.0813, 1.3961, 0.8078,\n                      0.7491, 1.1870, 0.9485, 1.9326, 1.1816, 1.8251, 1.4113, 0.8258, 4.3220,\n                      1.1244, 1.3917, 1.0257, 0.7020, 1.8906, 1.0274, 1.0852, 1.5542, 1.8357,\n                      1.0054, 1.3006, 0.6880, 1.6537, 1.2421, 2.8149, 2.2110, 1.4116, 1.8769,\n                      1.4996, 0.6629, 2.3449, 1.1922, 4.5027, 1.2844, 0.8370, 6.3610, 0.9607,\n                      1.0746, 2.1365, 1.6269, 0.7290, 1.0650, 1.8954, 1.4052, 0.8067, 0.7651,\n                      1.2117, 1.4078, 0.7060, 1.6576, 1.3864, 1.9933, 0.8398, 3.0810, 0.9603,\n                      1.6430, 0.5176, 1.4956, 0.7926, 0.8791, 1.0355, 1.0246, 3.4233, 3.7617,\n                      1.2558, 4.4541, 1.4688, 1.4231], device='cuda:0')),\n             ('module.vgg.15.num_batches_tracked',\n              tensor(942776, device='cuda:0')),\n             ('module.vgg.17.weight',\n              tensor([[[[-3.3331e-02,  2.7420e-02, -1.9256e-02],\n                        [ 2.9820e-02, -6.8840e-03, -3.1975e-02],\n                        [-8.3272e-03, -2.0598e-02,  4.7794e-02]],\n              \n                       [[-1.7643e-03, -5.0954e-03,  2.7249e-02],\n                        [-6.8749e-03, -4.5719e-02, -2.8261e-02],\n                        [-1.9473e-02, -9.0894e-03,  1.5988e-02]],\n              \n                       [[-3.4776e-03,  2.0754e-02,  1.0581e-02],\n                        [ 3.8667e-02, -5.7723e-04, -9.5456e-03],\n                        [-6.2574e-02,  1.1709e-02,  7.5276e-03]],\n              \n                       ...,\n              \n                       [[ 2.9508e-02, -8.9430e-03, -4.8565e-02],\n                        [ 1.3182e-02, -7.6490e-03, -2.5769e-03],\n                        [ 6.2366e-03,  6.6845e-03,  3.2538e-03]],\n              \n                       [[ 9.0240e-03, -3.2079e-02,  7.7996e-02],\n                        [ 7.5489e-03,  3.5187e-02, -3.1536e-03],\n                        [-3.4721e-02,  2.6305e-02, -2.6762e-02]],\n              \n                       [[-2.0987e-02, -2.7615e-02,  3.0260e-02],\n                        [ 4.9707e-02,  4.4180e-02, -2.7337e-02],\n                        [-6.2445e-02, -4.8028e-02,  1.8361e-02]]],\n              \n              \n                      [[[ 4.0058e-03,  1.6277e-02, -2.9638e-02],\n                        [ 3.5341e-03, -1.6888e-02,  3.0977e-02],\n                        [ 6.8524e-02, -4.9338e-02,  2.0293e-02]],\n              \n                       [[ 7.5199e-03,  2.9610e-02,  1.8010e-02],\n                        [ 1.8817e-02, -1.1420e-01, -1.0611e-02],\n                        [ 4.4569e-02,  5.4320e-03, -5.3913e-02]],\n              \n                       [[ 6.4885e-03,  4.5633e-03,  2.7753e-02],\n                        [ 1.2959e-02, -7.9926e-03, -3.6494e-02],\n                        [-3.2040e-02, -4.3249e-02, -3.0866e-02]],\n              \n                       ...,\n              \n                       [[ 8.3124e-03, -2.9034e-02,  1.2997e-02],\n                        [-2.4975e-02, -1.6882e-02,  2.8638e-03],\n                        [ 2.6574e-02,  8.0094e-02,  2.4852e-02]],\n              \n                       [[-3.9366e-02, -1.0063e-02,  6.1577e-02],\n                        [-2.4771e-02, -3.8604e-03,  4.1812e-02],\n                        [-2.4868e-03,  9.9114e-02, -6.7562e-03]],\n              \n                       [[ 1.5012e-02,  6.3874e-02, -5.8351e-02],\n                        [ 1.4739e-02, -4.5484e-03, -1.7639e-02],\n                        [-1.5456e-02,  4.0946e-02, -1.8169e-02]]],\n              \n              \n                      [[[-1.0862e-02,  1.4142e-02,  9.0913e-03],\n                        [-7.8148e-03, -2.2413e-02, -3.7458e-03],\n                        [ 3.3599e-02, -4.6315e-02, -2.4964e-02]],\n              \n                       [[ 2.0064e-02,  3.5494e-02, -2.7387e-02],\n                        [ 7.9127e-04, -2.5380e-02,  7.7160e-02],\n                        [ 9.3393e-03,  2.9523e-02,  6.5307e-02]],\n              \n                       [[ 2.0791e-02, -3.0295e-02, -1.1322e-02],\n                        [ 4.3809e-02,  1.0980e-02, -1.8867e-02],\n                        [ 3.1600e-05, -4.0538e-02,  5.5092e-02]],\n              \n                       ...,\n              \n                       [[-2.7171e-03,  3.1108e-02, -2.8183e-02],\n                        [-1.8490e-02, -3.9881e-02, -5.6952e-02],\n                        [-7.1387e-03,  3.1741e-03,  4.6777e-02]],\n              \n                       [[-3.4646e-02, -2.5304e-02,  4.6602e-02],\n                        [ 6.9260e-02,  4.5037e-02,  9.4120e-03],\n                        [-4.6263e-03,  9.5658e-03, -2.5205e-02]],\n              \n                       [[-1.7288e-03, -3.9230e-02,  1.7619e-02],\n                        [-7.0435e-03, -4.0847e-03, -7.4701e-03],\n                        [-2.9437e-02, -6.6768e-03,  6.8340e-03]]],\n              \n              \n                      ...,\n              \n              \n                      [[[-5.6930e-02, -2.9416e-03,  2.2134e-02],\n                        [ 4.9668e-02, -3.0757e-02, -6.5273e-03],\n                        [-2.9803e-02, -2.2474e-02, -1.5148e-02]],\n              \n                       [[-9.0812e-03, -2.3329e-02, -3.8275e-02],\n                        [-3.9678e-02,  1.1120e-02, -5.1246e-02],\n                        [-2.9380e-02,  2.3313e-02, -2.9300e-02]],\n              \n                       [[ 5.3262e-02, -3.8914e-02, -6.1937e-03],\n                        [-9.1206e-03,  1.8418e-02, -5.4234e-03],\n                        [-3.5789e-02,  5.1989e-02, -2.1631e-02]],\n              \n                       ...,\n              \n                       [[ 2.3723e-02,  5.3848e-03, -4.0373e-02],\n                        [-1.4882e-03, -3.3293e-03,  6.9616e-02],\n                        [-4.9880e-02, -5.8485e-03, -5.7414e-02]],\n              \n                       [[-8.3148e-03,  1.9338e-02, -4.2974e-02],\n                        [ 1.7301e-02, -1.3750e-02, -2.1945e-02],\n                        [-2.6088e-02,  8.6616e-03,  5.6371e-02]],\n              \n                       [[ 2.4670e-02, -1.1407e-02,  2.6495e-02],\n                        [ 5.4586e-02, -1.7441e-03,  4.6957e-02],\n                        [-6.5696e-02,  1.3675e-02, -4.3468e-02]]],\n              \n              \n                      [[[-2.6458e-02, -4.3239e-02,  2.0814e-02],\n                        [ 9.6154e-03, -4.0522e-02, -2.8240e-02],\n                        [ 3.8934e-02, -2.5141e-02, -5.0167e-02]],\n              \n                       [[ 3.2449e-02,  5.1454e-02, -4.3922e-02],\n                        [-6.2512e-02,  3.4741e-02,  2.3912e-02],\n                        [ 7.3347e-02, -1.3098e-02, -1.1738e-02]],\n              \n                       [[ 1.7065e-03,  2.4374e-02, -1.3096e-02],\n                        [-4.3966e-02,  1.6687e-02,  4.3549e-03],\n                        [ 3.3143e-02,  1.5867e-02, -1.4491e-02]],\n              \n                       ...,\n              \n                       [[ 2.3491e-02,  1.1209e-03, -5.4127e-02],\n                        [-1.4791e-02,  1.0019e-02, -1.0420e-02],\n                        [ 2.2738e-02,  3.6584e-03, -2.7271e-02]],\n              \n                       [[-7.9855e-03, -3.6834e-02,  1.7426e-02],\n                        [-6.0873e-03,  3.2568e-02,  2.3080e-02],\n                        [-4.5229e-02,  1.6830e-02, -1.0216e-02]],\n              \n                       [[ 4.2337e-02,  1.5194e-02, -1.8811e-02],\n                        [-3.5612e-02,  1.7115e-02, -5.2261e-02],\n                        [ 3.7333e-03, -1.2810e-02,  3.5211e-02]]],\n              \n              \n                      [[[-5.2922e-02, -2.5308e-02,  3.5710e-03],\n                        [ 1.8339e-02, -7.9080e-03, -1.0458e-02],\n                        [-4.4113e-02,  6.2191e-02, -2.6082e-02]],\n              \n                       [[ 2.6775e-02,  7.2221e-03, -1.3820e-02],\n                        [ 5.5810e-02, -9.8386e-03,  5.6092e-02],\n                        [-4.2979e-02,  1.5166e-02,  3.8946e-02]],\n              \n                       [[-2.0187e-02, -1.5112e-02, -8.9083e-03],\n                        [ 9.5287e-03,  3.3810e-02, -1.2016e-02],\n                        [ 3.7898e-02,  3.4281e-02, -1.3095e-02]],\n              \n                       ...,\n              \n                       [[-1.9390e-02, -1.7990e-03, -4.2701e-02],\n                        [-2.0437e-02, -2.9775e-02,  1.4457e-02],\n                        [-2.3260e-02, -7.2107e-03,  2.9755e-03]],\n              \n                       [[ 8.1422e-03,  1.0935e-02, -1.0035e-02],\n                        [-1.9243e-03,  5.3829e-02,  5.9396e-04],\n                        [ 2.0207e-02,  3.8618e-02,  2.9340e-02]],\n              \n                       [[ 1.6136e-02,  6.0208e-02,  1.3284e-02],\n                        [-2.8487e-02,  2.2406e-02, -4.2972e-02],\n                        [-1.0854e-02,  2.2080e-03, -1.3726e-02]]]], device='cuda:0')),\n             ('module.vgg.17.bias',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n                     device='cuda:0')),\n             ('module.vgg.18.weight',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1.], device='cuda:0')),\n             ('module.vgg.18.bias',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n                     device='cuda:0')),\n             ('module.vgg.18.running_mean',\n              tensor([-1.7097e-01, -4.3635e-01,  6.4501e-04, -2.8082e-01, -8.7621e-01,\n                       2.5420e-01, -3.7142e-01,  2.6182e-01,  7.6650e-01,  7.4066e-01,\n                      -3.0441e-01,  7.1139e-01, -2.4871e-01,  3.4225e-01,  5.9480e-01,\n                       7.9434e-01, -1.7562e-01,  1.6638e-01,  7.7431e-02,  4.3393e-01,\n                       5.5631e-01, -9.6254e-01, -8.9494e-02,  9.2917e-01,  1.5586e-01,\n                       4.6549e-01, -4.0644e-01,  4.5484e-01,  1.0412e-01, -8.0711e-02,\n                      -6.8320e-01,  5.0087e-01,  5.3455e-01,  6.7963e-02,  3.0667e-01,\n                       1.4238e+00,  5.1693e-02,  9.1565e-02,  3.0463e-01, -4.7623e-01,\n                      -9.0543e-01, -7.4583e-03,  4.7525e-01,  5.5220e-01,  1.4225e-01,\n                      -3.8470e-01, -4.0166e-02, -3.4717e-01,  3.2621e-01, -1.2034e+00,\n                       6.9209e-01, -4.2755e-01, -5.0808e-02, -3.3194e-01, -6.1840e-01,\n                      -1.3111e+00, -4.9311e-01, -5.4754e-02, -6.7704e-01,  3.3683e-01,\n                       1.0431e+00, -8.4734e-02, -1.0710e-01,  6.9252e-02, -2.0092e-01,\n                      -1.2488e-01, -1.3229e-01, -9.0665e-01,  7.2715e-02, -3.5081e-02,\n                      -2.9501e-01, -1.2992e-02,  4.1583e-01, -5.4752e-01, -1.7470e-01,\n                      -2.4668e-01, -4.6842e-01,  5.1846e-03,  9.7080e-02,  1.5599e-01,\n                       3.8352e-01, -1.6536e-01, -7.5775e-01,  5.7537e-01,  3.4043e-01,\n                      -2.4421e-01,  2.3531e-02, -1.1472e-01,  4.3241e-01, -7.1073e-01,\n                      -4.2249e-01,  7.8833e-02, -3.7150e-03, -3.2177e-02, -7.8999e-02,\n                      -9.3797e-01,  4.3459e-01,  2.8676e-01,  9.2394e-03, -4.3654e-01,\n                      -1.3153e-01,  9.7817e-01,  9.0638e-02,  4.6073e-02, -1.5066e-01,\n                       2.6486e-01,  3.1994e-01, -3.5223e-01,  2.9678e-01, -9.0838e-01,\n                       8.4804e-02, -5.6893e-01,  5.8599e-02,  4.1382e-01, -1.1585e-01,\n                       1.4099e-02, -1.3903e-01,  6.4887e-01, -3.6897e-01,  6.8897e-01,\n                      -5.1477e-01, -1.6210e-01,  6.6398e-01,  3.5018e-01, -4.0314e-01,\n                      -5.6169e-01,  5.3723e-01, -1.3557e-01,  9.7142e-01, -5.3609e-01,\n                       8.4208e-01, -1.3802e-01,  5.3754e-01,  5.2274e-01,  1.3027e-01,\n                       6.2591e-01, -5.6163e-01,  2.4804e-01,  1.4230e-01,  8.0476e-02,\n                      -4.4046e-01, -3.9494e-01,  1.2733e+00,  1.5806e-01, -3.5970e-01,\n                      -1.8797e-01, -1.6186e-01, -3.5574e-01,  1.9257e-01, -2.4018e-02,\n                       4.1557e-01,  3.1922e-01, -3.6340e-01,  7.4172e-03,  3.0534e-01,\n                      -5.2571e-02, -3.1115e-01, -6.0523e-01, -2.5307e-01,  2.0562e-01,\n                       7.7665e-01, -4.9473e-01, -9.4733e-02,  2.9089e-01, -1.0276e-01,\n                      -3.4948e-01,  3.1885e-01, -2.0434e-01, -1.3791e-01, -1.0062e-01,\n                      -6.0462e-02,  6.0135e-01,  2.4349e-01, -8.9079e-01, -5.1364e-01,\n                       4.1983e-01, -4.7769e-01,  2.5294e-01, -8.0008e-01, -1.4645e-01,\n                      -5.2275e-01, -2.1581e-01, -7.6534e-01,  4.2350e-01,  1.2078e-01,\n                       4.7731e-01,  3.2649e-01, -7.0954e-01,  2.9845e-01, -3.8771e-03,\n                       5.6532e-01,  5.3130e-01,  1.0084e-01,  1.0423e-01,  1.6050e-01,\n                       1.5327e-01,  4.5539e-01,  4.1244e-01,  1.1508e-01, -3.2857e-01,\n                      -3.2704e-01,  3.8828e-01,  2.0705e-01,  4.2821e-01, -3.4226e-01,\n                      -2.8183e-01, -9.6078e-02, -4.9329e-01,  3.4128e-03,  4.2463e-01,\n                      -4.0750e-01,  6.1722e-01, -2.9433e-01, -8.7403e-02,  8.3595e-01,\n                       3.4410e-01,  1.0371e+00, -5.6433e-01,  3.8973e-01,  1.8409e-01,\n                      -4.1930e-01,  2.3369e-01, -7.2298e-02,  8.2276e-02,  5.0086e-01,\n                      -7.2038e-01, -1.3160e-01,  7.1096e-01,  4.1670e-02, -7.1270e-02,\n                      -5.6400e-01,  1.7935e-01,  2.2743e-01,  5.2158e-01,  2.5849e-01,\n                       2.8758e-01,  9.3420e-01,  5.0222e-02,  3.1732e-01,  9.3737e-01,\n                      -3.8094e-03, -8.8640e-02, -6.8826e-01, -3.2763e-01, -3.5829e-01,\n                       6.6177e-01, -7.0108e-01, -9.8235e-02,  6.3562e-01,  7.7044e-01,\n                      -3.5378e-01, -5.3212e-02, -4.2832e-01,  1.2343e-01, -4.5490e-01,\n                      -3.0507e-01], device='cuda:0')),\n             ('module.vgg.18.running_var',\n              tensor([0.5892, 0.5865, 0.4691, 0.7029, 1.1000, 0.6382, 1.1034, 0.8721, 0.7892,\n                      1.7119, 1.5119, 1.9946, 0.6245, 0.8701, 1.1307, 0.6178, 1.1091, 0.8850,\n                      0.8360, 0.4969, 1.1896, 0.7462, 0.4156, 0.9839, 1.0414, 0.5509, 0.9020,\n                      1.0354, 1.1263, 0.6056, 1.3061, 0.6447, 0.9488, 0.7025, 0.7741, 1.1001,\n                      1.1308, 0.4766, 0.6248, 0.4937, 1.9961, 0.5452, 0.6588, 0.8922, 0.4462,\n                      0.6997, 0.8476, 0.5955, 1.1035, 2.1008, 0.7829, 0.5777, 1.9782, 0.8064,\n                      0.5788, 2.2671, 0.5152, 0.6527, 2.1588, 0.5020, 2.0217, 0.6750, 0.5505,\n                      0.5396, 0.5704, 0.6138, 0.7811, 1.2174, 0.9000, 0.8500, 0.5683, 0.5890,\n                      0.6914, 0.8083, 0.8847, 0.4431, 0.5080, 0.4982, 1.0868, 0.7790, 0.9439,\n                      0.5710, 2.2146, 0.7475, 0.4563, 0.8713, 0.7660, 0.7353, 1.3260, 0.8812,\n                      0.5567, 0.5784, 0.7140, 0.4709, 0.8169, 1.0448, 1.3301, 0.7309, 0.5245,\n                      0.6322, 0.5654, 1.0570, 0.5018, 1.0558, 1.5598, 0.5341, 0.4548, 0.6347,\n                      0.5925, 1.4793, 0.6352, 0.5133, 0.4121, 2.2402, 0.5307, 0.5554, 0.7965,\n                      1.8633, 1.6304, 0.9045, 1.3048, 0.5185, 0.5018, 0.5733, 0.5772, 0.9145,\n                      1.3361, 1.0105, 1.3390, 0.9169, 1.7421, 0.8022, 0.7273, 1.3705, 0.7774,\n                      1.4609, 0.7838, 0.8027, 0.4624, 0.6527, 0.9839, 0.5201, 1.1608, 0.8905,\n                      0.6255, 0.6450, 0.6477, 0.5714, 0.6518, 0.6310, 0.7361, 0.7618, 0.4786,\n                      0.5833, 0.6292, 0.6798, 0.5182, 0.6862, 1.5055, 0.6506, 0.9329, 0.8683,\n                      0.5089, 0.8152, 0.4166, 1.4269, 0.6033, 0.7673, 0.5813, 0.8497, 0.5605,\n                      0.5552, 1.3017, 1.0447, 2.2111, 1.3932, 0.7030, 0.5587, 0.8629, 1.3317,\n                      1.0865, 0.4914, 1.3136, 0.9275, 0.6707, 0.9453, 0.5189, 0.9175, 0.6099,\n                      0.4342, 1.1131, 0.8104, 0.9629, 0.7138, 0.5616, 0.5580, 1.2177, 0.8454,\n                      0.5321, 1.7492, 0.5519, 0.7482, 0.7134, 1.7078, 0.4560, 0.7704, 0.9220,\n                      1.1382, 0.6479, 0.5200, 1.2373, 1.0436, 0.9233, 0.7945, 1.5574, 1.0696,\n                      1.6745, 0.4858, 0.6064, 0.7851, 0.6478, 0.4638, 0.6559, 0.8796, 0.5757,\n                      1.6865, 0.6726, 1.5733, 0.4496, 0.4710, 1.6917, 0.5120, 0.5016, 0.6491,\n                      0.6328, 0.5263, 1.3992, 0.6069, 0.6964, 3.9531, 0.4729, 0.8092, 0.6079,\n                      0.9995, 0.7174, 0.4528, 0.8927, 0.6127, 0.6513, 0.5930, 0.6571, 0.7536,\n                      0.4517, 0.6187, 0.5554, 0.6860], device='cuda:0')),\n             ('module.vgg.18.num_batches_tracked',\n              tensor(942776, device='cuda:0')),\n             ('module.vgg.20.weight',\n              tensor([[[[-0.0138,  0.0095, -0.0012],\n                        [-0.0037,  0.0118,  0.0214],\n                        [-0.0326, -0.0022, -0.0162]],\n              \n                       [[-0.0070,  0.0574,  0.0093],\n                        [-0.0156,  0.0023,  0.0109],\n                        [-0.0232,  0.0047, -0.0044]],\n              \n                       [[ 0.0468,  0.0126, -0.0318],\n                        [ 0.0101, -0.0427,  0.0213],\n                        [-0.0263, -0.0368, -0.0070]],\n              \n                       ...,\n              \n                       [[ 0.0119, -0.0237, -0.0106],\n                        [-0.0004,  0.0241,  0.0270],\n                        [-0.0153,  0.0543,  0.0031]],\n              \n                       [[-0.0503, -0.0374,  0.0154],\n                        [ 0.0106,  0.0034, -0.0220],\n                        [-0.0085,  0.0147, -0.0034]],\n              \n                       [[-0.0368,  0.0067,  0.0163],\n                        [ 0.0221, -0.0276, -0.0316],\n                        [ 0.0019,  0.0100, -0.0432]]],\n              \n              \n                      [[[-0.0108,  0.0262, -0.0176],\n                        [-0.0146,  0.0095, -0.0250],\n                        [-0.0465, -0.0569,  0.0082]],\n              \n                       [[-0.0088,  0.0218,  0.0015],\n                        [-0.0566,  0.0332,  0.0587],\n                        [ 0.0156, -0.0260, -0.0135]],\n              \n                       [[-0.0171,  0.0380,  0.0117],\n                        [-0.0328,  0.0209,  0.0053],\n                        [ 0.0124,  0.0096,  0.0384]],\n              \n                       ...,\n              \n                       [[ 0.0247, -0.1001, -0.0217],\n                        [ 0.0161, -0.0615,  0.0426],\n                        [-0.0099,  0.0589, -0.0442]],\n              \n                       [[-0.0455,  0.0133,  0.0018],\n                        [-0.0169,  0.0298,  0.0193],\n                        [ 0.0075, -0.0432,  0.0241]],\n              \n                       [[ 0.0142, -0.0577, -0.0061],\n                        [-0.0049, -0.0221,  0.0048],\n                        [ 0.0016,  0.0374, -0.0565]]],\n              \n              \n                      [[[ 0.0149, -0.0184,  0.0748],\n                        [ 0.0213,  0.0049,  0.0333],\n                        [ 0.0183,  0.0793, -0.0109]],\n              \n                       [[-0.0009, -0.0392, -0.0103],\n                        [-0.0477, -0.0281,  0.0180],\n                        [ 0.0170,  0.0135, -0.0027]],\n              \n                       [[ 0.0335, -0.0093,  0.0151],\n                        [ 0.0313,  0.0092,  0.0174],\n                        [ 0.0217,  0.0148,  0.0498]],\n              \n                       ...,\n              \n                       [[-0.0072, -0.0357,  0.0012],\n                        [ 0.0856,  0.0608, -0.0046],\n                        [-0.0134, -0.0748,  0.0024]],\n              \n                       [[-0.0127, -0.0023,  0.0022],\n                        [-0.0353, -0.0760, -0.0263],\n                        [ 0.0098, -0.0325,  0.0279]],\n              \n                       [[-0.0153,  0.0339,  0.0285],\n                        [ 0.0216, -0.0070,  0.0112],\n                        [ 0.0408, -0.0365,  0.0154]]],\n              \n              \n                      ...,\n              \n              \n                      [[[-0.0080, -0.0185,  0.0243],\n                        [ 0.0160,  0.0243,  0.0022],\n                        [ 0.0018, -0.0588,  0.0250]],\n              \n                       [[ 0.0191,  0.0507, -0.0194],\n                        [ 0.0142,  0.0018, -0.0447],\n                        [-0.0260,  0.0098,  0.0213]],\n              \n                       [[ 0.0264,  0.0089, -0.0494],\n                        [ 0.0110, -0.0277, -0.0159],\n                        [-0.0089,  0.1065,  0.0291]],\n              \n                       ...,\n              \n                       [[-0.0184,  0.0051, -0.0285],\n                        [-0.0232,  0.0141, -0.0108],\n                        [ 0.0082, -0.0078, -0.0115]],\n              \n                       [[ 0.0118,  0.0003,  0.0544],\n                        [ 0.0346,  0.0403, -0.0534],\n                        [-0.0231, -0.0031, -0.0402]],\n              \n                       [[-0.0289,  0.0334,  0.0268],\n                        [ 0.0110,  0.0055,  0.0475],\n                        [-0.0506,  0.0224,  0.0256]]],\n              \n              \n                      [[[-0.0289, -0.0717, -0.0709],\n                        [ 0.0129, -0.0128, -0.0134],\n                        [-0.0680, -0.0510, -0.0454]],\n              \n                       [[ 0.0076, -0.0123, -0.0384],\n                        [ 0.0129,  0.0211, -0.0225],\n                        [-0.0351, -0.0045, -0.0538]],\n              \n                       [[ 0.0261, -0.0320,  0.0601],\n                        [-0.0218, -0.0239, -0.0686],\n                        [-0.0350,  0.0405,  0.0040]],\n              \n                       ...,\n              \n                       [[-0.0029,  0.0591,  0.0407],\n                        [ 0.0309, -0.0150, -0.0321],\n                        [-0.0217,  0.0014,  0.0071]],\n              \n                       [[-0.0020, -0.0087,  0.0590],\n                        [-0.0033,  0.0156, -0.0051],\n                        [ 0.0225,  0.0084,  0.0142]],\n              \n                       [[-0.0385, -0.0511,  0.0410],\n                        [-0.0147,  0.0261, -0.0240],\n                        [ 0.0022,  0.0356,  0.0312]]],\n              \n              \n                      [[[ 0.0239, -0.0427, -0.0281],\n                        [-0.0451, -0.0219, -0.0230],\n                        [ 0.0176,  0.0008,  0.0067]],\n              \n                       [[-0.0144, -0.0143,  0.0393],\n                        [ 0.0297,  0.0137, -0.0283],\n                        [ 0.0033, -0.0079, -0.0082]],\n              \n                       [[-0.0479, -0.0120,  0.0544],\n                        [ 0.0459, -0.0630, -0.0445],\n                        [ 0.0491, -0.0027, -0.0384]],\n              \n                       ...,\n              \n                       [[-0.0152,  0.0185,  0.0102],\n                        [-0.0285, -0.0515, -0.0140],\n                        [ 0.0290, -0.0317, -0.0106]],\n              \n                       [[ 0.0345,  0.0312, -0.0185],\n                        [-0.0227, -0.0334,  0.0519],\n                        [-0.0017,  0.0369,  0.0355]],\n              \n                       [[-0.0131, -0.0038, -0.0505],\n                        [ 0.0474,  0.0127,  0.0037],\n                        [-0.0027, -0.0115, -0.0006]]]], device='cuda:0')),\n             ('module.vgg.20.bias',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n                     device='cuda:0')),\n             ('module.vgg.21.weight',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1.], device='cuda:0')),\n             ('module.vgg.21.bias',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n                     device='cuda:0')),\n             ('module.vgg.21.running_mean',\n              tensor([-1.1421e-01, -9.9880e-01,  2.4670e-01, -2.0856e-01, -2.3558e-01,\n                      -3.0706e-01, -2.8591e-01, -3.4334e-01, -6.4284e-01,  3.7576e-01,\n                      -3.4819e-01,  4.4664e-01, -6.8401e-01, -7.4752e-01, -4.3463e-01,\n                       7.9598e-02, -5.5418e-01,  7.9742e-01, -1.5434e-01, -6.9022e-01,\n                       5.8054e-01, -7.5903e-01, -2.1859e-02, -2.9369e-01, -1.6454e-01,\n                      -1.3062e-01,  6.9797e-01,  1.1238e-01,  2.0809e-01, -1.6390e-02,\n                      -4.3600e-01,  3.4520e-02,  1.3731e-01,  2.6949e-01,  6.5764e-02,\n                       3.8611e-02, -3.0271e-01,  2.0539e-02,  2.9571e-01, -1.8053e-01,\n                       1.7817e-01, -5.1160e-02, -2.8658e-01, -2.6157e-02, -6.3112e-01,\n                       1.0832e-01, -3.0321e-01,  1.3282e-01, -1.9693e-01, -6.4559e-02,\n                      -3.2092e-01, -1.5203e-01,  2.9035e-01,  1.0445e-01,  2.4542e-01,\n                      -8.8299e-01,  4.5231e-01,  1.9001e-01,  1.0461e-02,  4.5499e-01,\n                       5.7731e-01, -3.1879e-01,  5.5889e-01,  2.4127e-01, -5.6146e-01,\n                      -2.9304e-01, -9.1143e-01,  3.4754e-01,  2.3870e-01,  5.4642e-01,\n                       1.6110e-01,  2.9399e-01, -4.1566e-01, -5.7483e-02,  7.6505e-02,\n                       7.0372e-02, -6.2419e-01,  4.9343e-01,  5.5312e-01, -5.1305e-01,\n                       6.2922e-01, -3.3128e-01,  1.5519e-01,  8.3627e-01, -1.0935e-02,\n                       1.7152e-01,  1.4129e-01,  4.3312e-01,  3.9055e-01, -9.8603e-01,\n                      -3.6014e-01,  7.9720e-02,  6.2534e-01,  7.2154e-01,  6.9540e-01,\n                      -1.0715e-01,  4.2568e-01,  1.7619e-01, -8.6419e-02, -1.3216e-01,\n                      -3.8370e-01,  1.5487e-01,  8.6601e-01, -3.6006e-01,  4.7823e-01,\n                       1.5130e-01, -1.9388e-01, -7.2655e-01, -6.9775e-01,  3.1370e-01,\n                       3.6362e-01, -1.1514e-01,  1.4140e-01, -6.4759e-01,  5.6739e-01,\n                       5.4295e-01,  2.7464e-01, -1.2824e-01, -5.0610e-02,  2.4558e-01,\n                      -7.9946e-02, -2.2865e-02,  2.1605e-01, -5.3371e-01, -3.0829e-01,\n                      -8.5953e-01,  1.0731e+00, -3.1849e-01,  6.3352e-01,  7.6867e-02,\n                      -2.7856e-01, -4.5257e-01,  4.6220e-01,  2.0873e-01, -3.0691e-01,\n                       1.2238e-01, -1.3455e-01, -1.1684e-01, -1.9360e-02, -3.4829e-01,\n                      -1.0422e-01,  3.8795e-02, -1.8519e-01,  5.4793e-01, -5.3661e-01,\n                      -2.0540e-01,  1.3468e-02, -7.5531e-02, -3.3773e-01, -2.9375e-01,\n                       1.6389e-01,  2.1123e-01, -4.2364e-01,  6.7998e-02,  2.8359e-02,\n                      -1.0525e-01, -2.0286e-01, -7.0852e-01, -6.8476e-01, -5.2568e-01,\n                       5.7895e-01,  4.5422e-01, -1.3033e-01, -2.8030e-01,  2.7335e-01,\n                      -4.3860e-01, -9.3489e-01, -1.6419e-01,  4.4253e-01,  2.4585e-01,\n                       3.4686e-01,  8.3148e-01,  4.2108e-01, -2.0793e-01, -1.5145e-01,\n                       2.7084e-01,  8.7013e-01,  6.2439e-01, -8.4615e-01, -6.2761e-01,\n                      -7.8699e-01,  9.6143e-01, -1.0625e-01, -7.4381e-01,  6.5460e-02,\n                       1.7679e-01,  3.1440e-01,  6.8080e-03,  3.2305e-01,  1.8340e-01,\n                      -6.1679e-01,  1.7481e-01, -5.9917e-02, -5.2381e-02, -2.6542e-01,\n                      -6.9404e-01, -4.3610e-01,  2.8520e-01, -1.3826e-01, -1.9685e-01,\n                      -7.2519e-01, -2.2603e-01,  5.9294e-04,  1.8576e-01,  8.0781e-01,\n                      -5.7173e-01,  3.1950e-01,  3.2346e-01, -7.7982e-01, -2.1070e-01,\n                      -4.2087e-01,  2.5251e-01,  1.1137e-01, -4.4714e-01,  2.0893e-01,\n                       9.8212e-02,  1.3224e-01, -8.9327e-01, -2.3068e-02,  2.2266e-01,\n                       1.5936e-01, -6.0630e-01,  3.3958e-01,  1.6578e-01,  2.3447e-01,\n                       4.9202e-01,  2.2246e-01, -1.4351e-01,  3.3733e-01,  3.5229e-01,\n                      -2.1832e-02, -1.0935e-01,  1.0374e-01,  8.8053e-02,  2.3595e-01,\n                      -1.4479e-02, -1.6201e-01,  6.7290e-01,  8.2503e-01,  1.8995e-01,\n                      -7.0360e-03, -3.6001e-01, -2.4232e-01, -2.2536e-01, -1.7291e-01,\n                      -5.8985e-01,  5.4731e-02, -2.0463e-01,  5.2415e-01,  2.4011e-01,\n                      -1.3494e+00,  3.5167e-01,  2.1106e-01,  2.5932e-01, -1.9439e-02,\n                      -5.5565e-02], device='cuda:0')),\n             ('module.vgg.21.running_var',\n              tensor([0.5237, 1.4131, 0.8129, 0.6729, 0.7020, 0.5755, 0.9181, 0.8387, 0.5889,\n                      0.7133, 0.4113, 1.0572, 0.6395, 0.8255, 0.5600, 0.5102, 1.0693, 0.8099,\n                      0.6297, 1.4529, 0.9297, 0.9118, 0.6292, 0.5345, 0.6730, 0.5712, 1.4522,\n                      0.5787, 0.7561, 0.5402, 0.5972, 0.5973, 0.7596, 0.6290, 0.5429, 0.7346,\n                      0.6968, 0.6233, 0.5956, 1.0639, 0.5598, 0.7133, 0.6409, 0.7042, 0.7766,\n                      0.5973, 1.3820, 0.4544, 0.5813, 0.5684, 0.5979, 0.6029, 0.8432, 0.9485,\n                      0.6636, 1.2745, 0.4558, 1.0662, 0.4780, 0.4923, 1.0282, 0.6299, 0.6364,\n                      1.0724, 0.9447, 0.5624, 2.3979, 0.6675, 0.5935, 1.2026, 0.7494, 1.0718,\n                      0.7975, 0.5111, 0.6220, 0.8028, 1.2681, 2.3799, 0.6734, 1.1490, 0.5779,\n                      1.4696, 0.5223, 0.7876, 0.7646, 0.6563, 0.7031, 0.6263, 0.5488, 0.9642,\n                      0.5775, 0.7579, 0.9014, 1.8709, 0.5956, 0.6379, 0.5773, 0.8967, 0.6853,\n                      0.6138, 1.3854, 0.5622, 1.2665, 1.0957, 0.7621, 0.5979, 0.7296, 0.6610,\n                      1.1718, 0.5807, 0.7804, 0.6230, 0.7746, 1.8947, 0.6652, 0.6537, 0.4657,\n                      0.5648, 0.8206, 0.8713, 0.6000, 0.6491, 1.6320, 0.6061, 0.6340, 1.6889,\n                      2.0782, 0.6463, 0.6076, 0.5607, 1.2454, 0.9816, 0.8762, 0.5902, 0.6562,\n                      0.6271, 0.8127, 0.4792, 0.6697, 0.7254, 0.6678, 0.7263, 0.6904, 1.4309,\n                      0.9063, 0.6111, 1.4771, 0.4695, 0.4845, 0.5605, 0.5548, 0.5431, 0.6059,\n                      0.5880, 0.8029, 0.5858, 0.8940, 0.5988, 0.8355, 0.7335, 0.7451, 0.6530,\n                      1.2436, 0.5655, 0.6935, 0.8648, 0.7180, 0.4983, 0.6032, 0.5898, 0.5754,\n                      0.4964, 0.6610, 0.5163, 0.8609, 0.7695, 0.9218, 0.5146, 1.3439, 0.9028,\n                      0.9375, 1.6569, 0.5780, 1.2253, 0.5185, 0.6002, 0.7339, 0.8924, 0.5882,\n                      0.6171, 1.5723, 0.8170, 0.4396, 0.6086, 0.6259, 1.2995, 0.9388, 0.8308,\n                      1.3498, 0.8888, 0.9085, 0.5083, 0.5975, 0.6489, 0.7159, 1.1164, 0.6006,\n                      1.2796, 0.5565, 0.4780, 0.7302, 0.4546, 1.0762, 0.6847, 0.5533, 0.6196,\n                      0.5479, 1.2074, 0.5129, 1.8907, 0.5458, 0.6234, 0.4962, 0.6192, 1.1118,\n                      0.6227, 0.4877, 0.5450, 0.4848, 0.9875, 0.5749, 0.5177, 0.7262, 0.5008,\n                      0.5621, 0.6076, 0.4704, 0.5688, 0.5952, 0.6830, 0.5023, 0.5433, 0.6741,\n                      0.7855, 0.5850, 0.8446, 0.6973, 0.5037, 0.5932, 0.4810, 1.4852, 0.5696,\n                      0.4799, 0.4915, 0.5635, 0.8295], device='cuda:0')),\n             ('module.vgg.21.num_batches_tracked',\n              tensor(942776, device='cuda:0')),\n             ('module.vgg.24.weight',\n              tensor([[[[-0.0332,  0.0043, -0.0320],\n                        [ 0.0008, -0.0130,  0.0096],\n                        [ 0.0181, -0.0091, -0.0452]],\n              \n                       [[-0.0601,  0.0276,  0.0306],\n                        [ 0.0131, -0.0543,  0.0106],\n                        [-0.0052,  0.0283, -0.0082]],\n              \n                       [[ 0.0347,  0.0556, -0.0018],\n                        [ 0.0087,  0.0075,  0.0007],\n                        [ 0.0191,  0.0152,  0.0480]],\n              \n                       ...,\n              \n                       [[ 0.0239, -0.0228, -0.0052],\n                        [ 0.0312, -0.0172,  0.0035],\n                        [ 0.0207, -0.0140, -0.0008]],\n              \n                       [[ 0.0150, -0.0275, -0.0012],\n                        [ 0.0563,  0.0156,  0.0002],\n                        [-0.0090, -0.0756, -0.0109]],\n              \n                       [[ 0.0373,  0.0232, -0.0219],\n                        [ 0.0161, -0.0024, -0.0307],\n                        [ 0.0269, -0.0330,  0.0099]]],\n              \n              \n                      [[[ 0.0078, -0.0689,  0.0578],\n                        [ 0.0011,  0.0238, -0.0629],\n                        [ 0.0190,  0.0257,  0.0046]],\n              \n                       [[ 0.0133,  0.0086,  0.0383],\n                        [-0.0030, -0.0383, -0.0454],\n                        [-0.0085, -0.0372,  0.0347]],\n              \n                       [[ 0.0434,  0.0166, -0.0362],\n                        [-0.0633,  0.0213, -0.0362],\n                        [-0.0766,  0.0184,  0.0200]],\n              \n                       ...,\n              \n                       [[ 0.0005,  0.0527, -0.0133],\n                        [ 0.0299, -0.0292, -0.0007],\n                        [-0.0108, -0.0159,  0.0207]],\n              \n                       [[-0.0156, -0.0170, -0.0287],\n                        [-0.0125,  0.0392, -0.0369],\n                        [ 0.0362,  0.0060, -0.0137]],\n              \n                       [[-0.0002,  0.0114,  0.0093],\n                        [ 0.0496, -0.0176, -0.0090],\n                        [ 0.0287, -0.0442,  0.0111]]],\n              \n              \n                      [[[-0.0116, -0.0353, -0.0102],\n                        [-0.0435,  0.0492, -0.0149],\n                        [-0.0239,  0.0120,  0.0330]],\n              \n                       [[-0.0078, -0.0297,  0.0251],\n                        [-0.0148, -0.0278,  0.0211],\n                        [-0.0847,  0.0162,  0.0147]],\n              \n                       [[ 0.0079,  0.0275,  0.0199],\n                        [-0.0399, -0.0121,  0.0293],\n                        [ 0.0083, -0.0031,  0.0182]],\n              \n                       ...,\n              \n                       [[-0.0175, -0.0323, -0.0252],\n                        [-0.0072,  0.0104, -0.0426],\n                        [-0.0065, -0.0099,  0.0462]],\n              \n                       [[-0.0263, -0.0110, -0.0313],\n                        [-0.0525, -0.0595, -0.0203],\n                        [ 0.0214,  0.0595, -0.0223]],\n              \n                       [[ 0.0101, -0.0046, -0.0282],\n                        [-0.0327,  0.0062, -0.0097],\n                        [ 0.0080, -0.0111,  0.0312]]],\n              \n              \n                      ...,\n              \n              \n                      [[[ 0.0235,  0.0360,  0.0560],\n                        [ 0.0013,  0.0387, -0.0080],\n                        [ 0.0186, -0.0080,  0.0084]],\n              \n                       [[ 0.0057,  0.0238,  0.0141],\n                        [-0.0039, -0.0028,  0.0337],\n                        [-0.0320,  0.0064, -0.0367]],\n              \n                       [[-0.0251, -0.0333, -0.0222],\n                        [-0.0104, -0.0132, -0.0085],\n                        [-0.0089, -0.0051,  0.0119]],\n              \n                       ...,\n              \n                       [[ 0.0370,  0.0097,  0.0219],\n                        [ 0.0225, -0.0797, -0.0278],\n                        [-0.0078, -0.0172, -0.0160]],\n              \n                       [[ 0.0329,  0.0267, -0.0080],\n                        [-0.0049,  0.0487, -0.0022],\n                        [ 0.0147,  0.0018,  0.0330]],\n              \n                       [[-0.0119,  0.0260, -0.0292],\n                        [-0.0080,  0.0020,  0.0042],\n                        [-0.0284,  0.0527,  0.0247]]],\n              \n              \n                      [[[ 0.0300, -0.0353,  0.0133],\n                        [-0.0251,  0.0090, -0.0206],\n                        [-0.0062, -0.0303,  0.0188]],\n              \n                       [[-0.0546,  0.0077,  0.0056],\n                        [ 0.0373, -0.0297,  0.0151],\n                        [-0.0005,  0.0460,  0.0247]],\n              \n                       [[-0.0459, -0.0239,  0.0106],\n                        [ 0.0158, -0.0167,  0.0192],\n                        [ 0.0224, -0.0214,  0.0236]],\n              \n                       ...,\n              \n                       [[-0.0239,  0.0124,  0.0305],\n                        [-0.0291,  0.0014,  0.0252],\n                        [ 0.0030,  0.0237,  0.0108]],\n              \n                       [[-0.0058, -0.0249,  0.0317],\n                        [ 0.0094, -0.0108,  0.0070],\n                        [ 0.0544,  0.0383,  0.0074]],\n              \n                       [[-0.0188, -0.0050,  0.0222],\n                        [-0.0392,  0.0023, -0.0209],\n                        [-0.0256,  0.0287,  0.0167]]],\n              \n              \n                      [[[-0.0277, -0.0137,  0.0610],\n                        [ 0.0281,  0.0105,  0.0208],\n                        [-0.0033, -0.0218,  0.0725]],\n              \n                       [[ 0.0051,  0.0254, -0.0379],\n                        [-0.0392, -0.0074,  0.0064],\n                        [-0.0061,  0.0102,  0.0277]],\n              \n                       [[ 0.0176,  0.0015,  0.0023],\n                        [-0.0177, -0.0588, -0.0040],\n                        [ 0.0238, -0.0187,  0.0223]],\n              \n                       ...,\n              \n                       [[-0.0436,  0.0481,  0.0532],\n                        [ 0.0151, -0.0049, -0.0216],\n                        [ 0.0616, -0.0429,  0.0118]],\n              \n                       [[ 0.0002,  0.0069, -0.0408],\n                        [ 0.0058, -0.0116,  0.0025],\n                        [ 0.0544,  0.0240,  0.0059]],\n              \n                       [[-0.0213, -0.0351, -0.0167],\n                        [ 0.0067, -0.0443, -0.0166],\n                        [ 0.0572,  0.0419,  0.0211]]]], device='cuda:0')),\n             ('module.vgg.24.bias',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n             ('module.vgg.25.weight',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1.], device='cuda:0')),\n             ('module.vgg.25.bias',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n             ('module.vgg.25.running_mean',\n              tensor([-0.3443, -0.3021, -0.2326, -1.4575, -0.0416, -0.1995,  0.7029,  0.7789,\n                      -1.3074,  0.5725,  0.1725,  1.4682, -0.7261, -0.3567,  0.7141,  0.4146,\n                       0.3522,  0.3869,  1.2085, -1.0785,  0.1693, -1.3032,  0.9674, -1.0485,\n                      -0.1471, -0.4890, -1.2153, -0.6905,  1.1633,  0.5194,  1.1306,  0.1924,\n                       0.0060,  0.2055, -0.6160, -0.5030, -0.5012, -1.0412,  0.1450, -0.0609,\n                      -0.3873,  1.6773, -1.5478, -1.7971, -0.7900,  0.4315,  0.1301,  0.0760,\n                      -0.1554,  0.1528,  0.6489, -1.5368,  0.5954, -1.2021, -0.0465, -1.0955,\n                       0.2717, -0.1618,  0.3668, -0.3411,  0.9332,  0.1649, -0.4798, -0.1440,\n                       0.8945, -0.4753,  0.7074, -0.5117,  0.5316, -0.0743,  0.6764,  1.1543,\n                       0.3795, -0.3825,  0.2470,  0.7454, -0.2209, -0.0732, -0.4611, -0.1385,\n                      -0.5857,  0.1526, -1.1181,  0.4335, -0.0264, -1.0762, -0.7713,  0.3546,\n                       0.9150,  0.7900,  0.4065, -0.9196,  1.2091,  1.8832, -0.5639,  0.5278,\n                       0.3046,  1.3869,  0.3638,  0.6468,  1.5722,  0.2378, -0.6598, -1.0773,\n                       1.0118, -0.7452, -0.1434,  0.1383, -1.1200,  1.5877, -2.0904, -1.3736,\n                       0.2274,  1.3918,  0.6209, -0.2960, -0.5860,  0.5311,  1.0305,  0.9540,\n                       0.1562,  0.1750, -1.5092,  0.4062,  1.3914,  0.4172,  0.5159,  0.8501,\n                      -0.3161, -0.0750, -1.0081, -0.3232, -0.6848, -0.5362, -0.3268,  0.3870,\n                      -0.6381,  1.1894,  0.7488,  1.1797,  1.9674,  0.0728, -0.1234, -0.4545,\n                      -0.2686,  0.6421,  0.4901,  0.9745, -0.4070,  0.7950,  1.1847, -0.4843,\n                       0.1788, -0.1779, -0.6702,  0.2836,  1.3824, -0.6784,  1.0476,  0.6974,\n                      -0.0890,  2.0504,  1.0219, -0.0424,  0.2615, -0.0661, -0.6315,  0.1407,\n                      -0.0693,  1.4936, -1.1819,  1.3884, -0.1591,  1.4389, -1.6134, -0.2454,\n                      -0.3070,  0.7665,  0.2481,  1.9681, -0.4536, -0.0776,  0.6384,  0.2987,\n                       0.5656,  0.0884,  0.5214, -0.8988, -0.1771,  0.0024, -1.2937, -1.2249,\n                       0.0827,  0.9289,  1.9061,  0.0725,  0.4795,  0.2887, -0.9531,  0.0594,\n                      -0.8600, -0.5540, -0.0248, -0.0478, -0.0073,  1.5427, -0.4544,  0.2563,\n                       0.4084,  0.1549, -0.3600,  1.2018, -0.3495, -0.4470, -0.2941, -0.0969,\n                      -0.8334,  0.0553,  0.0235, -1.4642, -1.1080,  0.4072,  1.0583, -0.0879,\n                       0.2596,  1.2056, -0.5214,  0.2596, -0.9042,  0.0384,  0.2428,  0.4284,\n                       0.7375, -0.7759,  0.6282,  0.3776,  0.5359, -1.1535, -0.5408,  0.3016,\n                      -0.8329, -0.8597, -0.3952,  0.3760, -1.3902, -0.3139,  0.7668, -0.3150,\n                      -0.7652,  0.2775, -0.9956,  0.0173,  0.1372,  0.0401,  1.2768,  0.8755,\n                      -0.2630,  0.0659, -0.7548,  0.0120,  0.3397,  0.2184,  0.1788, -0.2149,\n                      -0.0682, -0.8383,  0.6613, -0.1711, -0.4490,  0.4302, -0.8286,  1.3522,\n                      -0.5325,  0.0873, -1.1476, -0.6056,  0.7501,  1.3542,  0.3227,  0.9212,\n                       0.9452,  0.4511,  1.3676,  0.5478,  0.7615,  0.4555, -1.6263, -0.0590,\n                      -0.3136,  0.2051,  0.2094, -0.6875, -0.0112,  0.2871, -0.2653, -0.3747,\n                       0.1792, -1.3184,  0.1700, -1.5592, -0.9427, -0.2759,  1.5438, -0.6177,\n                      -0.7008,  0.5025,  0.4108,  1.2111, -0.5290, -0.5944,  0.5126,  0.7413,\n                       0.2142, -0.0497,  0.7603, -0.4754,  0.4002, -0.5792,  0.0939,  0.3110,\n                      -1.1112, -0.9272, -0.3899,  0.2770, -1.4477,  0.1824,  0.4585,  0.2290,\n                      -0.2535,  0.8895, -0.8602,  0.3668,  0.9927, -0.3138,  0.4428,  0.5385,\n                       0.1080, -0.5448, -0.3188, -0.2063, -0.8648,  0.2332,  1.0369, -1.0381,\n                      -0.0806, -0.6346, -0.6671, -0.1052,  0.2809, -0.1952,  1.3288,  0.2133,\n                       0.0268,  0.3005, -0.8250, -0.4718,  1.1720,  1.6698,  0.4665, -0.4687,\n                      -0.0312,  0.4028,  0.4036, -0.0900, -0.3437, -0.2991, -0.0281, -0.8132,\n                       0.2361, -1.4177,  0.0704,  0.8897,  0.2860, -0.4398, -0.4756,  1.4773,\n                       0.1809,  0.6327,  0.8988,  0.1026, -0.2713,  1.3615,  0.2297,  0.3909,\n                      -0.2711,  0.2233,  0.6274,  0.2509,  0.0921,  0.3802,  0.5291,  0.5336,\n                      -0.7434, -0.3586,  0.1497,  0.9630,  0.3789, -0.2642, -0.4366, -0.4409,\n                       0.0204, -0.2707, -0.0809,  0.7860,  0.3962,  0.4609,  0.2275, -1.0019,\n                       0.7347,  0.8091,  1.0304, -0.4753, -0.3275,  0.0263, -0.3814, -1.3419,\n                       0.4373,  0.5946,  1.9254,  0.4752,  0.8452,  0.8611, -0.0321, -0.8450,\n                       0.0546,  0.7543,  0.0533,  0.3823,  0.9463, -1.2089, -1.0432,  0.2131,\n                       0.6484, -0.7305,  0.3216,  0.8300, -0.0132,  1.2995,  0.1489,  1.0004,\n                      -0.3229, -0.7952, -0.3704,  0.7793, -0.5762,  0.8512, -0.4065, -0.7170,\n                       0.5614,  0.3016, -1.0183, -0.0398,  1.2135,  0.4433,  0.1628, -0.2140,\n                       0.3125, -0.4127, -0.4206,  0.2004, -0.0309, -1.5715,  0.4399, -0.2905,\n                      -1.0133, -0.5653,  0.7924,  0.7941, -0.1967, -0.7020, -0.8771, -0.3397,\n                      -0.0665, -1.0705, -0.1533, -0.3526,  1.2273, -0.2249, -0.6772,  0.1253,\n                       0.3758,  0.5475, -0.1095,  0.5531, -0.7108,  1.4198,  0.2137,  0.1323,\n                      -0.4803,  0.0766,  0.3575,  0.4951, -1.1581, -0.6997, -0.6018,  0.1243,\n                       0.9047,  0.0157, -0.7500,  0.8812,  0.2322,  0.1163,  0.8623,  0.8625,\n                      -0.8933,  0.4560,  0.1185,  0.5994,  0.1244,  0.9525,  0.8537, -0.4628],\n                     device='cuda:0')),\n             ('module.vgg.25.running_var',\n              tensor([1.1015, 2.0900, 0.9423, 5.3893, 0.7500, 1.2797, 1.6124, 3.5920, 1.8809,\n                      0.8718, 0.7889, 2.4556, 1.2177, 1.5730, 1.2045, 0.9338, 1.0530, 1.0664,\n                      3.4215, 2.1912, 1.1123, 3.9023, 2.4083, 3.2445, 0.9824, 1.1262, 1.8699,\n                      1.6061, 1.1855, 1.1953, 1.9751, 0.9224, 1.3918, 1.0634, 1.0514, 1.5060,\n                      1.0709, 0.9999, 1.0295, 1.4967, 0.7266, 3.4041, 2.5782, 3.8281, 2.0147,\n                      1.3983, 0.6509, 1.1188, 1.1184, 0.6972, 2.4325, 3.4102, 0.7448, 2.0702,\n                      1.2915, 1.6028, 0.8316, 0.9185, 2.5476, 0.9116, 2.2396, 1.5016, 1.1625,\n                      1.2096, 1.3215, 1.2509, 1.6986, 1.4818, 1.5389, 1.1057, 2.0243, 1.4717,\n                      1.0159, 1.2193, 1.8755, 1.6903, 0.9509, 0.8993, 0.8967, 0.8933, 1.1863,\n                      1.3687, 1.1386, 0.9393, 1.0168, 2.1501, 0.9966, 1.1771, 1.1110, 1.9028,\n                      1.6553, 1.5636, 3.6909, 3.6891, 1.9171, 2.0392, 1.2210, 2.9876, 1.2648,\n                      1.0643, 3.1970, 0.7861, 0.9885, 1.7236, 2.4350, 1.5114, 1.3651, 1.4641,\n                      2.7751, 4.0233, 5.8566, 1.8052, 0.9249, 3.2961, 0.7520, 1.2945, 0.9606,\n                      1.0788, 2.5852, 2.4125, 0.8937, 1.2150, 4.0956, 0.9414, 2.9823, 1.5828,\n                      2.2981, 1.9550, 1.3590, 1.4782, 3.0639, 0.8178, 1.6115, 1.1946, 2.6443,\n                      0.8456, 1.2606, 1.3788, 2.0419, 1.5405, 3.5511, 0.7316, 0.9897, 1.5961,\n                      1.1063, 1.2854, 1.8247, 2.0434, 1.7070, 2.4517, 1.9285, 1.0614, 0.7768,\n                      1.6474, 2.3470, 1.2489, 2.9436, 0.9659, 3.2838, 2.1454, 1.7647, 9.0804,\n                      1.6794, 0.7958, 1.8090, 1.0873, 1.3764, 1.2085, 2.1258, 4.9628, 1.5856,\n                      3.8553, 0.9105, 2.3611, 6.3165, 0.8856, 1.1945, 1.4348, 1.7269, 3.2938,\n                      1.2632, 1.5298, 1.4478, 0.8638, 3.4842, 1.2254, 1.5156, 1.7816, 1.9273,\n                      1.3209, 1.3904, 1.6149, 0.7568, 2.2833, 8.2462, 0.7941, 1.2245, 1.1927,\n                      1.9123, 0.6962, 0.9867, 1.3181, 0.8676, 1.2706, 0.8122, 4.5041, 0.9192,\n                      1.6040, 0.9134, 1.5466, 1.5838, 3.2002, 1.1724, 0.8444, 1.2667, 0.7848,\n                      2.6116, 0.9471, 0.9515, 2.4433, 2.7154, 1.3026, 2.8101, 1.5601, 1.0123,\n                      3.3255, 1.1058, 1.0787, 3.7025, 1.3250, 1.3858, 1.2237, 1.2850, 1.7617,\n                      1.1519, 0.8471, 1.2926, 1.5627, 1.4206, 2.0371, 1.7037, 1.7922, 1.0117,\n                      0.8511, 5.0976, 1.0466, 1.5999, 1.5752, 2.9023, 1.3362, 2.7407, 1.2466,\n                      1.0323, 0.7688, 3.7822, 1.6336, 0.9712, 1.1117, 1.0371, 0.7443, 0.7751,\n                      1.4894, 1.2388, 1.0310, 1.0397, 1.7003, 1.0687, 0.7570, 1.4405, 1.5078,\n                      1.3727, 1.3321, 1.2092, 0.9790, 2.9237, 1.2983, 1.3735, 2.3593, 0.7615,\n                      1.3022, 1.3297, 0.8403, 2.8852, 1.2661, 2.2517, 2.3092, 4.2963, 1.4305,\n                      1.0876, 1.1277, 1.4139, 1.7419, 0.9798, 0.8027, 0.7660, 1.6044, 0.9228,\n                      1.4759, 1.2755, 4.3074, 1.1897, 0.8187, 3.9051, 0.8695, 1.2894, 0.9571,\n                      2.0744, 2.2013, 1.1400, 1.0694, 1.4993, 2.0024, 1.7327, 1.0933, 2.3260,\n                      2.4798, 1.4511, 1.3147, 0.9122, 1.0597, 5.2531, 1.1891, 1.5041, 0.9940,\n                      1.9255, 0.8454, 1.3410, 0.7890, 0.8632, 3.8221, 2.8027, 1.2019, 2.3929,\n                      0.9646, 1.0525, 2.3738, 1.0122, 1.8850, 1.2325, 1.0263, 1.8276, 1.0257,\n                      1.8095, 3.1977, 0.8487, 1.0653, 1.5512, 1.1987, 1.0683, 1.1003, 2.2736,\n                      0.6978, 0.9948, 0.9226, 0.8866, 0.7543, 1.3714, 5.2232, 1.8361, 1.2612,\n                      1.0743, 0.7061, 1.3730, 1.1257, 1.5671, 1.0840, 1.0703, 1.4438, 1.3512,\n                      4.1537, 0.7249, 1.7097, 0.8476, 2.8115, 1.1759, 2.6505, 1.0763, 1.1401,\n                      1.1849, 1.1972, 1.1104, 3.4916, 1.1834, 1.1506, 0.9945, 0.7349, 0.9797,\n                      2.1214, 0.9345, 1.0358, 1.5715, 1.3136, 1.1926, 1.0560, 1.1829, 1.6578,\n                      1.3534, 1.2482, 2.0366, 1.5365, 0.9120, 1.4696, 1.0263, 1.8995, 1.2165,\n                      1.4831, 0.7926, 1.9154, 1.6559, 0.9389, 2.3983, 0.8710, 1.5743, 0.6918,\n                      0.9318, 2.0700, 1.1633, 2.2802, 6.3707, 2.7972, 1.1646, 1.8285, 0.6817,\n                      4.2078, 1.0329, 1.1432, 1.1676, 1.1981, 1.5832, 3.1210, 2.2836, 0.9840,\n                      1.0746, 1.0724, 1.3897, 2.8608, 0.9033, 4.1720, 1.3185, 3.3621, 1.0739,\n                      3.1863, 2.4644, 0.8605, 1.3567, 0.9808, 1.7366, 2.2244, 1.2572, 1.0471,\n                      3.3588, 0.7412, 3.5822, 1.0380, 0.6928, 1.6430, 0.9454, 1.2264, 1.3701,\n                      1.7074, 0.9474, 5.1627, 1.2822, 1.2017, 2.2927, 1.8308, 1.0612, 3.8756,\n                      1.3928, 0.9032, 2.0426, 0.8508, 0.9201, 1.6395, 1.5677, 2.1639, 3.1489,\n                      1.5379, 1.1319, 1.0035, 1.3325, 1.1992, 1.2503, 1.2338, 2.0691, 4.1616,\n                      1.1478, 0.9209, 1.5747, 1.5546, 1.2660, 1.0802, 1.5727, 2.0256, 2.5479,\n                      0.9358, 2.6995, 0.8999, 1.7345, 1.5601, 0.9534, 1.2478, 1.6060, 1.4017,\n                      1.5546, 0.9051, 0.9725, 1.3571, 1.2624, 1.5217, 1.4391, 1.3066],\n                     device='cuda:0')),\n             ('module.vgg.25.num_batches_tracked',\n              tensor(942776, device='cuda:0')),\n             ('module.layer1.0.weight',\n              tensor([[[[ 0.4887]],\n              \n                       [[-2.0006]],\n              \n                       [[ 0.9030]],\n              \n                       ...,\n              \n                       [[-0.5286]],\n              \n                       [[ 0.0752]],\n              \n                       [[ 0.1795]]],\n              \n              \n                      [[[ 1.0781]],\n              \n                       [[-0.5349]],\n              \n                       [[ 1.1220]],\n              \n                       ...,\n              \n                       [[-0.3025]],\n              \n                       [[ 2.5153]],\n              \n                       [[ 0.7680]]],\n              \n              \n                      [[[-1.5925]],\n              \n                       [[-0.5473]],\n              \n                       [[-1.3801]],\n              \n                       ...,\n              \n                       [[ 0.0886]],\n              \n                       [[-2.1178]],\n              \n                       [[-2.1647]]],\n              \n              \n                      ...,\n              \n              \n                      [[[ 0.5426]],\n              \n                       [[ 1.4333]],\n              \n                       [[-1.4828]],\n              \n                       ...,\n              \n                       [[-1.8241]],\n              \n                       [[ 2.1038]],\n              \n                       [[-0.7201]]],\n              \n              \n                      [[[ 1.4418]],\n              \n                       [[ 0.2209]],\n              \n                       [[-0.4296]],\n              \n                       ...,\n              \n                       [[-0.7204]],\n              \n                       [[-0.3177]],\n              \n                       [[ 1.8532]]],\n              \n              \n                      [[[-0.5224]],\n              \n                       [[-2.9974]],\n              \n                       [[-0.4963]],\n              \n                       ...,\n              \n                       [[-0.6215]],\n              \n                       [[-1.7051]],\n              \n                       [[ 0.1552]]]], device='cuda:0')),\n             ('module.layer1.0.bias',\n              tensor([-4.1606e-01,  3.9297e-01,  6.2066e-01, -3.5583e-01,  3.8794e-01,\n                      -4.2382e-01, -5.8891e-02,  3.0810e-01,  4.4706e-01, -4.1316e-01,\n                       2.0030e-01, -7.6007e-01, -1.1886e+00, -3.6227e-01,  1.9435e-01,\n                       1.1671e-01,  2.8010e-01,  4.2516e-01,  3.4364e-01, -1.5528e-01,\n                      -2.9934e-01, -4.7719e-01,  5.0062e-01, -9.7003e-02,  4.8031e-01,\n                      -5.6568e-01, -6.3507e-02,  4.4376e-01,  3.8007e-02,  1.8454e-01,\n                       5.0068e-01,  5.7446e-01, -8.1685e-02,  6.4478e-01,  1.2755e+00,\n                      -1.1738e-01, -2.3770e-01,  4.1324e-01,  5.2079e-01, -2.0481e-01,\n                      -1.9673e-01,  2.3627e-01,  7.1041e-01, -4.9889e-01,  3.2673e-01,\n                       5.9224e-02,  2.0581e-01, -3.3291e-03,  7.1563e-01,  5.1393e-01,\n                       4.0727e-02, -2.9584e-01,  2.4666e-01, -1.4724e-02, -5.6360e-01,\n                       1.1845e+00,  5.5757e-01,  8.8617e-01,  7.5789e-02, -3.4228e-01,\n                       1.9591e-01,  3.6539e-02, -8.8855e-01, -2.5108e-01,  6.2226e-01,\n                       4.1430e-01, -4.5924e-03, -2.3165e-01,  3.6215e-01, -4.4307e-01,\n                      -9.7805e-01,  2.9372e-01,  4.8236e-01, -2.4863e-01, -8.6428e-02,\n                       5.6613e-01,  3.3395e-01, -1.3771e-01,  2.8645e-01, -5.5910e-01,\n                      -4.9619e-01,  2.7205e-01,  2.7625e-02,  3.2088e-01, -4.8504e-01,\n                       2.2962e-01, -2.9214e-01, -4.0109e-01,  3.9808e-01,  1.6972e-01,\n                      -3.9573e-02,  3.4756e-01,  5.9568e-01,  9.6685e-02,  2.5653e-01,\n                      -1.5516e-01, -5.3438e-01,  3.3753e-01,  3.4342e-01,  1.1059e-01,\n                      -4.2983e-02, -3.3912e-01,  7.6628e-01,  2.3268e-01,  1.0200e-01,\n                      -4.9672e-01, -4.5810e-01, -8.9035e-01,  2.7995e-01,  2.1713e-01,\n                      -7.8035e-01,  2.9332e-01, -4.3858e-02, -8.9530e-01,  1.8379e-01,\n                      -7.1157e-02, -6.8987e-01, -4.1957e-02, -6.7297e-01,  7.9874e-02,\n                       2.6503e-01, -5.0371e-01,  6.0316e-01,  3.0634e-01,  1.6979e-01,\n                       7.4122e-01,  1.6065e-01,  5.3853e-02,  7.4159e-01,  2.2113e-01,\n                       4.0116e-01, -2.7041e-01,  4.1149e-01,  5.7814e-01, -5.5497e-01,\n                       8.8388e-01,  3.8367e-01,  2.7941e-01, -1.4485e-02, -4.7209e-01,\n                      -3.2314e-01, -4.7559e-02, -5.9029e-01,  1.1974e-02,  5.2335e-01,\n                      -5.3724e-01, -4.9564e-01,  6.4478e-01,  6.8809e-02,  3.2442e-01,\n                      -8.0308e-01,  3.8886e-01,  3.5043e-01,  6.0176e-01,  5.2512e-02,\n                       1.9605e-01, -7.1120e-02,  1.8333e-01, -5.1635e-01, -1.3760e-01,\n                      -1.5559e-01, -1.4089e-01, -6.8518e-01,  2.2375e-01,  5.4107e-01,\n                       1.9468e-01, -3.6922e-02,  6.5783e-01, -5.6109e-01, -2.5360e-01,\n                      -1.6978e+00,  7.7279e-02, -2.6376e-01, -3.1484e-01,  1.2137e-01,\n                       3.0827e-02, -3.4307e-01, -1.2555e+00, -6.2091e-01,  2.1825e-01,\n                       2.6572e-01, -2.7015e-01, -3.5660e-01,  2.0327e-01, -6.3299e-01,\n                       7.2728e-01, -7.0472e-02, -5.3498e-01,  1.2528e-01, -3.2020e-01,\n                      -1.8833e-02,  5.7402e-02,  8.2112e-01,  3.7492e-01,  3.4127e-01,\n                       2.1401e-01,  1.2929e-01,  3.3313e-02, -1.9916e-01, -5.4408e-01,\n                       2.4882e-01,  1.3344e-01,  2.0100e-01,  2.2965e-01,  7.5826e-01,\n                      -5.7011e-01, -2.6177e-01, -2.5152e-02,  4.7245e-01,  1.5134e-01,\n                      -2.1296e-01,  6.5144e-02,  2.1303e-01, -2.0916e-01,  2.2323e-01,\n                      -2.8054e-01,  4.3947e-01,  1.0420e-01, -1.0472e-01,  3.0041e-01,\n                       1.2722e-01, -7.3859e-01,  3.4090e-01,  2.7849e-01, -8.1165e-02,\n                      -2.5710e-01,  3.1873e-01, -5.9023e-01,  4.2317e-01, -1.2984e-01,\n                      -6.9461e-01,  7.8262e-01,  5.7992e-01, -8.7761e-01,  3.2816e-02,\n                      -2.7743e-02,  6.8974e-01,  3.8523e-01,  2.7310e-01,  2.3425e-01,\n                      -1.8041e-01,  2.4877e-02, -1.4867e-01,  5.0285e-01,  4.7306e-01,\n                       6.1378e-01, -3.1596e-01,  2.2640e-01, -9.1607e-02,  4.2648e-01,\n                       2.3960e-01,  3.7965e-01,  2.0152e-01, -4.9470e-01,  1.5772e-01,\n                      -3.2854e-01, -1.0279e+00, -4.1205e-01,  1.6534e-01, -2.4602e-01,\n                      -3.0604e-04,  4.3732e-01, -4.6682e-01,  1.1232e-01, -2.8183e-01,\n                      -2.9673e-01, -5.7454e-02,  3.3240e-01, -3.3069e-01,  1.3677e-01,\n                       1.3017e+00,  3.0750e-01, -7.2223e-01, -1.1231e-02,  2.1720e-02,\n                       2.5637e-01, -5.8758e-01, -3.3590e-03, -5.4469e-01, -2.5487e-01,\n                      -6.3454e-01,  6.2163e-01, -2.4024e-01,  2.9318e-01, -5.8117e-01,\n                      -1.3746e-01, -5.2093e-01, -2.8787e-01, -2.1581e-01, -3.1269e-01,\n                      -1.7411e-01, -3.3384e-02, -1.6283e-01, -3.3151e-01, -3.9045e-01,\n                      -3.2861e-01,  6.8333e-01,  8.5595e-01,  3.2493e-01, -4.8660e-02,\n                       2.9894e-01, -1.1409e-01, -5.0541e-01, -2.5134e-01, -2.8848e-01,\n                       4.3804e-01,  8.3592e-01,  1.4542e-01,  4.7953e-01, -6.8618e-01,\n                       8.1590e-02,  1.9027e-01, -4.5097e-01,  9.4783e-01, -1.6738e-01,\n                       2.0999e-01, -1.7660e-01,  1.3504e+00, -3.0800e-01, -2.5669e-01,\n                       8.3708e-02, -4.5393e-01, -6.9099e-01,  5.2644e-01, -2.2460e-02,\n                      -2.9957e-01,  4.9064e-01, -6.9745e-01,  8.5332e-01,  7.6768e-02,\n                      -1.3745e+00,  8.5640e-01, -3.6814e-01, -7.9325e-03,  7.8990e-01,\n                       4.6589e-01, -6.9638e-02, -1.4191e-01, -1.4737e-02, -4.9991e-01,\n                       3.6305e-01,  2.2519e-01,  2.1630e-02,  7.6418e-02, -4.0550e-01,\n                       2.4612e-01,  1.2506e-02, -1.0654e+00, -5.7259e-01, -6.0503e-01,\n                       3.4256e-01,  2.2089e-01,  3.4845e-01,  5.2545e-01,  5.4633e-01,\n                       1.6356e-01, -1.6749e-01, -5.4770e-01, -2.9796e-01,  2.8067e-01,\n                      -1.4662e-01,  5.3558e-01, -8.8893e-02,  9.7523e-01,  6.7119e-01,\n                       3.8585e-01,  1.7564e-01,  2.1079e-02, -3.4017e-01,  5.7587e-02,\n                      -1.9607e-01, -1.2184e-01,  5.3020e-03, -8.3936e-01, -1.5261e-01,\n                       8.1026e-01,  1.6544e-01, -5.9977e-01, -4.9999e-01,  1.6447e-01,\n                       4.5425e-01, -8.3261e-01, -3.2819e-01,  5.8414e-01, -9.0317e-01,\n                      -9.3614e-02, -3.5469e-01,  9.5689e-01,  4.1443e-01, -5.6141e-01,\n                      -4.9089e-02, -9.0704e-02, -4.7026e-01, -2.3501e-01,  5.8066e-01,\n                       5.2148e-01,  1.6321e-01,  3.6771e-01, -4.4258e-02, -4.8978e-04,\n                       6.3760e-01,  1.0219e-01, -5.1389e-01,  2.2376e-01,  2.0727e-01,\n                      -5.4445e-01, -1.0079e+00,  3.4982e-01, -6.6983e-02, -4.6664e-01,\n                       2.3005e-01, -6.4683e-01, -7.1812e-01,  6.3194e-01,  3.8073e-02,\n                      -2.3557e-02,  5.4596e-01, -1.6401e-01,  5.9541e-01, -5.8503e-01,\n                      -1.1377e+00,  4.9526e-01, -1.5421e-01, -6.3153e-03, -3.2983e-01,\n                       3.3742e-01, -2.1299e-01, -3.8968e-01, -1.0660e-01,  1.2084e-01,\n                      -1.4040e+00, -1.2217e+00, -7.6740e-01,  3.5414e-01,  5.5511e-01,\n                      -3.6162e-02,  4.3352e-01,  4.5822e-01,  5.3497e-02, -3.3507e-01,\n                       1.9235e-01,  2.8498e-01, -1.8725e-01, -1.0563e-01, -1.1882e-01,\n                       2.2790e-01, -4.9619e-01,  4.0663e-01,  3.5161e-01, -3.5397e-01,\n                       3.0034e-01,  2.1472e-01,  6.7882e-01, -3.2183e-01, -7.0015e-01,\n                       4.1259e-01, -7.1643e-01,  1.2952e+00, -2.6463e-01, -2.5716e-01,\n                      -7.3285e-01,  4.7648e-01,  8.0377e-01, -2.2303e-02,  2.2857e-01,\n                      -8.8659e-02,  9.6783e-01,  3.7183e-01,  2.3808e-01,  8.5385e-02,\n                      -5.1569e-01,  3.0054e-02, -7.4903e-02, -5.2774e-01, -4.6704e-01,\n                       7.2125e-01, -2.6398e-01,  4.6351e-02, -7.3489e-01, -3.9104e-01,\n                       1.0724e-01, -2.9697e-01, -3.7696e-01,  2.5761e-01, -3.6576e-01,\n                       5.0942e-01,  5.2582e-01, -4.3481e-01,  9.8987e-03, -5.0895e-01,\n                       2.1709e-01,  8.3334e-02, -1.9223e-01, -3.6545e-01, -3.0682e-01,\n                      -3.1738e-01,  1.0906e-01, -1.6627e-01,  5.4997e-02,  1.0626e-01,\n                      -2.8917e-01, -1.9104e-01,  1.5568e-01, -3.8677e-01, -1.1773e-02,\n                      -6.4313e-02,  6.0266e-01,  9.0629e-03,  8.1278e-01, -2.0416e-01,\n                       4.8018e-01,  6.7007e-02], device='cuda:0')),\n             ('module.layer1.1.weight',\n              tensor([ 3.4505,  1.0172,  3.2156,  2.8978,  3.5625,  2.0084,  4.2670,  2.3392,\n                       2.8897,  1.5910,  3.4178,  3.2179,  2.5428,  3.2986,  3.2561,  3.1126,\n                       2.0831,  2.7935,  3.4136,  3.5261,  1.7691,  2.9425,  1.8000,  4.2095,\n                       1.6086,  3.2594,  3.6472,  2.7353,  4.9720,  3.1302,  3.3494,  1.8737,\n                       0.8627,  2.5454,  2.9582,  2.3185,  3.7980,  3.1333,  3.2890,  1.3332,\n                       4.3916,  3.8540,  2.8417,  2.0060,  4.6561,  4.9639,  2.2645,  2.8996,\n                       4.0181,  1.7821,  1.8830,  3.9742,  3.0957,  1.2306,  3.0581,  3.8706,\n                       3.2246,  3.6727,  2.1327,  3.0226,  1.9381,  2.8539,  4.0465,  1.3505,\n                       2.5659,  2.1017,  3.3472,  3.0004,  3.4609,  2.8970,  2.4947,  2.0356,\n                       5.2104,  2.6772,  1.9445,  2.8640,  3.1565,  2.8393,  2.7573,  1.6700,\n                       3.5144,  3.7758,  4.9946,  2.9139,  4.7590,  1.8374,  3.8243,  2.8871,\n                       2.7440,  1.8275,  1.5760,  1.9714,  2.4626,  2.8184,  2.6661,  2.1304,\n                       2.4372,  2.2838,  2.4085,  2.7636,  3.8227,  2.1770,  4.0170,  2.5468,\n                       2.8855,  2.3015,  1.2468,  2.9357,  2.4872,  3.2992,  2.7710,  2.8518,\n                       4.2339,  3.4732,  1.7171,  2.3041,  4.0166,  3.3855,  2.0014,  2.1594,\n                       3.7807,  3.8442,  2.9245,  3.6134,  2.3195,  2.0701,  3.2000,  2.6871,\n                       3.5489,  2.4857,  2.9981,  1.3554,  2.7785,  2.2667,  3.8025,  2.6387,\n                       3.3929,  2.7868,  2.5680,  4.0903,  3.7023,  2.6351,  2.5503,  4.0487,\n                       2.8625,  1.6659,  3.4982,  3.5395,  3.4569,  2.6662,  4.3658,  3.2622,\n                       2.5907,  2.9332,  3.5570,  4.1710,  3.5034,  3.4065,  4.3904,  2.7206,\n                       1.7422,  2.7780,  4.0712,  2.7316,  1.7481,  2.2045,  1.8501,  2.5095,\n                       2.4571,  2.8103,  3.0784,  3.4479,  3.9166,  3.3894,  2.5319,  3.3941,\n                       3.2149,  2.2028,  1.9287,  5.9128,  3.3928,  1.3225,  3.0169,  2.8286,\n                       2.3319,  3.3963,  2.0604,  3.9706,  4.4769,  3.4785,  3.0021,  2.3111,\n                       2.9985,  3.9161,  3.5723,  2.8648,  3.4968,  3.5572,  2.6851,  2.7000,\n                       3.9080,  2.8369,  2.7469,  3.9735,  2.8172,  5.9764,  2.7857,  1.0702,\n                       3.6343,  3.4090,  3.5317,  1.7482,  2.1901,  3.1830,  4.0739,  2.1079,\n                       3.4362,  2.7640,  3.4147,  3.6405,  3.6959,  2.4408,  4.5236,  2.9691,\n                       3.4225,  0.8383,  2.9226,  2.6053,  3.8945,  2.4683,  1.7252,  2.1994,\n                       3.3620,  4.1000,  3.7480,  2.8023,  3.2007,  3.3101,  3.3309,  2.6329,\n                       3.4821,  3.0606,  2.4290,  4.1982,  3.0677,  2.8796,  2.1733,  2.2858,\n                       2.2965,  3.2176,  3.6201,  4.1157,  3.2419,  3.7493,  3.4430,  2.1229,\n                       2.7396,  3.0773,  2.1790,  3.3694,  2.2504,  2.5780,  4.3775,  3.2489,\n                       3.8911,  3.8561,  3.0744,  2.5079,  4.4835,  2.9190,  3.3358,  3.5935,\n                       2.4084,  5.8479,  3.9067,  3.6623,  1.8402,  2.2009,  2.2172,  3.1547,\n                       4.4565,  3.1682,  3.3923,  3.0867,  3.5138,  1.0566,  2.6353,  4.1441,\n                       3.2146,  2.9979,  2.2516,  2.9101,  3.4917,  2.6189,  3.5110,  1.7910,\n                       2.6854,  3.0268,  2.3849,  2.8959,  3.5580,  2.4757,  2.8805,  4.3532,\n                       2.9654,  4.1311,  3.1806,  2.9055,  3.2959,  3.8471,  1.8914,  3.4230,\n                       4.3234,  2.4081,  4.1159,  2.7499, -0.9860,  4.0839,  3.1687,  3.8591,\n                       3.6029,  3.2830,  3.6610,  4.2397,  3.6264,  2.9700,  4.3236,  3.9068,\n                       3.8464,  2.4720,  2.5827,  3.2759,  3.1077,  2.2489,  3.3201,  2.8365,\n                       3.3648,  3.5666,  4.4796,  3.6585,  2.9702,  2.4397,  3.0689,  2.7961,\n                       2.3048,  3.1943,  3.8117,  2.7891,  4.2781,  3.5401,  3.5758,  2.4127,\n                       3.2206,  3.0736,  3.3198,  2.2242,  2.4279,  2.9993,  3.7175,  2.0762,\n                       3.2409,  1.8082,  1.9873,  3.2251,  3.0561,  4.0858,  2.1024,  2.9257,\n                       4.6164,  2.7949,  3.3190,  4.2016,  1.9911,  2.6468,  2.8762,  2.5787,\n                       3.2788,  4.6103,  2.3943,  3.3362,  2.1834,  3.9939,  1.7843,  3.5158,\n                       2.3035,  2.9314,  2.6222,  4.2731,  1.1197,  2.2392,  2.5562,  2.6648,\n                       3.3926,  3.0688,  4.9646,  2.4712,  2.2821,  2.5267,  2.1071,  4.0405,\n                       2.9914,  2.4654,  3.6918,  3.1617,  2.4585,  3.4811,  1.8821,  2.9075,\n                       2.4103,  4.3507,  2.9541,  3.4398,  2.9505,  1.8065,  2.0843,  2.5223,\n                       2.0365,  3.6317,  1.6395,  2.5891,  3.5677,  3.1782,  3.3672,  3.9876,\n                       2.9979,  2.2027,  1.4484,  1.6505,  4.0362,  3.7192,  4.1765,  3.5374,\n                       4.5732,  3.1474,  2.2763,  2.9865,  4.1046,  2.9665,  2.7678,  2.2213,\n                       4.1007,  3.5011,  2.4196,  2.3580,  1.7752,  2.9949,  3.0078,  3.3226,\n                       2.7268,  2.6680,  2.9237,  4.1799,  3.1718,  4.3451,  3.5533,  2.7574,\n                       4.0306,  2.3086,  3.6779,  2.7056,  3.0794,  2.1119,  2.9843,  2.5834,\n                       2.2551,  2.1255,  3.9347,  3.5324,  4.2705,  3.3649,  2.1673,  3.2819,\n                       2.2432,  2.5206,  4.7895,  3.6933,  1.0961,  2.9973,  1.9409,  1.6223,\n                       2.9213,  3.6572,  2.6598,  4.1090,  1.7443,  3.1601,  3.5186,  2.7263,\n                       2.6761,  2.3684,  3.4812,  3.0624,  1.9784,  2.1797,  3.3431,  4.1641,\n                       2.2898,  3.9824,  4.7100,  3.5450,  2.0368,  3.5347,  3.3530,  4.5632,\n                       2.8426,  3.5059,  2.3685,  4.2860,  4.6986,  3.8968,  1.5455,  2.9170],\n                     device='cuda:0')),\n             ('module.layer1.1.bias',\n              tensor([-2.9469, -1.0280, -2.4264, -2.6128, -2.4011, -3.0381, -1.8530, -2.3283,\n                      -3.1467, -1.4231, -2.5205, -2.2156, -2.4098, -1.8759, -2.7939, -2.1191,\n                      -2.8366, -2.0946, -2.2381, -1.9961, -2.3018, -2.1216, -2.8860, -3.4839,\n                      -2.8167, -2.0031, -2.4010, -3.9711, -2.8665, -1.5575, -2.8941, -3.1340,\n                      -1.2322, -1.9291, -2.6658, -4.3922, -2.9078, -2.4785, -2.6791, -1.3102,\n                      -1.8065, -1.7649, -2.4486, -1.3364, -3.1079, -2.9250, -3.4408, -2.3344,\n                      -1.9989, -3.9161, -1.4057, -1.6722, -3.1875, -1.4469, -2.7592, -3.2935,\n                      -2.8421, -2.7620, -4.1927, -3.0527, -2.7866, -1.5611, -3.1222, -1.9286,\n                      -2.3298, -1.6718, -2.9451, -2.5503, -3.7301, -3.1212, -1.9994, -1.5368,\n                      -3.7491, -2.4126, -2.1731, -1.8586, -3.8328, -3.6975, -2.4948, -3.3821,\n                      -2.3923, -3.0474, -3.9565, -2.5031, -2.3287, -1.8960, -2.4501, -2.7177,\n                      -3.5646, -3.1885, -1.4081, -1.9556, -3.9335, -2.3092, -1.6386, -2.1342,\n                      -1.9133, -2.5256, -4.2186, -2.8087, -3.3666, -3.7045, -4.6111, -2.1821,\n                      -2.1427, -1.6745, -2.3280, -2.1284, -2.2876, -2.3042, -2.6742, -1.4649,\n                      -2.2934, -2.7978, -1.4977, -2.0647, -3.0825, -2.6805, -3.1541, -1.5825,\n                      -2.9633, -2.5433, -2.2183, -2.0270,  0.0984, -2.5329, -2.7255, -1.6902,\n                      -2.8507, -1.2529, -3.5412, -2.2694, -3.0360, -2.5083, -2.3736, -3.3045,\n                      -2.5786, -2.3480, -3.8338, -2.9587, -2.7009, -2.5559, -0.9646, -2.4416,\n                      -2.5122, -1.3606, -2.7245, -2.3500, -2.7752, -3.6734, -2.7341, -2.1537,\n                      -1.8641, -1.9493, -2.9771, -3.4825, -3.3073, -1.8167, -2.9440, -2.2533,\n                      -2.2091, -1.6241, -2.9791, -1.7023, -2.7487, -2.0763, -2.4852, -2.0553,\n                      -2.4444, -1.5614, -1.9383, -2.6817, -2.8274, -2.7321, -2.7410, -2.4096,\n                      -2.2379, -1.5377, -3.2352, -3.7124, -2.7464, -2.8346, -2.4146, -2.7167,\n                      -2.0769, -2.4802, -1.8142, -2.8735, -2.8069, -2.3636, -2.3764, -1.4673,\n                      -2.7324, -3.3385, -2.7836, -2.2149, -2.9850, -3.2532, -2.9390, -1.8572,\n                      -2.9341, -2.2490, -2.4384, -2.5954, -2.0270, -2.3939, -2.2066, -2.0403,\n                      -2.7769, -2.5380, -1.4390, -2.6621, -1.6132, -2.5759, -2.6462, -3.5335,\n                      -2.8208, -2.2774, -2.2744, -2.7017, -0.8588, -2.9633, -2.1948, -2.5299,\n                      -2.2125, -2.9591, -2.4155, -2.3861, -3.0973, -3.6402, -2.6650, -2.0978,\n                      -3.6082, -4.1842, -2.0423, -2.1820, -2.5189, -2.6947, -2.7986, -2.2440,\n                      -3.4269, -2.7347, -1.4576, -2.3878, -2.1258, -1.9276, -1.7957, -1.7352,\n                      -1.2212, -1.9916, -3.1002, -2.0067, -2.4288, -2.6541, -2.4777, -3.4263,\n                      -2.3640, -2.3431, -2.3544, -2.2720, -2.4778, -3.4994, -3.1119, -2.3333,\n                      -3.4451, -2.8019, -2.5635, -3.1534, -2.8540, -2.2247, -2.9573, -2.9703,\n                      -3.4145, -2.0991, -2.6604, -3.7028, -1.6849, -2.3841, -1.5490, -2.3087,\n                      -2.9811, -2.2899, -1.0626, -2.6900, -2.3509, -1.5682, -0.2613, -2.5818,\n                      -2.4485, -2.3906, -2.0644, -3.5085, -2.1955, -1.4489, -3.8532, -3.1344,\n                      -2.3056, -2.3855, -3.6015, -2.4330, -2.3873, -1.9659, -2.9713, -1.8833,\n                      -2.2555, -3.0619, -2.9464, -2.5896, -2.9471, -2.4801, -2.6704, -2.1594,\n                      -3.5252, -1.9829, -2.2723, -2.6426, -1.5230, -3.0648, -2.7096, -2.0027,\n                      -2.8352, -2.6945, -3.1142, -3.5072, -3.3287, -2.5943, -2.4759, -2.8989,\n                      -2.5228, -3.2638, -1.8460, -2.0004, -2.2918, -3.1409, -2.3213, -2.6712,\n                      -3.0072, -2.7623, -3.2176, -2.1710, -2.2121, -3.2523, -3.7441, -2.1221,\n                      -2.3609, -1.8969, -3.6688, -2.2808, -2.6740, -2.8912, -2.9875, -2.7266,\n                      -2.8255, -1.7240, -2.3432, -1.9238, -2.0999, -2.7756, -2.9204, -3.4425,\n                      -3.2799, -2.8198, -3.4226, -3.0734, -3.9224, -2.8650, -3.8459, -4.1216,\n                      -3.7907, -2.4794, -2.7030, -0.7766, -2.9369, -3.5981, -4.4198, -2.2755,\n                      -2.6287, -3.7419, -1.6426, -2.5335, -2.7510, -2.7930, -1.6128, -3.7736,\n                      -2.2563, -2.6352, -2.0259, -3.5705, -0.8791, -2.0074, -1.8960, -1.8377,\n                      -4.8880, -1.9654, -2.8637, -2.1339, -2.4466, -3.0533, -2.2091, -3.1179,\n                      -2.5294, -4.0507, -3.1307, -2.7777, -1.9203, -2.3786, -3.1278, -2.0465,\n                      -3.5349, -2.9435, -2.7596, -2.5701, -2.3760, -0.9651, -2.5286, -1.9921,\n                      -3.4707, -2.8188, -1.4054, -2.8547, -2.5768, -2.4274, -2.1862, -2.9816,\n                      -2.7238, -3.6912, -3.0531, -2.3909, -3.0681, -2.2244, -3.2991, -3.1057,\n                      -2.4427, -2.6681, -1.9086, -2.1454, -2.4285, -2.1737, -1.9810, -3.0964,\n                      -3.5007, -3.8310, -2.3076, -3.7252, -3.0174, -4.8863, -4.7530, -2.7559,\n                      -2.3179, -3.2952, -2.6662, -2.5062, -2.7539, -2.8555, -2.4736, -2.5330,\n                      -2.5480, -2.9916, -3.1119, -2.0385, -1.9227, -2.5518, -2.0868, -2.3333,\n                      -2.6577, -1.3182, -3.1363, -2.5597, -2.9447, -2.0662, -1.3907, -2.4168,\n                      -1.6139, -4.1925, -1.5385, -2.9755, -1.6289, -1.3706, -1.8698, -2.7165,\n                      -2.4375, -3.0380, -2.0369, -3.2207, -1.8362, -2.2300, -3.7393, -2.9660,\n                      -1.8909, -1.7156, -2.8629, -2.0778, -2.8291, -2.7254, -2.6254, -2.6826,\n                      -1.9126, -2.1035, -1.8240, -2.4983, -1.5575, -2.3216, -2.7595, -2.6008,\n                      -2.7969, -2.7062, -1.6107, -3.2113, -2.9596, -3.1003, -3.5699, -2.4098],\n                     device='cuda:0')),\n             ('module.layer1.1.running_mean',\n              tensor([-10.4428, -25.1264, -29.2716, -29.7208,  11.6901, -41.6141,  -0.2505,\n                      -16.8552, -42.0625, -23.1559, -15.3549, -26.9391, -31.4268, -17.8500,\n                      -15.4561, -17.0668, -21.1386, -34.0555, -20.3583, -27.3789, -33.5323,\n                      -36.2160, -15.1426, -14.2319,  -7.2010, -17.4062, -32.6374, -30.2262,\n                        3.0019, -15.2794,  -5.1366, -43.4627, -92.5971, -14.0511, -11.6010,\n                      -54.3088, -19.2031, -18.8897, -23.7187, -21.8713, -10.5117,   9.2206,\n                      -31.2574, -49.9474,   7.2071,  -2.8674, -43.4169, -33.5046, -15.3194,\n                      -14.7262, -32.6167,   6.9529, -13.9751, -14.5053, -27.4793, -10.2115,\n                      -15.8238, -13.6978, -34.3766,   0.5145, -14.5802, -29.0784, -26.4853,\n                      -21.9039,   0.5212, -22.2958, -38.6061, -24.4756, -34.4308, -15.1682,\n                      -23.3596,   2.9827, -16.2115, -45.5848, -27.5755,  10.6871, -26.0379,\n                      -32.1112, -38.9687, -38.3594, -20.3304,  -1.2922,  -9.6721, -23.4629,\n                      -10.9435, -18.1078, -23.6324, -22.5765, -16.1623, -35.0218,  23.7754,\n                      -22.1853, -26.7931, -17.8252, -17.8455, -25.9516, -22.4462, -45.4824,\n                      -33.5876, -13.2306, -25.3302, -29.4231,  -6.9291, -22.4125, -14.1551,\n                      -24.9380, -15.2289, -10.3510, -20.4210, -17.5489, -12.7645, -20.9538,\n                      -11.0347, -31.4821, -12.6837, -19.7285,  -9.9916, -23.2763, -26.8817,\n                      -22.3632, -32.5328, -12.3634,   1.4112, -11.2445, -87.9746, -30.7962,\n                      -23.6876,   2.4120, -43.8766, -13.2307, -16.5182, -28.7592, -16.8321,\n                      -14.5569, -14.3071, -32.9269, -25.0541, -30.9247, -42.4054, -23.5723,\n                        5.4536,  -4.3650, -10.4289, -22.8864, -11.0155, -28.8801, -25.4268,\n                      -43.8740,  -1.5009, -45.6136, -36.9770, -28.4199,  -8.5923, -41.8901,\n                      -19.3596, -18.2844, -17.2989, -46.0938, -19.3119,  -0.2754, -36.3557,\n                      -40.7948,   4.0504,  -0.1482, -29.2260, -27.5500, -25.2705, -10.6660,\n                      -40.6769, -21.0888,  -0.7911, -18.0296,  -9.3019, -34.3658, -30.1795,\n                      -31.9922, -15.0041, -21.4531, -30.5314,   5.1751, -34.5011, -13.7780,\n                      -23.7186, -29.8291, -16.3957, -21.4760, -19.4680, -11.3905, -15.0503,\n                      -16.4324, -14.2104,   2.0505, -18.3790, -26.6806,  -9.4699, -11.4959,\n                       -6.0270,  -8.2630, -32.1038, -22.2760, -17.1647,   4.2079, -17.5466,\n                      -27.7404, -33.0907,  21.7861,  -8.0482,   1.6187, -24.2478, -20.6471,\n                      -42.0086, -34.1026, -18.2168,  -1.2288,  -0.7054, -47.3451, -11.7029,\n                      -24.1984, -32.5869,  -9.0971,  -2.2393, -23.6910, -17.0028, -36.0429,\n                      -25.7095, -29.3145,   4.0145, -11.0441, -27.2985, -12.4661, -17.9199,\n                      -27.6737, -28.7026, -20.4877,  -4.9630, -10.5733, -10.1071, -27.1610,\n                       -5.7805, -26.5542, -34.2929, -38.2589,  -5.3532, -20.6161, -23.6478,\n                      -17.9705, -30.0969,  15.5018,  -3.6742, -28.8905, -16.0381,  13.1115,\n                      -23.0195, -18.8379, -30.5860, -31.1526, -26.7237,  -7.9255, -43.3679,\n                      -24.0050, -18.2991, -29.5383, -29.8829, -13.8211, -10.8333, -11.7344,\n                      -18.3083, -28.6906, -20.0056, -20.4762,   2.6275,   3.0204, -21.3860,\n                      -11.4222, -12.9857,  -9.5205, -19.0739, -22.7461, -44.2832, -32.8590,\n                      -13.9422, -30.4931, -22.3796, -22.1754, -49.4873, -63.1192,   3.2573,\n                      -14.0102, -18.2005,  -6.6101, -28.8167, -23.1248,   3.3786, -30.8976,\n                      -23.1863, -30.4615, -18.9287, -14.3515, -27.7197, -23.6456, -35.0137,\n                       -4.8090, -18.3606, -39.3770, -16.5930, -17.3243, -29.1410, -15.4657,\n                      -11.9621, -27.6553, -33.6072, -23.1948, -12.6985, -28.3843, -21.5429,\n                      -12.5474, -11.5369,  -0.9377, -22.7092,  14.9458,  -4.0656, -21.2208,\n                      -12.0433, -29.4818, -30.7578, -15.8605,   1.5254, -19.4772, -10.7569,\n                      -25.4511, -11.2910,  -8.0481, -11.1268, -43.8543, -45.2654, -15.3580,\n                       -7.6250, -39.1409, -22.2206, -21.3592,   7.9967, -41.3661, -21.8836,\n                      -24.9695, -22.5199, -14.9829, -23.1485, -21.3980, -35.0645,  -7.3023,\n                      -31.3318, -23.7607, -22.3631, -18.9145,  -6.4856, -16.1085,   0.2751,\n                      -25.1908, -16.2038, -46.3892, -27.4541, -28.3736, -31.5943, -23.0347,\n                      -32.1343, -27.2019, -40.0998, -26.0774, -14.7035,   1.4083, -22.1925,\n                      -19.7051, -18.1288, -24.7555, -28.8014, -31.4258, -29.3100,  -4.8971,\n                      -18.4484, -13.2673, -15.0279,  -5.6833,  -3.9880, -20.2799, -24.6721,\n                      -41.1174, -28.0020, -17.7440, -29.5154, -23.3438, -10.6578, -19.4330,\n                      -24.4464, -15.0290,  20.0246, -10.1145, -26.8727, -45.9618, -54.5273,\n                      -22.7843, -27.2147, -21.7310, -20.5492, -19.3357, -24.7565, -44.6408,\n                      -15.5101, -43.1505, -18.1028, -27.9403, -26.4897, -15.4475, -21.0786,\n                      -11.3063, -25.1532, -20.5319, -33.6525, -23.3927, -19.3138, -39.0319,\n                      -23.3231, -23.1045, -33.3654, -15.5165, -20.1931, -37.2881,  -6.9305,\n                      -35.0712, -43.1548, -19.5173, -11.6958, -15.8517,   2.3928, -23.0704,\n                      -24.0132, -16.0041,  -8.6860, -31.0647, -22.7447, -38.2611, -19.2005,\n                      -17.3056, -35.6581, -23.8731, -27.8928, -23.3744, -38.6276, -29.8771,\n                       -5.5033,  -3.8055, -26.4706, -14.8443, -23.7112, -22.1053, -22.6266,\n                      -33.9665,  -1.9694, -17.5608, -11.0188, -24.1890,   2.8288, -22.9971,\n                       -6.7684, -37.7522, -32.6992,  -3.8050, -14.5303, -26.1370, -37.1765,\n                      -27.4748,  -2.9973,  -3.9367, -25.5146, -36.2795,  11.4469,  -1.5947,\n                      -16.4057, -32.7776, -17.9335, -36.3809, -20.4294, -15.0401, -40.1903,\n                      -19.4167, -46.4516, -13.0447, -34.1993, -12.8156, -14.3663, -20.6459,\n                      -30.6174, -21.2992, -32.3683, -17.1349, -14.7337,  -6.0507, -17.7605,\n                      -42.5506, -10.3145, -31.4210, -12.8141, -39.5749,   1.6170,  -2.0419,\n                      -23.5075, -29.6729,  -3.1117, -19.1918, -15.1836,  -9.8219, -14.2572,\n                      -31.7901], device='cuda:0')),\n             ('module.layer1.1.running_var',\n              tensor([  384.0808,   153.0184,   464.0530,   407.7739,   432.4768,   766.8857,\n                        790.5023,   338.1289,   288.8529,   235.0119,   563.8464,   458.9013,\n                        279.1921,   623.5306,   313.8439,   434.5729,   197.6038,   360.7010,\n                        411.3786,   596.4634,   207.8530,   646.4375,   193.3215,   354.8870,\n                        211.0912,   429.8738,   744.2427,   228.6990,   609.7998,   580.7001,\n                        274.4208,   249.9517, 11690.9580,   446.0007,   302.6090,   198.4122,\n                        386.5358,   317.5574,   501.1498,   168.5703,   622.2388,   802.6478,\n                        337.7914,   471.0075,   520.9804,   776.7678,   217.4133,   354.7948,\n                        918.0082,   168.5085,   504.8812,   528.0453,   292.0965,    82.4847,\n                        519.9318,   405.7573,   388.3041,   464.8193,   188.8898,   349.3549,\n                        251.8530,   548.6792,   551.1536,   127.7316,   469.6893,   403.7307,\n                        662.7427,   337.8720,   293.5962,   255.9033,   256.4123,   272.0939,\n                        664.7073,   388.8629,   241.2825,   546.3469,   284.8479,   283.1297,\n                        410.9687,   163.7030,   707.1469,   435.0328,   484.2281,   314.9264,\n                        955.2161,  2265.3362,   957.5558,   268.1819,   240.3125,   193.8457,\n                       1396.0245,   181.1322,   271.5757,   259.5104,   352.1110,   284.3025,\n                        369.9021,   321.1230,   195.2549,   199.5656,   410.1047,   234.7242,\n                        234.5864,   300.8643,   356.9805,   387.8650,   247.9172,   468.9450,\n                        247.2080,   445.6179,   315.7104,   717.1782,   591.9473,   632.7370,\n                        219.3603,   295.3204,   314.5136,   405.0539,   193.7542,   375.4760,\n                        548.8865,   430.3260,   368.1123,   592.6743,  5471.7539,   331.6538,\n                        352.4372,   333.5441,   553.4036,   458.7363,   351.8775,   149.1811,\n                        261.8245,   250.3406,   397.2125,   233.4314,   492.3030,   363.9026,\n                        234.8374,   584.8453,   420.4472,   270.0905,   444.1826,   556.4763,\n                        378.6757,   145.0787,   457.7943,   909.5804,   347.5152,   453.1577,\n                        968.8436,   456.1336,   538.5270,   657.0177,   556.0762,   510.6531,\n                        342.6087,   753.7171,   525.5374,   243.6335,   299.6305,   474.2096,\n                        540.6723,   395.9632,   232.0472,   310.4009,   201.0556,   260.9784,\n                        352.1931,   590.6339,   453.0006,   486.8774,   368.7264,   499.7223,\n                        330.4382,   719.0470,   774.5752,   323.7404,   251.9257,   628.3307,\n                        569.8766,   109.9579,   332.6097,   476.3085,   386.1132,   497.5200,\n                        334.2550,   485.0309,   415.9738,   586.4153,   474.2144,   380.6458,\n                        291.1877,   413.8781,   353.8276,   403.7578,   318.4051,   388.8079,\n                        272.5811,   441.9003,   483.4957,   400.2475,   325.3949,   555.2126,\n                        483.7649,   613.9986,   465.6791,    98.3630,   438.9210,   438.4164,\n                       2348.4368,   178.7785,   258.4602,   407.5888,   402.6450,   233.1184,\n                        484.1489,   237.4661,   662.6658,   516.7814,   494.7317,   284.8217,\n                        748.0857,   355.3119,   567.0313,   343.8058,   266.4506,   327.2953,\n                        399.1928,   228.4121,   194.7620,   221.6109,   297.5310,   348.2489,\n                        903.7021,   358.2349,   306.5252,   257.5054,   470.5851,   321.4765,\n                        312.1017,   489.8719,   317.4213,   813.5155,   401.3210,   428.0058,\n                        359.8412,   287.6802,   405.8770,   683.6218,   407.6683,   461.6391,\n                        343.9122,   418.0379,   461.3590,   183.5636,   371.9659,   401.9433,\n                        281.9213,   572.9038,   188.9552,   288.8147,   550.4024,   482.6155,\n                        490.6311,   443.6316,   399.7360,   227.7715,   902.3892,   382.0996,\n                        278.6336,   341.1986,   163.6678,  1327.7961,   580.5453,   346.4499,\n                        252.7261,   287.3570,   682.1999,   726.3818,   488.7236,   565.8592,\n                        818.4830,   433.7445,   957.0067,  4430.4639,   523.5357,   785.8300,\n                        300.5521,   223.5162,   292.9511,   182.6830,   547.3727,  1429.7283,\n                        352.0040,   155.0512,   390.3568,   384.5034,   207.0959,   317.8155,\n                        645.5477,   312.6179,   240.9629,  1361.6357,   335.3587,   624.1590,\n                        431.3109,   288.5676,   306.6469,   667.2802,   222.6443,   537.8131,\n                        365.5829,   422.5509,   567.0977,   290.6834,    61.0199,   364.6299,\n                        293.3695,   578.7110,   379.3885,   489.4363,   322.2406,   595.7217,\n                        334.7874,   317.7387,   524.5765,   606.1996,   462.7573,   266.0273,\n                        393.6835,   338.5976,   314.5906,   249.2260,   765.1855,   319.3394,\n                        342.4514,   747.2261,   571.0714,   800.1067,   360.8211,   324.7561,\n                        222.3938,   396.4037,   287.4834,   544.1755,   390.9736,   355.1574,\n                        922.0015,   561.3314,   345.2039,   289.5182,   482.8860,   411.6524,\n                        429.3330,   359.6120,   294.1702,   377.3509,   425.0302,   232.2985,\n                        320.8855,   212.9632,   308.6181,   343.5127,   371.3881,   616.8219,\n                        184.7988,   239.8320,   394.6577,   289.6030,   453.9789,  1053.0684,\n                        216.7811,   236.5347,   233.1607,   364.9523,   385.4209,   253.1017,\n                        405.8849,   414.6630,   192.2293,   433.6334,   200.1064,   331.3320,\n                        294.8630,   472.1909,   376.9494,   328.0401,   175.7473,   240.1879,\n                        369.1393,   421.2574,   214.9942,   466.5652,   454.5812,   265.4633,\n                        287.4756,   357.3037,   383.8936,   518.7931,   381.3394,   205.3232,\n                        372.9657,   507.7945,   276.2546,   903.5245,   247.9955,   612.6645,\n                        306.8838,   707.9718,   450.8504,   394.7308,   307.5833,   365.2735,\n                        230.4887,   421.6861,   202.6669,   511.3620,   236.1184,   308.3277,\n                        466.0917,   517.1035,   611.1174,   577.2103,   332.2851,   304.3817,\n                        199.3244,   183.7456,   578.9583,   753.5228,   382.0143,   301.7545,\n                        570.4512,   444.8022,   271.1925,   373.3451,   729.4587,   494.9827,\n                        583.1721,   240.5227,   375.6824,   357.3631,   349.2070,   205.3118,\n                        227.4934,   185.9591,   212.0437,   364.0453,   261.8418,   278.2628,\n                        369.4364,   509.7852,   399.3061,   562.8188,   491.3230,   372.5799,\n                        451.3451,   235.5800,   465.7563,   438.1652,   491.0586,   222.1680,\n                        440.9193,   441.8265,   245.5322,   796.8062,   472.9410,   432.9764,\n                        666.0534,   675.9594,   378.6325,   363.6741,   464.7885,   346.7985,\n                        826.5706,   484.1694,    81.8486,  1768.1001,   210.5477,   329.0284,\n                        449.6754,   451.5811,   546.7238,   636.2308,   230.5666,   512.9547,\n                        302.5553,   334.6672,   366.6764,   294.0149,   377.5423,   397.0439,\n                        219.8486,   318.8091,   383.6508,   427.4285,   229.1961,   807.0539,\n                       1119.6368,   510.0027,   253.6392,   702.0932,   368.0678,   402.6942,\n                        297.7346,   444.1829,   348.1441,   480.8118,   791.0246,   370.2216,\n                        165.7770,   398.0129], device='cuda:0')),\n             ('module.layer1.1.num_batches_tracked',\n              tensor(942776, device='cuda:0')),\n             ('module.layer2.0.weight',\n              tensor([[[[ 5.8823e-01,  6.9700e-03, -1.5640e+00],\n                        [-2.0581e+00, -3.2517e-01,  1.7164e-01],\n                        [ 2.2711e-01,  1.3228e+00,  1.5159e+00]],\n              \n                       [[-6.4805e-01,  5.6394e-01,  8.6547e-01],\n                        [-2.8180e+00, -3.5665e-01,  3.5595e-01],\n                        [ 5.0524e-02,  1.3579e+00,  6.8263e-01]],\n              \n                       [[ 2.8357e-01, -1.8802e-01, -9.1418e-01],\n                        [ 1.1087e+00,  2.6568e-01, -9.4544e-01],\n                        [ 3.9670e-01,  3.9660e-01,  4.7288e-01]],\n              \n                       ...,\n              \n                       [[-6.6285e-01,  1.9414e-01,  9.9143e-01],\n                        [-1.9621e+00,  6.0496e-01, -1.9326e-01],\n                        [-1.4308e+00,  2.6251e-01, -5.9138e-01]],\n              \n                       [[ 9.9480e-01, -1.7146e-01, -6.7314e-01],\n                        [-5.3160e-02, -1.8176e+00, -1.3858e+00],\n                        [-3.4475e-01, -4.1287e-01,  5.0207e-01]],\n              \n                       [[ 3.7111e-01,  2.1184e-01,  4.7297e-01],\n                        [ 9.5594e-02,  5.7513e-01,  1.1377e+00],\n                        [ 6.4238e-01,  6.8794e-01,  7.9016e-01]]],\n              \n              \n                      [[[-2.4327e+00,  4.4040e-01,  8.0279e-01],\n                        [ 6.6099e-01,  1.4152e-03,  1.8745e-01],\n                        [ 1.1394e+00,  1.6482e-01, -6.1353e-01]],\n              \n                       [[ 1.3497e+00,  8.4636e-01, -1.9826e-01],\n                        [ 1.0315e+00, -9.5846e-02,  1.0526e+00],\n                        [-6.3879e-01, -1.9169e-01, -6.9479e-01]],\n              \n                       [[ 9.2606e-01,  1.6476e-01,  4.0542e-01],\n                        [ 1.3379e+00,  7.1226e-01,  9.5617e-01],\n                        [ 2.5647e-01,  8.7609e-01,  1.1722e+00]],\n              \n                       ...,\n              \n                       [[-7.4802e-01, -4.2633e-01, -2.7387e-01],\n                        [-1.8025e+00,  1.4774e-01,  3.0653e-01],\n                        [ 1.0195e+00, -5.2942e-01,  2.3807e-01]],\n              \n                       [[ 5.2256e-01,  7.1535e-01,  5.3892e-01],\n                        [-2.3367e-02, -1.2435e+00, -1.7805e-01],\n                        [-1.0747e+00, -9.4347e-01,  1.6841e-01]],\n              \n                       [[ 3.7157e-01, -4.7517e-01, -1.2899e+00],\n                        [-6.7145e-01, -1.4263e+00, -2.6403e+00],\n                        [-8.8023e-01, -1.2762e+00, -2.8221e+00]]],\n              \n              \n                      [[[-8.7724e-01, -6.8941e-01, -4.9273e-01],\n                        [-1.1720e+00, -1.9169e+00,  1.0741e+00],\n                        [-2.1099e+00, -7.6755e-01,  6.0467e-01]],\n              \n                       [[-4.6669e-01, -6.2296e-02,  5.0138e-02],\n                        [ 2.8094e-01, -6.5009e-02, -2.6434e+00],\n                        [ 6.4571e-01, -1.5298e-01, -6.3029e-01]],\n              \n                       [[-1.4228e+00, -1.6932e+00, -8.1748e-01],\n                        [-7.0911e-01, -4.3656e-02,  1.3372e-01],\n                        [ 1.0183e+00, -5.1407e-01,  3.1039e-01]],\n              \n                       ...,\n              \n                       [[ 8.8358e-01,  1.7775e+00,  7.7719e-02],\n                        [ 8.1004e-01,  6.1077e-01,  1.4120e+00],\n                        [-5.9086e-01, -4.5956e-01, -2.3194e+00]],\n              \n                       [[-9.9575e-01, -8.6523e-01,  7.8618e-01],\n                        [ 4.4696e-01, -2.1867e-01,  1.8773e+00],\n                        [-5.1056e-01,  5.6321e-01, -4.4060e-01]],\n              \n                       [[ 7.4821e-01,  6.3303e-01,  4.1200e-01],\n                        [ 3.9164e-01,  8.0814e-01,  9.0512e-01],\n                        [ 4.6582e-01,  2.0803e-01, -2.1067e-01]]],\n              \n              \n                      ...,\n              \n              \n                      [[[ 3.1959e-01, -2.8759e+00, -1.6771e+00],\n                        [-1.9959e-01, -8.2574e-01,  1.9666e-01],\n                        [-1.4583e+00, -2.5405e-01,  1.9299e-01]],\n              \n                       [[-3.0578e-01,  5.5183e-01,  5.5529e-01],\n                        [-1.0382e-01, -1.0607e+00, -7.5232e-01],\n                        [ 6.3816e-02, -1.5116e+00,  7.0012e-01]],\n              \n                       [[ 6.5770e-01,  6.7117e-01,  1.4390e+00],\n                        [-3.1776e-01,  3.4478e-01,  1.0052e+00],\n                        [ 5.0491e-01, -2.6085e-01,  6.0396e-01]],\n              \n                       ...,\n              \n                       [[-4.2140e-01, -6.9259e-01,  4.2802e-01],\n                        [ 5.6818e-01,  5.8089e-01,  2.6231e-01],\n                        [-8.3674e-01, -1.4228e+00,  2.4606e-01]],\n              \n                       [[-1.1150e+00, -4.2815e-01, -2.0949e+00],\n                        [-1.5820e+00, -4.0544e-01, -1.7624e+00],\n                        [-3.3749e+00, -1.2309e+00,  1.6759e-01]],\n              \n                       [[-1.0083e+00, -2.0154e+00, -9.6954e-01],\n                        [-1.0956e+00, -5.3836e-01,  1.0400e+00],\n                        [-1.6646e+00, -1.5446e+00,  1.4288e+00]]],\n              \n              \n                      [[[ 2.2728e-01,  8.7663e-01, -4.2329e-01],\n                        [ 7.3232e-01,  4.7047e-01, -6.3877e-01],\n                        [-8.0913e-01, -8.6017e-01,  2.0868e-01]],\n              \n                       [[-1.0593e+00, -9.5743e-01, -2.5500e+00],\n                        [-4.0068e-01, -7.0860e-02, -5.4333e-01],\n                        [ 1.5948e+00,  1.0317e+00,  1.2638e+00]],\n              \n                       [[ 5.2634e-01,  3.4709e-01, -6.9023e-01],\n                        [-5.4627e-01,  1.7421e-01, -2.1320e-02],\n                        [-7.9857e-01, -3.7931e-01,  6.6053e-02]],\n              \n                       ...,\n              \n                       [[ 5.3861e-01,  8.9784e-01, -3.3822e-01],\n                        [-2.3012e+00, -2.9392e-01,  1.6920e-01],\n                        [-1.3856e+00, -1.3664e+00,  1.5202e+00]],\n              \n                       [[-8.0592e-01, -3.9365e-01, -1.2353e+00],\n                        [ 5.2657e-01,  7.3698e-01, -3.2658e-01],\n                        [-2.0396e+00,  4.2424e-01,  5.1539e-01]],\n              \n                       [[-1.3904e+00, -1.0057e+00, -7.3706e-01],\n                        [-5.9947e-01, -3.3176e-01, -5.1143e-01],\n                        [ 2.2924e-01,  1.9689e+00,  1.6259e+00]]],\n              \n              \n                      [[[-1.2517e+00, -7.6191e-01,  2.0752e-01],\n                        [ 3.1102e-02,  8.4660e-01,  2.4923e-01],\n                        [ 1.3958e-01,  7.0633e-01, -8.9239e-01]],\n              \n                       [[ 1.5036e+00, -2.6065e-01,  4.2989e-01],\n                        [ 7.4701e-01, -1.2326e-01,  2.1366e-01],\n                        [ 8.5217e-01,  2.4976e-01,  1.3030e+00]],\n              \n                       [[ 3.9045e-01, -9.6880e-01,  5.2155e-01],\n                        [ 6.6494e-01, -4.9934e-02, -1.3928e+00],\n                        [ 1.5108e-01, -9.1472e-02, -2.4323e+00]],\n              \n                       ...,\n              \n                       [[-2.8690e-01, -1.0813e+00,  1.0410e+00],\n                        [-9.7459e-01, -9.1425e-01,  5.8976e-01],\n                        [ 3.6247e-01,  9.5694e-01, -5.8201e-01]],\n              \n                       [[-1.4291e+00,  1.2816e-01, -2.4115e-01],\n                        [ 3.3543e-02,  4.8100e-01,  1.3957e+00],\n                        [ 2.0003e-02, -8.0223e-01, -1.1851e+00]],\n              \n                       [[ 3.2632e-01, -4.9314e-01,  4.5177e-01],\n                        [-6.2636e-01,  6.3628e-01, -3.3901e-01],\n                        [-1.5584e-01, -8.6183e-01,  1.6602e-01]]]], device='cuda:0')),\n             ('module.layer2.0.bias',\n              tensor([-0.1272,  0.0436, -0.0975,  0.0654,  0.0929, -0.0098, -0.0708, -0.0707,\n                       0.1134, -0.0656,  0.1861,  0.0688,  0.0072,  0.1425, -0.0212, -0.0562,\n                      -0.0517, -0.0478, -0.0636, -0.0793, -0.0065, -0.2623, -0.0970, -0.1842,\n                      -0.0393, -0.2248,  0.0295, -0.1921, -0.0347, -0.0781, -0.0303, -0.1982,\n                       0.1229, -0.0570,  0.1343,  0.0903,  0.0963,  0.0501,  0.0150,  0.0726,\n                       0.0209,  0.0406, -0.0450, -0.1023, -0.1490,  0.0355, -0.0486, -0.0465,\n                       0.0222,  0.0107, -0.0010, -0.0531,  0.0771,  0.1524,  0.1248,  0.0341,\n                      -0.0077,  0.1156, -0.0123,  0.0397, -0.0209, -0.2306,  0.0038, -0.0033,\n                      -0.0010,  0.1621, -0.1291, -0.0536,  0.1026, -0.1520, -0.0155,  0.0313,\n                      -0.0427,  0.0053, -0.0248, -0.0723, -0.0179, -0.0349, -0.1077, -0.0497,\n                      -0.0200, -0.0088, -0.0717, -0.0228,  0.0686, -0.0484,  0.0423,  0.0121,\n                       0.0419,  0.0201, -0.0142, -0.1963,  0.0584,  0.0673, -0.0030,  0.0333,\n                       0.0453,  0.0272, -0.0939,  0.0392,  0.0485,  0.0576, -0.0202, -0.0416,\n                      -0.0959, -0.0159, -0.1031,  0.0359,  0.0857,  0.0336, -0.0186, -0.0333,\n                       0.0746, -0.0029,  0.0311, -0.0031,  0.1921, -0.0307, -0.0861, -0.1323,\n                       0.1684,  0.0572,  0.2587, -0.0625, -0.1969, -0.0283,  0.0061, -0.0229,\n                      -0.0452, -0.0090,  0.0800, -0.0054,  0.0368, -0.0527, -0.0543,  0.0801,\n                       0.0450, -0.1252,  0.0344, -0.0117, -0.0466, -0.0095,  0.1565, -0.1470,\n                      -0.0812, -0.0650,  0.0205,  0.0405, -0.1325,  0.1193,  0.0120, -0.1030,\n                       0.1022,  0.2056, -0.1888, -0.0297, -0.1045,  0.1115,  0.1435, -0.2216,\n                       0.1166, -0.0212,  0.0179,  0.1136,  0.0494,  0.0023,  0.1012, -0.0447,\n                      -0.0554,  0.0060, -0.0094,  0.1695, -0.0407, -0.1412, -0.0134, -0.0578,\n                      -0.0432, -0.0884,  0.0309, -0.0560, -0.0678, -0.0184,  0.0590,  0.0051,\n                       0.0431,  0.2479,  0.0068,  0.0431,  0.0552, -0.0549,  0.0960,  0.0165,\n                       0.0274,  0.0054, -0.0368, -0.0090,  0.0208, -0.0912,  0.0286,  0.1472,\n                       0.0655,  0.0035, -0.0613,  0.0428, -0.0636, -0.0106, -0.0503,  0.0520,\n                      -0.1937, -0.1057,  0.0528, -0.0309,  0.1342,  0.0230, -0.0468,  0.0223,\n                       0.0981,  0.0085,  0.0094, -0.0236, -0.0693, -0.0201,  0.0745, -0.0529,\n                      -0.0947,  0.0305,  0.0508, -0.1091, -0.0803,  0.0514,  0.0025,  0.0836,\n                       0.0963,  0.1636, -0.1497,  0.0059,  0.1734, -0.1152,  0.0567,  0.0196,\n                       0.0783, -0.1278, -0.0558, -0.1228, -0.0082, -0.0463,  0.0318,  0.0646,\n                      -0.0670, -0.0897,  0.0280, -0.1705, -0.0432,  0.0433, -0.0647, -0.0426],\n                     device='cuda:0')),\n             ('module.layer2.1.weight',\n              tensor([2.4364, 1.5463, 0.6766, 1.4270, 2.0405, 1.8230, 1.5389, 1.5052, 0.8985,\n                      1.6454, 1.2790, 1.1974, 1.3988, 1.6609, 1.3193, 1.9732, 1.1679, 1.0869,\n                      1.9142, 1.1501, 1.5525, 0.6560, 1.6244, 1.5465, 1.9543, 2.1403, 1.2127,\n                      2.0186, 0.9988, 1.3582, 1.2740, 1.5560, 1.2262, 1.5035, 1.5205, 1.5904,\n                      1.3440, 0.9011, 0.5603, 1.1264, 1.3626, 1.6203, 1.5387, 1.2822, 1.7216,\n                      1.7812, 1.3759, 2.2268, 0.7498, 1.7125, 1.1626, 1.4288, 1.9080, 1.3918,\n                      2.0116, 0.7255, 1.0219, 1.7469, 1.8024, 1.7985, 1.2916, 0.9708, 1.5161,\n                      2.5346, 0.9380, 1.5649, 1.7716, 0.9757, 2.3236, 1.3826, 1.6204, 1.3082,\n                      1.6380, 1.8537, 1.4334, 1.4156, 1.1783, 1.7776, 1.6594, 1.6289, 1.1793,\n                      1.4134, 1.6124, 0.3876, 0.9540, 1.0627, 1.7692, 0.8550, 0.9656, 0.7827,\n                      1.5462, 0.5380, 1.4001, 1.6805, 1.6034, 1.6809, 1.1758, 1.3777, 1.4039,\n                      1.3076, 1.2610, 1.1925, 1.5569, 1.4638, 1.1840, 1.5966, 1.7786, 1.2210,\n                      1.0764, 1.2036, 0.9141, 2.1398, 2.1854, 0.9315, 0.9146, 2.0854, 1.7509,\n                      2.0552, 1.4050, 1.3879, 1.8438, 1.1544, 1.4136, 0.9084, 1.1174, 0.8027,\n                      1.4725, 1.2187, 1.2735, 1.4623, 2.0239, 1.4905, 2.0646, 1.7018, 1.1906,\n                      1.3508, 1.5388, 1.5005, 0.5632, 1.5826, 1.6383, 1.3699, 1.3263, 1.7872,\n                      1.4642, 1.8115, 1.3582, 1.6009, 1.7210, 1.6579, 1.2121, 1.4359, 1.2220,\n                      2.3900, 2.3890, 0.8343, 1.5341, 1.7465, 1.1951, 1.6800, 1.4895, 0.9313,\n                      1.3702, 1.5345, 1.4176, 1.1471, 1.1297, 0.8993, 2.1829, 1.9844, 1.2868,\n                      1.2142, 1.5191, 1.8964, 1.5597, 1.8936, 1.2445, 1.7112, 1.3015, 1.5087,\n                      1.1184, 0.7679, 1.2193, 1.8535, 1.2180, 1.5308, 1.2148, 1.1978, 1.5953,\n                      1.2533, 1.7387, 1.7182, 1.9197, 1.4030, 1.3692, 1.8988, 1.8581, 1.2166,\n                      1.2690, 1.7943, 2.0092, 1.8831, 1.7027, 1.9072, 1.5530, 1.5122, 2.0500,\n                      2.4254, 2.3241, 1.4137, 1.5750, 1.8187, 1.5782, 1.1722, 1.7443, 1.2957,\n                      1.4212, 1.2479, 1.5236, 1.1447, 1.3635, 1.6128, 1.3868, 0.8100, 1.2476,\n                      1.5846, 2.2186, 0.9803, 1.7496, 1.4623, 1.7175, 1.4748, 1.2617, 1.9115,\n                      1.8068, 2.1208, 1.8936, 0.8746, 1.1250, 1.3806, 1.2495, 1.3368, 1.1161,\n                      1.4885, 1.5938, 2.2966, 1.4205, 1.6488, 1.3243, 1.8540, 1.7716, 1.6735,\n                      1.3125, 1.3112, 1.4442, 0.9211], device='cuda:0')),\n             ('module.layer2.1.bias',\n              tensor([-2.6581, -4.6121, -1.2178, -1.4621, -7.2740, -3.4181, -3.2821, -2.0624,\n                      -2.0711, -4.8562, -2.3230, -2.5688, -2.8267, -2.7025, -1.7547, -3.0985,\n                      -1.0256, -2.0181, -3.0303, -2.1219, -0.6183, 11.9685, -2.1045, -3.5689,\n                      -3.1655, -3.3329, -2.2687, -3.1092, -1.7268, -2.2498, -2.8027, -2.9538,\n                      -2.8407, -2.7853, -2.0608, -1.9509, -2.8298, -2.3630, -0.6018, -0.6762,\n                      -1.9961, -1.9137, -2.2023, -2.1927, -2.5351, -2.9635, -2.4025, -3.4396,\n                      -1.1157, -2.6334, -2.8975, -3.4931, -2.9949, -2.6880, -3.4828, -0.7834,\n                      -1.2605, -2.4701, -4.1888, -3.6548, -2.8255, 15.7266, -2.6216, -4.5856,\n                      -2.4070, -2.3554, -2.8074, -1.4560, -4.5374, -2.2118, -3.8940, -1.7934,\n                      -2.6831, -2.9078, -2.4879, -4.1452,  1.0373, -2.5838, -2.7387, -2.6763,\n                      -1.7409, -2.0723,  1.0547, -0.1823, -1.7772, -1.8827, -2.8143, -0.2242,\n                      -2.6221, -1.1543, -2.1653, 12.1548, -4.6022, -2.8762, -1.9487, -2.9994,\n                      -1.0666, -3.7585, -2.1391, -3.6335, -1.8865, -2.3307, -2.7280, -2.5719,\n                      -2.1376, -2.3095, -2.9764, -2.5317, -1.5750, -2.1504, -1.8528, -2.9879,\n                      -3.2658, -1.2515, -0.4064, -3.1758, -5.3496, -3.4274, -2.0823, -2.7833,\n                      -2.6219, -1.9086, -4.8036, -1.2648, 13.5079,  1.0969, -2.4014, -2.0971,\n                      -1.8536, -4.3050, -2.5291, -2.1685, -3.2874, -4.7252, -2.0713, -3.6284,\n                      -2.4718, -2.1554,  0.7634, -1.6405, -2.2450, -2.2587, -3.9889, -2.7563,\n                      -2.0461, -4.4736, -1.9423, -4.5079, -3.0790, -3.2224, -1.5428, -2.1941,\n                      -1.5769, -3.8624, -4.2192, -0.1067, -3.0458, -3.3706, -4.4541, -3.7429,\n                      -4.0819, -1.2131, -1.5007, -2.6280, -2.9425, -3.0317, -1.5803, -1.5694,\n                      -3.0686, -3.1095, -2.2352, -2.4287, -2.5596, -2.5651, -2.6436, -2.9827,\n                      -1.9025, -2.6535, -1.4278, -2.7200, -2.5456, -0.7195, -1.8234, -2.7502,\n                      -3.5250, -2.4768, -3.7984, -4.4210, -4.6205, -3.2297, -2.8696, -3.1670,\n                      -3.4930, -3.2805, -1.7276, -2.8802, -2.9150, -3.8396, -3.1685, -2.6055,\n                      -3.0697, -2.4096, -3.4269, -3.7859, -1.7343, -2.7656, -3.0958, -4.0688,\n                      -3.5226, -1.1502, -4.6743, -3.0314, -3.0313, -1.9391, -2.6514, -2.0555,\n                      -3.2009, -0.4112, -1.9494, -3.2858, -2.3333, -4.3386, -3.9621, -1.2300,\n                      -2.5614, -2.5338, -4.0739, -2.2478, -2.3371, -2.3432, -3.3975, -2.3235,\n                      -3.0284, -2.9137, -2.7628, -4.1524, -3.8505, -1.9257,  0.3404, -2.2832,\n                      -2.5904, -2.6927, -2.0139, -2.2223, -0.9535, -3.9561, -2.1573, -2.8421,\n                      -1.8203, -3.0024, -3.3430, -2.9580, -0.9999, -2.1212, -2.2793, -1.1484],\n                     device='cuda:0')),\n             ('module.layer2.1.running_mean',\n              tensor([281.1917, 305.9900, 137.8253, 508.1630, 298.7276, 245.1656, 283.7648,\n                      437.1168, 450.2462, 324.8089, 465.2455, 409.0470, 270.2672, 344.3571,\n                      172.7604, 383.9328, 274.2777, 342.9146, 369.4375, 400.9157, 551.0762,\n                      -77.8006, 404.9977, 335.3937, 444.1201, 298.4734, 404.8324, 377.5756,\n                      178.9873, 310.9681, 238.3813, 357.8444, 334.7012, 354.5794, 408.5384,\n                      195.4579, 361.6565, 108.7945, 238.3520, 123.0481, 288.1679, 494.9373,\n                      344.3671, 186.2300, 429.0063, 274.8951, 345.4033, 143.1328, 255.8923,\n                      393.0230, 254.2548, 297.1626, 244.2180, 115.2813, 378.9873, 149.6518,\n                      433.3400, 424.3871, 320.5222, 250.0940, 327.0220,  23.6899, 304.8786,\n                      344.3250, 171.6241, 342.5205, 235.2862, 251.5216, 329.4798, 340.2448,\n                      315.7186, 286.8043, 251.1495, 329.0421, 360.7365, 305.4763,   9.1635,\n                      266.8515, 324.4850, 188.5513, 318.1665, 304.5194, 147.0239,  75.5795,\n                      159.1317, 181.8191, 280.7610, 130.7128,  77.6963, 249.5645, 499.2274,\n                      147.9874, 196.9056, 408.3078, 370.6289, 269.5923, 362.4853, 253.4701,\n                      176.0249, 220.0117, 273.0710, 237.3361, 306.2373, 187.3529, 259.9406,\n                      285.4574, 451.1211, 361.2000, 254.4484, 384.0990, 266.6926, 377.1576,\n                      168.5829, 295.4458, 170.2095, 351.3375, 346.5002, 294.2909, 328.1735,\n                      374.7777, 409.1404, 320.7268, 137.7346, 170.8147,  57.3411,   2.0118,\n                      301.5333, 397.8929,  71.7971, 267.0202, 587.3724, 458.4430, 402.5087,\n                      280.1724, 339.1761, 227.3980, 427.3273, 351.7661, -68.0483, 405.6834,\n                      403.2990, 278.8631, 242.6693, 450.8873, 417.6433, 169.3513, 277.0891,\n                      222.6061, 445.8282, 276.0786,  74.3312, 371.0196, 340.9141, 214.6695,\n                      343.0056, 241.2797, 333.4549, 355.7955, 279.5577, 246.4034, 272.8011,\n                      212.7579, 345.2701, 303.0055, 391.1941, 258.7728, 242.0434, 309.1750,\n                      398.4443, 478.9767, 153.5714, 208.6893, 242.3488, 332.7953, 328.7880,\n                      328.0501, 387.2271, 370.9093, 337.2453, 391.4080, 319.0778,  62.5701,\n                      272.5688, 300.3249, 360.2231, 286.7482, 338.2635, 138.0940, 340.8164,\n                      263.6525, 371.7262, 312.9997, 297.4763, 392.8179, 307.1281, 327.5270,\n                      364.2807, 121.7777, 339.8368, 302.6669, 145.9839, 274.3902, 333.2573,\n                      240.3464, 392.9608, 261.3205, 341.2297, 287.4956, 337.1718, 412.5771,\n                      361.8354, 285.6993, 304.3092, 114.4755, 442.5215, 162.0808, 407.5785,\n                      370.4124, 280.6884, 325.7266, 217.6815, 221.2525, 193.4598, 219.5479,\n                      342.2514, 373.5423, 251.3686, 350.6975, 435.7034, 279.2530, 279.3206,\n                      229.1668, 363.4866, 354.3629, 324.5268, 500.1169, 235.5952, 233.5852,\n                       41.1816, 201.4192, 415.7850, 194.0225, 327.0888, 241.2909, 396.7089,\n                      306.8259, 257.4503, 361.9528, 206.6915, 363.6519, 376.6086, 398.3264,\n                      465.5600, 275.4283, 312.4622, 392.2523], device='cuda:0')),\n             ('module.layer2.1.running_var',\n              tensor([ 28481.5684,   6009.0659,   9099.2822,  30108.4277,   3353.9900,\n                       10385.0234,  12251.0713,  17820.1855,   8904.0811,   5494.7915,\n                       14242.6973,  11634.4102,  11031.4395,  14615.4580,  17120.0684,\n                       19255.6875,  34317.8398,  13548.4160,  13883.6172,  12376.3740,\n                       55863.3516,  19944.0938,  13145.7158,  11806.9805,  15072.6230,\n                       12940.0029,  12389.7900,  20596.7852,  11373.5479,  13385.7109,\n                        4442.5659,  10109.3633,   8195.1621,  11417.1543,  19650.9199,\n                       21856.6543,  11418.8047,  10556.4902,  10074.8271,  36065.7266,\n                       22536.2070,  17841.8848,  18377.5000,  16123.1680,  13777.0449,\n                       19166.2910,  14900.1562,  24341.7207,  15820.2197,  21071.0566,\n                        8247.4766,  10329.2783,   8113.4551,  15735.4814,  16085.9443,\n                       11485.8340,  16349.5166,  18738.8887,   7199.1519,  12149.4805,\n                       10688.7969,  59162.3867,  18907.3867,  11915.4941,  14351.9453,\n                       17933.3594,  13738.1143,  12741.4043,   7080.0557,  15420.6846,\n                        9725.0986,  24475.3691,  17226.5078,  10253.3164,  12699.9922,\n                        6786.8984,  57856.4336,  17453.3672,  14447.8340,  15456.7998,\n                       13936.0107,  16982.7734, 101303.5156,  11491.3389,  14814.8145,\n                       11981.8848,  15752.2178,  24021.2461,   7286.1406,  16979.3691,\n                       16552.9395,  12212.4316,   5484.0161,  12964.5029,  21569.3398,\n                       12757.8438,  21204.6777,   6817.4458,  21255.6562,   7509.7524,\n                       20000.8633,  12976.0859,  11329.7412,  13134.9629,  12075.6240,\n                       15677.1221,  18866.9258,  13261.8027,  12972.9824,  11269.6982,\n                        9097.5576,  16186.7725,   8574.5439,  17058.4160,  24003.7715,\n                       12745.2012,   4956.6987,  15721.3965,  16518.7930,   9203.2373,\n                       12876.9756,  10862.5322,   5425.3525,  13779.5742,  65973.5078,\n                       36168.9883,  13919.2461,  14537.0771,  18429.3984,   4747.5181,\n                       14357.6348,  15777.1924,  16469.9590,   5625.0532,   9033.8994,\n                        9870.7910,  16255.9365,  19122.5762,  13033.3730,  17227.8477,\n                       20783.7422,  14865.0273,   6826.7192,  15901.3994,  19415.3535,\n                        9659.9434,  18131.3965,   7101.8423,  12420.8447,  10172.7256,\n                       32586.8066,  18232.4863,  18661.7754,  13586.3428,  10354.8613,\n                       28517.7441,  10379.6924,  13328.8984,   4088.2029,  12656.2393,\n                        7623.9297,  14763.6494,  18309.6914,  11138.0557,   7573.3975,\n                        9287.7549,  17523.7520,  13081.7383,  11259.0469,  13209.4346,\n                       12034.5000,  10366.6768,   9693.1895,   9525.6787,  14821.0996,\n                       13966.1426,  14130.6201,  15011.8486,  28296.5000,  11681.6611,\n                       10047.7100,  15087.1973,  12174.4102,  15925.7773,   8101.4966,\n                       12991.0928,   4537.5928,   4884.8481,   7500.7388,   6813.4272,\n                       16487.5938,  14622.2178,   9605.5039,   9810.7490,  16264.2100,\n                       17219.6035,  22305.8574,   6417.0894,   8669.9766,  11021.8721,\n                       15119.8311,  21310.1270,   9868.4678,  10244.1152,  14208.6240,\n                       18074.7891,  10557.6738,  11475.0781,   7288.5264,  28289.2383,\n                        5418.5928,  10519.2627,  10532.4385,  14142.8252,  16624.8691,\n                       16133.5225,  10012.4473,  50158.0625,  15835.0645,   4450.8374,\n                       12356.3408,   6354.0234,   7364.7466,  16590.9922,  11499.4307,\n                       14967.5752,   9822.3652,   8263.2402,  13093.8848,  16540.5195,\n                        7892.2847,  17183.1621,  11320.4795,  13298.7031,  12856.3350,\n                        9352.5840,  10586.2578,   8780.8115,  77121.2344,  14490.7314,\n                       14703.8789,  12951.6445,  15263.8340,  10071.7510,  42312.6680,\n                       11602.3438,  17624.5293,  13231.3271,  14777.9219,  15345.5283,\n                       12985.4111,  10902.0156,  33352.7773,  12955.3662,  13385.3174,\n                       25777.7793], device='cuda:0')),\n             ('module.layer2.1.num_batches_tracked',\n              tensor(942776, device='cuda:0')),\n             ('module.layer3.0.weight',\n              tensor([[[[-9.3630e-01, -7.8266e-01,  1.4686e+00],\n                        [-2.4647e-01, -1.4785e+00,  1.5816e+00],\n                        [ 1.8435e-01, -7.4160e-01, -2.0197e-01]],\n              \n                       [[-2.9779e-01,  1.3344e-01,  6.1727e-01],\n                        [ 6.5749e-01,  5.6455e-03,  1.0722e+00],\n                        [ 6.5377e-02, -1.1449e+00, -7.2627e-01]],\n              \n                       [[-2.0070e+00, -1.6393e+00, -1.8171e+00],\n                        [-2.5933e-01,  6.8119e-01, -9.5873e-01],\n                        [-5.3703e-01, -1.3007e+00, -2.8191e-01]],\n              \n                       ...,\n              \n                       [[ 1.0321e+00,  1.4975e+00, -1.1069e+00],\n                        [ 2.3712e-01, -7.6577e-01,  3.9701e-01],\n                        [ 5.3688e-01, -2.1627e-01,  7.9409e-01]],\n              \n                       [[ 1.3326e+00,  5.5688e-01, -1.9986e+00],\n                        [ 4.6771e-01,  6.1808e-01, -5.9565e-01],\n                        [-8.5899e-02,  7.6640e-02,  1.1762e+00]],\n              \n                       [[-1.6778e-01, -2.9751e-01, -2.4202e+00],\n                        [-3.8628e-01, -1.3276e-02, -8.7983e-01],\n                        [-1.1512e+00,  4.9754e-01,  7.6948e-02]]],\n              \n              \n                      [[[-3.2396e-01, -9.7086e-01,  5.5571e-02],\n                        [ 5.3116e-01, -1.5110e+00, -2.2640e-02],\n                        [-4.0643e-01,  1.7085e+00, -3.2848e-01]],\n              \n                       [[ 7.5772e-01, -4.4892e-01,  1.1104e+00],\n                        [-1.7335e-02, -3.0762e-01, -3.3178e-01],\n                        [-1.0216e+00, -6.7584e-01,  7.9576e-01]],\n              \n                       [[ 8.4087e-01,  1.4002e+00,  3.1532e-01],\n                        [ 7.7529e-01,  1.5207e-01,  6.9911e-01],\n                        [ 2.6514e-01,  1.5209e-02, -6.9056e-01]],\n              \n                       ...,\n              \n                       [[ 3.4322e-01, -8.6349e-01,  3.9375e-01],\n                        [ 3.5309e-01,  1.4148e+00, -7.8465e-02],\n                        [ 8.2815e-01, -6.3842e-01,  1.4142e+00]],\n              \n                       [[ 7.4121e-01, -1.1047e-02,  2.0869e+00],\n                        [ 7.6754e-01, -2.0716e-01,  9.5912e-01],\n                        [ 7.0796e-01, -4.0834e-01, -3.2704e-01]],\n              \n                       [[ 1.4111e-01,  6.7010e-01,  1.1266e+00],\n                        [ 1.1184e+00,  1.7630e+00,  1.5330e-01],\n                        [-1.7589e-01,  4.4113e-01, -7.4169e-01]]],\n              \n              \n                      [[[-1.4744e+00, -5.0415e-01, -6.8487e-02],\n                        [-3.5552e-01,  3.6116e-01,  8.4618e-02],\n                        [ 6.0023e-01, -3.0249e-01,  3.6019e-02]],\n              \n                       [[ 1.7511e+00,  9.4512e-02, -9.0209e-01],\n                        [-4.7920e-01, -7.2536e-01,  1.5880e-01],\n                        [ 7.0206e-01,  6.6636e-01, -8.7031e-01]],\n              \n                       [[ 1.1766e+00,  1.3518e-01, -1.0953e+00],\n                        [ 1.0171e+00,  1.2008e+00,  4.7195e-01],\n                        [ 7.8386e-02,  1.8546e-01,  6.4986e-01]],\n              \n                       ...,\n              \n                       [[ 5.1465e-01,  9.2502e-01,  6.7627e-01],\n                        [-5.8062e-01, -4.1070e-01,  6.0629e-01],\n                        [ 9.6048e-01,  2.4800e-01, -1.1391e+00]],\n              \n                       [[-8.6725e-02,  6.1375e-01,  1.3505e+00],\n                        [-4.5596e-01, -4.2737e-01, -1.3813e-01],\n                        [ 5.0531e-01, -4.2947e-01, -9.0257e-01]],\n              \n                       [[ 1.2941e+00,  8.8663e-03,  5.4255e-01],\n                        [-8.9301e-02, -2.4151e-01, -9.0747e-01],\n                        [ 1.2712e+00,  1.5576e-01, -2.9231e-01]]],\n              \n              \n                      ...,\n              \n              \n                      [[[-1.6204e-01, -9.2926e-01, -3.6696e-01],\n                        [-9.0101e-01, -6.6549e-01, -1.2646e+00],\n                        [-1.4947e+00, -9.9045e-01,  1.9072e-01]],\n              \n                       [[ 4.2424e-01,  2.3289e+00,  1.1193e+00],\n                        [ 9.6543e-01,  1.5250e-01,  4.3328e-01],\n                        [-2.7947e+00, -2.4687e-01, -6.6010e-01]],\n              \n                       [[-1.5796e+00, -6.9588e-01, -7.8569e-01],\n                        [ 2.5222e+00,  1.7187e+00,  5.4899e-01],\n                        [-5.9550e-01,  1.3605e+00, -8.2658e-01]],\n              \n                       ...,\n              \n                       [[-2.9478e-01, -4.7962e-01, -1.5074e-02],\n                        [-5.1170e-01, -2.1957e-01, -3.7304e-01],\n                        [ 3.1802e-01, -4.0611e-03, -2.7893e+00]],\n              \n                       [[-7.3544e-01,  6.6483e-01, -7.3764e-01],\n                        [ 9.0870e-01,  7.9642e-02,  2.7528e-01],\n                        [-4.9153e-01,  7.6478e-01,  1.5008e-01]],\n              \n                       [[ 8.9492e-01,  9.9683e-01,  1.3011e-01],\n                        [ 5.0558e-01,  2.3589e-01,  1.0482e+00],\n                        [-1.3429e+00, -9.7650e-02,  5.0108e-01]]],\n              \n              \n                      [[[-2.5951e-02, -5.7458e-01,  7.9142e-03],\n                        [-1.0300e-01, -3.4483e-01,  4.8761e-01],\n                        [-1.1504e+00,  1.3898e-01,  3.9914e-01]],\n              \n                       [[-5.9163e-01,  5.5283e-01, -2.4581e-01],\n                        [ 7.1462e-01,  3.4469e-01, -6.3122e-01],\n                        [ 1.8276e+00,  9.7828e-01,  1.3337e+00]],\n              \n                       [[-6.0075e-01,  9.0761e-02,  6.8145e-01],\n                        [-7.4352e-01, -8.8198e-01, -2.8603e-01],\n                        [-4.7152e-01, -3.0306e-02, -2.0130e+00]],\n              \n                       ...,\n              \n                       [[ 9.1222e-02,  1.8476e+00, -8.8641e-01],\n                        [ 7.9943e-01,  8.9234e-01,  8.5075e-01],\n                        [ 6.4280e-01, -1.9695e-02, -1.2567e-02]],\n              \n                       [[ 3.6666e-01, -3.5109e-01,  2.1149e-01],\n                        [-2.0834e-03,  4.5256e-01,  1.1470e+00],\n                        [ 1.6574e-01,  1.2237e+00,  1.1538e+00]],\n              \n                       [[-1.7008e+00, -3.3816e-01, -1.2725e-01],\n                        [ 1.5469e-01,  9.1662e-02, -8.8889e-01],\n                        [ 3.9517e-01,  4.2337e-01,  5.6965e-02]]],\n              \n              \n                      [[[ 1.9935e+00,  3.3359e-01,  2.3161e-01],\n                        [ 1.0820e+00, -4.0579e-01, -4.8246e-01],\n                        [ 8.5357e-01, -9.4017e-01, -4.2424e-01]],\n              \n                       [[ 7.7739e-01, -3.1425e-01,  3.4175e-01],\n                        [-1.3767e+00, -6.1358e-01,  7.0382e-02],\n                        [-2.9444e-01, -9.5054e-01,  4.6562e-01]],\n              \n                       [[ 2.8384e+00,  1.0734e+00,  1.0122e+00],\n                        [-1.0671e+00,  1.7344e-01, -1.0263e+00],\n                        [ 6.1345e-01,  1.1727e+00,  1.3428e-01]],\n              \n                       ...,\n              \n                       [[ 5.4965e-01,  6.7059e-01, -3.8353e-01],\n                        [ 5.8576e-01,  2.8915e-01,  8.4426e-01],\n                        [ 6.2232e-01,  1.6340e-01,  5.6636e-01]],\n              \n                       [[ 2.2016e+00,  1.3243e+00,  1.8815e+00],\n                        [ 3.7503e-01,  4.8984e-01, -1.9439e-02],\n                        [-2.4158e-01,  1.7666e+00,  2.7490e-01]],\n              \n                       [[ 1.7238e+00,  2.0236e-01,  4.8382e-02],\n                        [-1.8798e+00, -9.8453e-01,  4.0146e-01],\n                        [ 6.2499e-01,  3.0707e-01,  5.6207e-01]]]], device='cuda:0')),\n             ('module.layer3.0.bias',\n              tensor([-2.1727e-03,  7.9641e-04,  1.2880e-03, -3.9570e-04,  1.1915e-03,\n                      -2.8785e-03,  4.1325e-04,  8.4294e-04,  2.4212e-04,  1.5135e-03,\n                       7.6467e-04,  9.6688e-04,  8.3540e-04, -1.3721e-03, -5.3116e-04,\n                       2.6554e-03,  1.9233e-03,  1.4353e-04,  1.7235e-03, -1.1671e-03,\n                      -7.6879e-04, -2.0772e-03,  1.0829e-03,  6.9065e-04,  2.0902e-03,\n                       2.0133e-04, -7.9836e-04, -1.3575e-03, -3.8617e-04, -2.5429e-03,\n                      -6.4180e-04,  1.2811e-05,  1.4671e-04,  5.3111e-04,  7.8737e-05,\n                      -2.1484e-03,  1.6427e-03, -1.6595e-03, -1.0232e-03, -7.6395e-05,\n                       2.9562e-07, -5.6596e-05, -1.5678e-03,  1.5283e-03, -1.3859e-03,\n                      -1.2114e-03, -4.2296e-04, -7.5492e-04,  1.1546e-03,  2.3099e-03,\n                      -2.8201e-04,  8.3370e-04,  1.6947e-03,  1.4162e-03,  4.4391e-04,\n                      -1.9116e-03,  1.0149e-03,  2.2592e-03, -9.5085e-04,  2.8361e-03,\n                      -3.8427e-07, -5.5695e-04,  7.4530e-04, -1.3685e-03,  1.4264e-03,\n                       1.8032e-04, -4.8796e-04, -2.2970e-05, -5.6655e-04,  3.6531e-04,\n                      -6.8008e-04, -1.3139e-03,  2.0312e-03,  2.3689e-04,  7.5898e-04,\n                       6.9919e-04, -2.0782e-04, -1.0977e-03, -3.7790e-04,  6.1205e-04,\n                       9.5088e-04,  8.2777e-04,  2.2598e-03,  1.8824e-03, -8.6551e-04,\n                      -2.7700e-03, -3.9433e-04,  1.3212e-03, -9.7836e-04, -6.1814e-04,\n                      -3.9469e-05,  1.3926e-04,  9.5817e-05, -9.1518e-04,  1.0971e-03,\n                       5.4621e-05, -5.6639e-04,  2.9123e-05, -2.9208e-04,  5.8302e-04,\n                       1.4527e-03,  8.7167e-05,  7.4557e-05, -2.0023e-03,  1.6627e-03,\n                       6.7515e-04, -2.0045e-03, -2.4424e-03, -1.1275e-03,  1.5668e-03,\n                       7.3894e-04,  6.2421e-04,  6.4976e-04, -5.9269e-04,  4.5759e-04,\n                      -1.5159e-04, -1.5437e-04,  3.4284e-04,  1.0176e-03, -1.7888e-03,\n                       8.1311e-04, -5.2116e-04, -1.7716e-05, -5.7775e-04, -1.2651e-03,\n                      -4.2742e-04, -9.1208e-04,  7.6251e-04], device='cuda:0')),\n             ('module.layer3.1.weight',\n              tensor([3.1829, 3.2026, 3.1812, 2.8738, 3.2804, 3.3251, 3.3523, 2.7863, 3.1397,\n                      3.2402, 3.6369, 4.7625, 3.4398, 4.4807, 3.1893, 2.4714, 5.6433, 3.5547,\n                      3.1149, 3.6325, 3.8080, 2.6927, 3.1848, 3.1816, 3.3855, 4.2699, 4.0715,\n                      3.3165, 2.6851, 3.5845, 2.8584, 3.7278, 3.9823, 3.4822, 3.3735, 3.1595,\n                      3.2690, 3.2450, 3.1375, 2.8834, 3.6169, 3.0267, 3.7095, 3.3516, 3.5370,\n                      3.2333, 4.8562, 3.2694, 2.9056, 2.9871, 3.5048, 3.7980, 4.0747, 3.4348,\n                      3.5674, 3.4272, 3.6179, 3.9124, 2.7237, 3.3903, 3.0704, 3.3933, 3.2121,\n                      3.2141, 3.3057, 2.7009, 3.5062, 3.0207, 3.9155, 2.9827, 2.7094, 3.7840,\n                      3.8008, 2.8248, 3.0630, 3.1180, 3.0179, 3.5319, 3.6809, 3.3411, 3.2181,\n                      2.9652, 4.0989, 3.5290, 4.3890, 3.2812, 3.2690, 3.1144, 2.6629, 3.2688,\n                      2.3921, 3.5709, 3.3892, 3.7562, 3.3717, 4.2261, 3.1514, 3.3401, 2.9224,\n                      3.7437, 2.8871, 4.2863, 3.7230, 3.7845, 3.1302, 3.6114, 3.5637, 3.5193,\n                      3.9123, 3.8063, 3.0924, 3.3839, 3.1438, 3.8484, 3.6165, 3.0360, 3.4445,\n                      3.4130, 2.7935, 4.0912, 3.0682, 4.2526, 3.2879, 3.6890, 3.4892, 2.8568,\n                      3.6656, 2.5844], device='cuda:0')),\n             ('module.layer3.1.bias',\n              tensor([-1.0918, -1.2867, -0.8466, -1.2505, -0.5017, -1.6308,  0.1817,  0.5725,\n                      -0.6502, -0.7292, -1.8848, -2.1215, -1.4776, -0.0863, -0.7299, -0.8660,\n                      -3.5426, -0.2977, -1.6822, -1.6155, -2.3199, -0.2109, -0.0950, -1.5525,\n                      -1.3023, -0.8558, -2.0478,  0.0500, -0.5381, -2.1217,  0.8455, -1.1261,\n                      -1.7272, -1.8449, -0.9283, -0.9059, -1.6433, -1.2731, -1.2087, -1.6580,\n                      -0.4482, -1.0407, -1.0818, -1.2197, -1.0280, -0.7401, -1.9543, -1.0513,\n                      -1.3648, -1.0252,  0.1026, -2.3396, -1.9096, -1.3769, -1.3868, -0.8182,\n                      -1.5807, -1.3556, -0.7367, -0.9332, -1.0142, -1.4268,  0.1456, -0.7885,\n                      -0.9576, -0.7992, -0.9527, -1.6828, -1.3811, -1.2570,  0.1863, -1.9987,\n                      -1.4890, -0.4962, -0.9612, -0.8924, -0.7238, -1.4350, -1.2092, -1.3630,\n                      -0.8596, -1.1620, -1.6900, -1.5968, -2.4578, -1.3223, -0.3873, -1.5457,\n                      -1.0691, -1.4477, -0.4145, -1.5550, -1.6881, -2.1799, -0.9205, -2.2256,\n                      -1.0221, -1.0131, -1.1921, -1.3723, -0.9345,  0.1044, -1.8506, -1.9903,\n                      -0.8120, -1.6554, -1.9713, -1.1221, -1.3847, -2.3683, -0.5523, -0.8309,\n                      -0.6496, -1.7475, -0.9500, -1.3059, -0.6685, -1.7575, -0.7530, -1.6103,\n                      -0.4901, -2.4090, -1.1324, -0.3006, -1.6265, -0.5873, -1.8758,  0.1945],\n                     device='cuda:0')),\n             ('module.layer3.1.running_mean',\n              tensor([ -71.9953,   45.5785,   68.8912,  111.1307,  121.5777,  -78.1675,\n                        54.0575,  363.4310,   64.9505, -108.3899,  -58.8190, -167.3235,\n                       -87.6264, -210.8080,   95.6130,  163.7523,  -26.4489, -157.3604,\n                      -131.9952,   73.1594,   70.1247,   81.1934,  111.3936,  -97.2682,\n                        73.6213,   23.7118,   65.6414,  210.4380,  -19.7718,   93.3627,\n                       339.9909, -146.9333,  -13.7968, -129.7750,   48.4327,  -51.9743,\n                       167.6288,   82.4768, -106.0984,  -59.3415,   75.7411,  -54.1907,\n                        -1.7952, -109.1806, -164.5151,  132.8478,  100.3861,   64.5821,\n                       -37.9057,  169.4427,  -92.1280,  126.6012,  144.5087,   -5.3005,\n                       -60.5363,  -70.2981,   94.5615,  -64.8323,  -75.5910,  -42.2682,\n                        28.0347,  -42.6594,  -61.8523,    9.4002,  -64.6359,  -12.6457,\n                        88.7179,   67.8401,  -95.1252,   58.0942,  181.7466,  131.5326,\n                      -107.5762, -121.0183,   24.0452,   87.6121,   46.8577,   44.3252,\n                       -51.9704,   14.8045,  -98.2468,  -63.8123,   62.1281,   22.4517,\n                        11.2668,   50.8560, -151.8975, -132.7865,   79.4591,  -19.7370,\n                       246.7510,  271.4646,  -19.5653,   -3.6361,   17.0304,   34.0967,\n                       -12.5242, -153.8786,   71.5672,   34.9031,  -14.0677,  186.2873,\n                        36.0282,  144.3540,   65.3569, -213.0491,  166.7347,  101.2790,\n                        -8.0057,   -5.1273, -146.2434,   38.8628,   94.6243,  -30.1065,\n                       -23.4167,   61.2791,   17.7554,   19.3104,  101.0997, -164.3458,\n                      -105.5118,   26.6546, -170.9855, -195.3324,   15.5888,  -23.3298,\n                      -153.7904, -172.5564], device='cuda:0')),\n             ('module.layer3.1.running_var',\n              tensor([ 3951.2295,  4311.3062,  4040.2937,  3497.4500,  5011.2822,  3811.4365,\n                       7213.7959,  5264.6509,  3869.7578,  4735.4868,  4241.5947,  6570.8628,\n                       3774.8057,  8039.5571,  3764.7266,  3061.9412,  4884.4702,  7466.8892,\n                       3478.4158,  4388.8921,  3455.5857,  3488.1477,  5554.7124,  3101.2266,\n                       4832.5918,  8254.7051,  6585.0083,  5088.6743,  3218.2625,  3646.5042,\n                       3852.2515,  5411.1650,  5348.2354,  3172.8679,  4348.6973,  3271.5027,\n                       3319.4695,  4565.9746,  4388.8677,  2754.1519,  6180.6440,  3191.4065,\n                       6843.1309,  5947.0532,  4912.1157,  3455.3386,  8500.8408,  4683.2319,\n                       2826.6023,  3000.1135,  5692.6104,  3777.3157,  5628.1245,  4448.1260,\n                       4853.1655,  5385.6865,  3323.4910,  4256.4917,  3358.9436,  4783.7769,\n                       3236.2793,  4056.9624,  5104.4736,  4168.3306,  3722.4775,  3369.7017,\n                       4855.3965,  2774.9941,  4564.5327,  2949.0825,  3527.5281,  4518.2417,\n                       3189.2761,  4300.4316,  3425.0388,  4337.8828,  3696.7527,  4227.6436,\n                       4986.5034,  4876.7476,  3120.7002,  3461.4529,  4771.8574,  4191.4487,\n                       4054.3223,  3634.1694,  6372.2437,  3216.3198,  3001.2383,  3582.9148,\n                       2886.3884,  4332.8296,  3454.6982,  3415.6545,  4411.8638,  4282.6743,\n                       3409.0798,  3710.7639,  2621.2295,  3984.4290,  3699.7747, 10995.8682,\n                       4329.7397,  4346.6201,  3890.4495,  4719.3428,  4125.1299,  4510.6265,\n                       6288.3057,  4075.2603,  3767.8894,  4789.9761,  5794.3105,  5182.2334,\n                       4371.1675,  3692.8169,  5821.8179,  4376.6875,  3194.5901,  6678.8618,\n                       3886.2021,  4283.9775,  4363.6597,  5656.2134,  3902.1289,  3723.7559,\n                       4481.2290,  4560.0571], device='cuda:0')),\n             ('module.layer3.1.num_batches_tracked',\n              tensor(942776, device='cuda:0'))])"},"metadata":{}}]},{"cell_type":"code","source":"if torch.cuda.is_available():\n    encoder = torch.nn.DataParallel(encoder).cuda()\n    decoder = torch.nn.DataParallel(decoder).cuda()","metadata":{"execution":{"iopub.status.busy":"2024-09-23T22:05:49.365949Z","iopub.execute_input":"2024-09-23T22:05:49.366376Z","iopub.status.idle":"2024-09-23T22:05:49.385382Z","shell.execute_reply.started":"2024-09-23T22:05:49.366334Z","shell.execute_reply":"2024-09-23T22:05:49.384220Z"},"trusted":true},"execution_count":196,"outputs":[]},{"cell_type":"code","source":"encoder.load_state_dict(checkpoint['encoder_state_dict'])","metadata":{"execution":{"iopub.status.busy":"2024-09-23T22:05:51.282142Z","iopub.execute_input":"2024-09-23T22:05:51.282557Z","iopub.status.idle":"2024-09-23T22:05:51.296653Z","shell.execute_reply.started":"2024-09-23T22:05:51.282514Z","shell.execute_reply":"2024-09-23T22:05:51.295636Z"},"trusted":true},"execution_count":197,"outputs":[{"execution_count":197,"output_type":"execute_result","data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}]},{"cell_type":"code","source":"decoder.load_state_dict(checkpoint['decoder_state_dict'])","metadata":{"execution":{"iopub.status.busy":"2024-09-23T22:05:51.532514Z","iopub.execute_input":"2024-09-23T22:05:51.532967Z","iopub.status.idle":"2024-09-23T22:05:51.542766Z","shell.execute_reply.started":"2024-09-23T22:05:51.532928Z","shell.execute_reply":"2024-09-23T22:05:51.541634Z"},"trusted":true},"execution_count":198,"outputs":[{"execution_count":198,"output_type":"execute_result","data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}]},{"cell_type":"code","source":"Model = model(encoder,decoder)","metadata":{"execution":{"iopub.status.busy":"2024-09-23T21:19:46.411089Z","iopub.execute_input":"2024-09-23T21:19:46.412126Z","iopub.status.idle":"2024-09-23T21:19:46.416564Z","shell.execute_reply.started":"2024-09-23T21:19:46.412081Z","shell.execute_reply":"2024-09-23T21:19:46.415664Z"},"trusted":true},"execution_count":78,"outputs":[]},{"cell_type":"code","source":"img=cv.imread(\"/kaggle/input/bbbbbbbb/0093.png\")\nimg=cv.resize(img,(224,224))\nimg=torch.from_numpy(img).float().view(-1,224,224)","metadata":{"execution":{"iopub.status.busy":"2024-09-23T22:05:58.003480Z","iopub.execute_input":"2024-09-23T22:05:58.004202Z","iopub.status.idle":"2024-09-23T22:05:58.016202Z","shell.execute_reply.started":"2024-09-23T22:05:58.004162Z","shell.execute_reply":"2024-09-23T22:05:58.015343Z"},"trusted":true},"execution_count":199,"outputs":[]},{"cell_type":"code","source":"img.shape","metadata":{"execution":{"iopub.status.busy":"2024-09-23T22:05:58.200424Z","iopub.execute_input":"2024-09-23T22:05:58.200910Z","iopub.status.idle":"2024-09-23T22:05:58.208384Z","shell.execute_reply.started":"2024-09-23T22:05:58.200864Z","shell.execute_reply":"2024-09-23T22:05:58.207145Z"},"trusted":true},"execution_count":200,"outputs":[{"execution_count":200,"output_type":"execute_result","data":{"text/plain":"torch.Size([3, 224, 224])"},"metadata":{}}]},{"cell_type":"code","source":"img=img.unsqueeze(0).unsqueeze(0)","metadata":{"execution":{"iopub.status.busy":"2024-09-23T22:05:59.779685Z","iopub.execute_input":"2024-09-23T22:05:59.780762Z","iopub.status.idle":"2024-09-23T22:05:59.786239Z","shell.execute_reply.started":"2024-09-23T22:05:59.780714Z","shell.execute_reply":"2024-09-23T22:05:59.785054Z"},"trusted":true},"execution_count":201,"outputs":[]},{"cell_type":"code","source":"img.shape","metadata":{"execution":{"iopub.status.busy":"2024-09-23T22:06:01.091091Z","iopub.execute_input":"2024-09-23T22:06:01.092338Z","iopub.status.idle":"2024-09-23T22:06:01.100826Z","shell.execute_reply.started":"2024-09-23T22:06:01.092278Z","shell.execute_reply":"2024-09-23T22:06:01.099671Z"},"trusted":true},"execution_count":202,"outputs":[{"execution_count":202,"output_type":"execute_result","data":{"text/plain":"torch.Size([1, 1, 3, 224, 224])"},"metadata":{}}]},{"cell_type":"code","source":"out1=encoder(img)","metadata":{"execution":{"iopub.status.busy":"2024-09-23T22:06:04.938671Z","iopub.execute_input":"2024-09-23T22:06:04.939101Z","iopub.status.idle":"2024-09-23T22:06:04.956818Z","shell.execute_reply.started":"2024-09-23T22:06:04.939060Z","shell.execute_reply":"2024-09-23T22:06:04.955819Z"},"trusted":true},"execution_count":203,"outputs":[{"name":"stdout","text":"torch.Size([1, 512, 28, 28])\ntorch.Size([1, 512, 28, 28])\ntorch.Size([1, 256, 8, 8])\ntorch.Size([1, 128, 6, 6])\ntorch.Size([1, 1, 128, 6, 6])\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n","output_type":"stream"}]},{"cell_type":"code","source":"out1.shape","metadata":{"execution":{"iopub.status.busy":"2024-09-23T21:56:57.901155Z","iopub.execute_input":"2024-09-23T21:56:57.901570Z","iopub.status.idle":"2024-09-23T21:56:57.908682Z","shell.execute_reply.started":"2024-09-23T21:56:57.901529Z","shell.execute_reply":"2024-09-23T21:56:57.907316Z"},"trusted":true},"execution_count":179,"outputs":[{"execution_count":179,"output_type":"execute_result","data":{"text/plain":"torch.Size([1, 1, 128, 6, 6])"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"out=decoder(out1)","metadata":{"execution":{"iopub.status.busy":"2024-09-23T22:06:08.307411Z","iopub.execute_input":"2024-09-23T22:06:08.308111Z","iopub.status.idle":"2024-09-23T22:06:08.318974Z","shell.execute_reply.started":"2024-09-23T22:06:08.308067Z","shell.execute_reply":"2024-09-23T22:06:08.317845Z"},"trusted":true},"execution_count":204,"outputs":[]},{"cell_type":"code","source":"output=out[0]","metadata":{"execution":{"iopub.status.busy":"2024-09-23T22:06:08.824417Z","iopub.execute_input":"2024-09-23T22:06:08.825118Z","iopub.status.idle":"2024-09-23T22:06:08.829805Z","shell.execute_reply.started":"2024-09-23T22:06:08.825074Z","shell.execute_reply":"2024-09-23T22:06:08.828684Z"},"trusted":true},"execution_count":205,"outputs":[]},{"cell_type":"code","source":"output.shape","metadata":{"execution":{"iopub.status.busy":"2024-09-23T21:59:40.961479Z","iopub.execute_input":"2024-09-23T21:59:40.961912Z","iopub.status.idle":"2024-09-23T21:59:40.970661Z","shell.execute_reply.started":"2024-09-23T21:59:40.961871Z","shell.execute_reply":"2024-09-23T21:59:40.969076Z"},"trusted":true},"execution_count":184,"outputs":[{"execution_count":184,"output_type":"execute_result","data":{"text/plain":"torch.Size([1, 1, 9, 48, 48, 8])"},"metadata":{}}]},{"cell_type":"code","source":"!pip install trimesh","metadata":{"execution":{"iopub.status.busy":"2024-09-23T22:00:51.142884Z","iopub.execute_input":"2024-09-23T22:00:51.143776Z","iopub.status.idle":"2024-09-23T22:01:08.861945Z","shell.execute_reply.started":"2024-09-23T22:00:51.143728Z","shell.execute_reply":"2024-09-23T22:01:08.860721Z"},"trusted":true},"execution_count":186,"outputs":[{"name":"stdout","text":"Collecting trimesh\n  Downloading trimesh-4.4.9-py3-none-any.whl.metadata (18 kB)\nRequirement already satisfied: numpy>=1.20 in /opt/conda/lib/python3.10/site-packages (from trimesh) (1.26.4)\nDownloading trimesh-4.4.9-py3-none-any.whl (700 kB)\n\u001b[2K   \u001b[90m\u001b[0m \u001b[32m700.1/700.1 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n\u001b[?25hInstalling collected packages: trimesh\nSuccessfully installed trimesh-4.4.9\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport numpy as np\nfrom skimage import measure\nimport trimesh\n\ndef tensor_to_obj(tensor, threshold=0.5, filename='model.obj'):\n    # Ensure tensor is on CPU and convert to numpy array\n    voxels = tensor.cpu().detach().numpy()\n    \n    # Remove batch and channel dimensions if present\n    if voxels.shape[0] == 1:\n        voxels = voxels[0]\n    if voxels.shape[0] == 1:\n        voxels = voxels[0]\n    \n    # Combine the last dimension (8) into the others to make it 3D\n    voxels = voxels.reshape(9, 48, 48, 8).transpose(1, 2, 3, 0).reshape(48, 48, 72)\n    \n    # Use marching cubes to get the mesh\n    vertices, faces, _, _ = measure.marching_cubes(voxels, threshold)\n\n    # Create a trimesh object\n    mesh = trimesh.Trimesh(vertices=vertices, faces=faces)\n    \n    # Save as OBJ\n    mesh.export(filename)\n    \n    print(f\"Model saved as {filename}\")\n\n# Example usage\n# tensor = torch.randn(1, 1, 9, 48, 48, 8)  # Replace this with your actual tensor\ntensor_to_obj(output, filename='3d_mnist_model.obj')","metadata":{"execution":{"iopub.status.busy":"2024-09-23T22:06:12.346116Z","iopub.execute_input":"2024-09-23T22:06:12.346551Z","iopub.status.idle":"2024-09-23T22:06:14.619145Z","shell.execute_reply.started":"2024-09-23T22:06:12.346510Z","shell.execute_reply":"2024-09-23T22:06:14.617970Z"},"trusted":true},"execution_count":206,"outputs":[{"name":"stdout","text":"Model saved as 3d_mnist_model.obj\n","output_type":"stream"}]},{"cell_type":"code","source":"\n\n\n\n\n\n\n\n\n# print(Model)\n\n\n# img=img.convert_to_tensor()\n\n\n\n# x=torch.randn(1,1,3, 224, 224)\n# print(x.shape)\n# print(Model(x).shape)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import cv2 as cv","metadata":{"execution":{"iopub.status.busy":"2024-09-23T20:39:15.326819Z","iopub.execute_input":"2024-09-23T20:39:15.327231Z","iopub.status.idle":"2024-09-23T20:39:15.523209Z","shell.execute_reply.started":"2024-09-23T20:39:15.327189Z","shell.execute_reply":"2024-09-23T20:39:15.522143Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"img=cv.imread(\"/kaggle/input/bbbbbbbb/0093.png\")","metadata":{"execution":{"iopub.status.busy":"2024-09-23T20:45:25.949526Z","iopub.execute_input":"2024-09-23T20:45:25.949960Z","iopub.status.idle":"2024-09-23T20:45:25.961281Z","shell.execute_reply.started":"2024-09-23T20:45:25.949920Z","shell.execute_reply":"2024-09-23T20:45:25.960162Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"type(img)","metadata":{"execution":{"iopub.status.busy":"2024-09-23T20:45:26.260948Z","iopub.execute_input":"2024-09-23T20:45:26.261365Z","iopub.status.idle":"2024-09-23T20:45:26.268914Z","shell.execute_reply.started":"2024-09-23T20:45:26.261323Z","shell.execute_reply":"2024-09-23T20:45:26.267661Z"},"trusted":true},"execution_count":37,"outputs":[{"execution_count":37,"output_type":"execute_result","data":{"text/plain":"numpy.ndarray"},"metadata":{}}]},{"cell_type":"code","source":"img.shape","metadata":{"execution":{"iopub.status.busy":"2024-09-23T20:45:27.253625Z","iopub.execute_input":"2024-09-23T20:45:27.254587Z","iopub.status.idle":"2024-09-23T20:45:27.261312Z","shell.execute_reply.started":"2024-09-23T20:45:27.254538Z","shell.execute_reply":"2024-09-23T20:45:27.260235Z"},"trusted":true},"execution_count":38,"outputs":[{"execution_count":38,"output_type":"execute_result","data":{"text/plain":"(500, 379, 3)"},"metadata":{}}]},{"cell_type":"code","source":"img=cv.resize(img,(224,224))","metadata":{"execution":{"iopub.status.busy":"2024-09-23T20:45:28.741609Z","iopub.execute_input":"2024-09-23T20:45:28.742089Z","iopub.status.idle":"2024-09-23T20:45:28.747370Z","shell.execute_reply.started":"2024-09-23T20:45:28.742044Z","shell.execute_reply":"2024-09-23T20:45:28.746185Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"type(img)","metadata":{"execution":{"iopub.status.busy":"2024-09-23T20:45:36.551227Z","iopub.execute_input":"2024-09-23T20:45:36.551685Z","iopub.status.idle":"2024-09-23T20:45:36.558765Z","shell.execute_reply.started":"2024-09-23T20:45:36.551640Z","shell.execute_reply":"2024-09-23T20:45:36.557653Z"},"trusted":true},"execution_count":40,"outputs":[{"execution_count":40,"output_type":"execute_result","data":{"text/plain":"numpy.ndarray"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img=img.view(-1,224,224)","metadata":{"execution":{"iopub.status.busy":"2024-09-23T20:45:38.458308Z","iopub.execute_input":"2024-09-23T20:45:38.458889Z","iopub.status.idle":"2024-09-23T20:45:38.496529Z","shell.execute_reply.started":"2024-09-23T20:45:38.458844Z","shell.execute_reply":"2024-09-23T20:45:38.495048Z"},"trusted":true},"execution_count":41,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[41], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m img\u001b[38;5;241m=\u001b[39m\u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m224\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m224\u001b[39;49m\u001b[43m)\u001b[49m\n","\u001b[0;31mTypeError\u001b[0m: view() takes from 0 to 2 positional arguments but 3 were given"],"ename":"TypeError","evalue":"view() takes from 0 to 2 positional arguments but 3 were given","output_type":"error"}]},{"cell_type":"code","source":"img=img.float()","metadata":{"execution":{"iopub.status.busy":"2024-09-23T20:43:57.676914Z","iopub.execute_input":"2024-09-23T20:43:57.677325Z","iopub.status.idle":"2024-09-23T20:43:57.682565Z","shell.execute_reply.started":"2024-09-23T20:43:57.677285Z","shell.execute_reply":"2024-09-23T20:43:57.681240Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"type(img)","metadata":{"execution":{"iopub.status.busy":"2024-09-23T20:44:52.681637Z","iopub.execute_input":"2024-09-23T20:44:52.682050Z","iopub.status.idle":"2024-09-23T20:44:52.688737Z","shell.execute_reply.started":"2024-09-23T20:44:52.682010Z","shell.execute_reply":"2024-09-23T20:44:52.687622Z"},"trusted":true},"execution_count":35,"outputs":[{"execution_count":35,"output_type":"execute_result","data":{"text/plain":"torch.Tensor"},"metadata":{}}]},{"cell_type":"code","source":"\n# img=img.convert_to_tensor()\n# img=torch.from_numpy(img).float()\n\nimg=img.unsqueeze(0).unsqueeze(0)\n\n# x=torch.randn(1,1,3, 224, 224)\n# print(x.shape)\n# print(Model(x).shape)\nprint(encoder(img).shape)","metadata":{"execution":{"iopub.status.busy":"2024-09-23T20:47:56.009194Z","iopub.execute_input":"2024-09-23T20:47:56.009861Z","iopub.status.idle":"2024-09-23T20:47:56.047290Z","shell.execute_reply.started":"2024-09-23T20:47:56.009820Z","shell.execute_reply":"2024-09-23T20:47:56.045934Z"},"trusted":true},"execution_count":44,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","Cell \u001b[0;32mIn[44], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# img=img.convert_to_tensor()\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# img=torch.from_numpy(img).float()\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m img\u001b[38;5;241m=\u001b[39m\u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# x=torch.randn(1,1,3, 224, 224)\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# print(x.shape)\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# print(Model(x).shape)\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(encoder(img)\u001b[38;5;241m.\u001b[39mshape)\n","\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'unsqueeze'"],"ename":"AttributeError","evalue":"'numpy.ndarray' object has no attribute 'unsqueeze'","output_type":"error"}]},{"cell_type":"code","source":"x=torch.rand(16,1,3,224,224)\nencoder(x)","metadata":{"execution":{"iopub.status.busy":"2024-09-23T20:26:36.646411Z","iopub.execute_input":"2024-09-23T20:26:36.647583Z","iopub.status.idle":"2024-09-23T20:26:41.747629Z","shell.execute_reply.started":"2024-09-23T20:26:36.647520Z","shell.execute_reply":"2024-09-23T20:26:41.746533Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"tensor([[[[[ 2.8042e+00,  4.6067e-01,  9.3434e-01,  ..., -2.2924e-01,\n            -1.6860e-01,  7.5713e-01],\n           [ 7.0829e-02,  7.3486e-01, -5.8765e-01,  ...,  1.1128e+00,\n            -3.1826e-01,  8.3921e-01],\n           [-3.8712e-01,  1.0639e+00, -7.0990e-01,  ...,  2.3116e-01,\n            -1.3939e-01,  1.1504e+00],\n           ...,\n           [ 2.3133e-01, -7.6070e-01, -2.4583e-01,  ...,  7.6493e-01,\n            -5.5759e-01,  2.6145e+00],\n           [ 1.1542e+00, -5.0894e-01, -4.7033e-01,  ..., -6.3919e-01,\n             6.0548e-01,  1.2903e+00],\n           [ 2.3429e-01,  4.9251e-01,  1.6428e+00,  ..., -8.6822e-01,\n            -3.2450e-01, -5.0321e-01]],\n\n          [[-8.4073e-01,  4.3254e-02, -4.1820e-01,  ...,  1.2874e+00,\n             7.9539e-02,  2.5669e+00],\n           [ 1.0433e+00, -2.6519e-01,  9.6795e-01,  ...,  9.0870e-01,\n            -2.8022e-01, -3.8838e-01],\n           [ 7.9655e-01, -7.2384e-01, -2.8887e-03,  ..., -7.4136e-01,\n             3.7464e-01, -4.2114e-01],\n           ...,\n           [-6.7893e-01,  3.7212e-01, -5.9425e-01,  ..., -8.9540e-02,\n            -3.5559e-01,  3.0525e-01],\n           [-3.9272e-01, -2.1857e-01, -6.8440e-01,  ...,  1.6314e+00,\n             1.2695e-01, -8.6455e-02],\n           [-8.4337e-01, -4.8535e-01, -6.8790e-01,  ..., -7.8775e-01,\n             2.7513e-01, -2.6159e-01]],\n\n          [[ 1.2811e+00,  6.4567e-01,  9.1408e-01,  ...,  1.8433e-01,\n             7.2115e-01,  9.2786e-01],\n           [-5.9789e-01,  1.1980e+00,  3.2321e-01,  ..., -6.9442e-01,\n             2.3760e-01, -3.5617e-01],\n           [ 4.3307e-01,  9.9291e-01, -2.6876e-01,  ...,  1.9288e+00,\n            -5.6244e-02,  7.6447e-01],\n           ...,\n           [-3.1991e-02, -8.7102e-01,  7.6939e-01,  ..., -3.1957e-01,\n            -4.9015e-01,  3.9654e-01],\n           [ 2.2191e+00, -5.5676e-01,  1.2230e+00,  ..., -7.5279e-01,\n             1.2784e+00, -5.0542e-01],\n           [-4.9428e-01, -2.7500e-01, -1.9385e-01,  ..., -8.6657e-01,\n            -6.7595e-01, -4.3384e-01]],\n\n          ...,\n\n          [[-6.0443e-01,  5.6813e-01,  4.7040e-02,  ..., -6.9818e-01,\n             1.1318e+00,  5.1364e-02],\n           [-2.9947e-01, -1.8630e-01, -6.1791e-01,  ...,  3.5015e-01,\n            -3.2380e-01,  1.2874e+00],\n           [ 1.1192e-01,  5.4719e-01,  2.3074e+00,  ...,  4.0012e-01,\n             8.1803e-01,  4.8343e-01],\n           ...,\n           [ 1.1257e+00,  3.0362e-01,  4.4146e-01,  ..., -8.1325e-01,\n            -1.7306e-01, -7.8463e-01],\n           [-8.3756e-01,  1.3865e+00, -7.9167e-02,  ..., -6.9404e-01,\n            -6.2331e-01, -1.9476e-01],\n           [-3.0106e-01,  5.8766e-01, -4.6479e-01,  ..., -2.7081e-01,\n            -8.1897e-01,  2.5567e-01]],\n\n          [[-7.8643e-01,  1.6293e-01,  1.3221e-01,  ..., -7.6625e-01,\n             4.4298e-01,  1.4954e+00],\n           [-8.1415e-01, -7.5637e-01,  6.9986e-01,  ...,  7.9658e-01,\n             1.5995e+00,  1.4237e-01],\n           [ 1.3991e-01, -2.8156e-01,  1.2371e+00,  ...,  7.6202e-01,\n             4.4790e-01,  3.2603e-01],\n           ...,\n           [ 2.9026e-01,  1.9085e+00,  7.3299e-01,  ...,  6.4620e-01,\n            -5.4873e-01, -6.8958e-01],\n           [ 1.2243e-01, -1.1447e-01, -9.7156e-02,  ...,  1.2978e+00,\n             2.4503e-01, -3.1696e-03],\n           [ 4.6108e-03, -7.8632e-01, -2.0303e-02,  ..., -4.3981e-01,\n             5.9404e-01, -2.4232e-01]],\n\n          [[ 1.6711e+00, -8.6332e-01,  1.8807e+00,  ...,  1.3042e+00,\n            -7.2637e-01, -2.0277e-01],\n           [ 8.5514e-01, -3.7623e-01,  5.5571e-01,  ...,  1.0929e+00,\n            -2.2984e-01, -3.6849e-01],\n           [ 5.3997e-01, -7.8998e-01,  1.2227e+00,  ...,  1.2664e+00,\n             6.9471e-01,  1.4251e+00],\n           ...,\n           [ 1.7306e+00,  1.9543e+00,  4.0238e-01,  ..., -3.2294e-01,\n            -4.1227e-01,  1.5011e+00],\n           [ 1.2349e+00,  4.9491e-01,  3.4706e-01,  ..., -4.3357e-01,\n             1.3175e+00,  3.4968e-01],\n           [ 3.1885e-01, -2.8649e-01, -6.5169e-02,  ..., -3.7229e-01,\n             1.4693e+00,  3.7382e-01]]]],\n\n\n\n        [[[[ 3.0620e-01, -2.5495e-01, -6.2452e-01,  ...,  3.2304e-01,\n             4.2455e-01,  1.7299e+00],\n           [-5.2326e-01, -8.2474e-02,  7.4830e-01,  ...,  3.1035e-01,\n            -5.0319e-01,  1.9313e-01],\n           [ 5.6562e-01, -5.1571e-01,  8.0460e-02,  ..., -3.2902e-01,\n            -7.6125e-01,  8.3395e-01],\n           ...,\n           [ 1.1268e+00,  1.0019e+00, -2.3163e-01,  ..., -5.6527e-01,\n             2.1743e+00, -5.2497e-01],\n           [-5.1537e-01, -8.1686e-01, -3.1846e-01,  ...,  1.2146e+00,\n             6.1833e-01, -5.6978e-01],\n           [ 5.1166e-01,  1.4619e+00,  9.8150e-01,  ...,  1.3839e+00,\n             2.4641e+00,  1.1945e+00]],\n\n          [[ 2.1574e+00,  4.8318e-01,  1.1406e+00,  ...,  2.9919e-01,\n            -2.8828e-01,  1.8935e+00],\n           [-6.4023e-01,  8.6311e-01, -6.2184e-01,  ..., -6.1028e-01,\n             5.1752e-01,  2.8691e-01],\n           [-4.8414e-01,  7.6509e-01, -6.4800e-01,  ...,  1.5154e-01,\n             4.3180e-01,  1.7933e+00],\n           ...,\n           [-6.7901e-01,  6.9865e-01,  5.3518e-01,  ...,  4.7795e-01,\n             1.0912e+00, -3.0986e-01],\n           [ 6.1796e-01, -7.5938e-01, -8.5610e-02,  ..., -9.0909e-01,\n            -3.9018e-01, -6.2483e-02],\n           [-7.1867e-02, -2.9779e-01, -7.7097e-01,  ..., -5.0485e-01,\n             3.8309e-01,  3.9673e-03]],\n\n          [[ 4.4524e-02, -1.7914e-01, -1.1538e-01,  ...,  1.1131e+00,\n             1.5879e-01, -6.2780e-01],\n           [ 4.0406e-01,  6.4534e-01,  3.4051e-01,  ...,  6.8662e-01,\n             4.3566e-02, -7.8933e-01],\n           [ 3.9305e-01,  1.4170e+00,  1.2094e+00,  ..., -5.9961e-01,\n             8.4301e-01, -6.4802e-01],\n           ...,\n           [-1.3147e-01,  1.4325e+00,  8.4041e-01,  ..., -6.9489e-02,\n             1.6567e+00,  3.0037e-01],\n           [-7.5848e-01, -7.1048e-01,  8.2525e-01,  ...,  1.5111e+00,\n             3.1279e-01, -6.6444e-01],\n           [-8.4531e-01, -5.2911e-01,  7.7507e-01,  ...,  5.9860e-01,\n            -7.2822e-01, -7.0828e-01]],\n\n          ...,\n\n          [[ 3.0220e-01,  2.3668e+00,  1.9847e+00,  ..., -8.6289e-01,\n             5.2784e-01,  1.0155e+00],\n           [-6.0699e-01, -6.7092e-01,  4.8578e-01,  ..., -5.0682e-01,\n             1.8849e+00,  2.8719e-01],\n           [ 2.2685e+00,  6.5141e-01,  3.2024e-02,  ...,  4.2074e-01,\n             1.9865e-01,  1.0949e+00],\n           ...,\n           [ 5.8433e-01,  1.9815e-01,  1.3540e+00,  ...,  5.6650e-01,\n             6.7989e-01,  1.7871e+00],\n           [ 8.4439e-03,  1.1212e+00,  4.9882e-01,  ..., -7.7348e-01,\n            -7.9934e-01, -3.2155e-01],\n           [-3.7602e-01, -2.2535e-01, -3.3032e-01,  ..., -7.9963e-01,\n            -8.0454e-01,  3.5223e-01]],\n\n          [[-4.7288e-01,  3.3128e-01, -7.4266e-01,  ...,  2.6192e+00,\n            -1.5224e-01, -9.2552e-01],\n           [ 3.5089e-01, -5.8517e-01,  1.4049e+00,  ..., -8.4425e-01,\n            -8.4564e-01, -2.4958e-01],\n           [ 1.0083e-01, -4.0302e-01,  4.4849e-02,  ...,  6.6361e-01,\n            -4.3705e-01, -8.4968e-01],\n           ...,\n           [-5.1486e-01, -7.4541e-01,  4.3441e-01,  ...,  4.6359e-01,\n             9.1942e-01,  1.7914e+00],\n           [-2.6163e-02, -8.0822e-01,  4.0617e-01,  ..., -5.3876e-01,\n            -6.7576e-01,  4.4728e-01],\n           [-3.2527e-01,  1.1610e+00,  3.7761e-01,  ...,  1.1481e+00,\n             9.5974e-01, -3.0289e-01]],\n\n          [[ 1.3255e+00,  3.9825e-01,  2.7793e-01,  ..., -6.5320e-01,\n            -2.9325e-01,  4.4251e-01],\n           [ 1.6116e+00, -1.5480e-01,  4.9902e-01,  ...,  1.0436e+00,\n             1.3039e-02, -5.9360e-01],\n           [ 1.5086e-01, -8.3902e-01, -5.5433e-01,  ...,  1.5314e-01,\n             2.5859e-02,  4.4690e-01],\n           ...,\n           [-9.4251e-02, -1.6526e-01,  1.2784e+00,  ..., -6.0740e-01,\n            -6.7235e-01, -7.7899e-01],\n           [-5.2330e-01,  5.2096e-01,  1.3908e+00,  ...,  2.7280e-01,\n             5.7653e-01,  1.0336e-01],\n           [ 1.2469e+00,  1.1561e+00,  3.4291e-01,  ..., -3.8269e-01,\n            -7.6579e-01, -7.9534e-01]]]],\n\n\n\n        [[[[-1.3400e-01, -2.6698e-01, -8.1216e-01,  ..., -6.9465e-01,\n             1.0323e+00, -2.3126e-01],\n           [ 1.1844e+00,  4.7901e-02, -2.6982e-01,  ..., -2.6596e-01,\n             6.8013e-01,  1.2565e+00],\n           [-7.0146e-01, -3.8839e-01, -8.5186e-01,  ...,  3.7586e-01,\n            -7.5114e-01, -5.0200e-01],\n           ...,\n           [-3.5544e-01, -3.7187e-01,  8.3246e-01,  ..., -1.9779e-01,\n             8.0824e-01,  4.9575e-01],\n           [-5.3696e-01,  1.0626e+00, -3.4384e-01,  ...,  3.2084e-01,\n            -6.2836e-01, -4.3491e-01],\n           [-3.7857e-01,  3.1276e-01,  6.6013e-01,  ..., -5.7414e-01,\n            -4.7848e-01,  2.9107e+00]],\n\n          [[ 1.3008e+00, -1.3728e-01,  2.2446e-01,  ...,  7.7950e-01,\n            -9.4974e-01, -7.4109e-01],\n           [ 9.1788e-01,  3.7503e-01,  1.1141e-01,  ..., -6.2225e-02,\n            -7.1591e-01,  1.2515e+00],\n           [ 2.0282e-01, -4.7538e-01, -4.9740e-01,  ..., -1.0091e-01,\n            -5.4261e-01,  1.1993e+00],\n           ...,\n           [ 1.0072e+00,  1.2941e+00,  8.1694e-01,  ..., -6.6925e-01,\n             7.3997e-01, -3.6175e-01],\n           [ 2.3432e-01,  1.1967e+00, -6.5072e-01,  ..., -6.5171e-01,\n            -1.4506e-01,  6.6854e-01],\n           [-3.3599e-01, -6.7126e-01, -5.0239e-01,  ...,  4.8982e-02,\n            -8.2912e-01, -4.3488e-01]],\n\n          [[-6.2264e-01,  4.3143e-01,  3.6163e-01,  ..., -5.6893e-01,\n             7.2283e-02, -6.2639e-01],\n           [-1.5151e-02,  5.6741e-01,  1.6583e-01,  ..., -8.2029e-01,\n             4.4802e-01,  7.3042e-01],\n           [ 1.6671e+00,  1.6644e-01,  8.4359e-01,  ..., -2.5544e-01,\n            -8.8043e-01,  6.9543e-01],\n           ...,\n           [-2.2961e-02,  2.1640e+00,  4.0257e-01,  ...,  1.3881e-02,\n            -5.2146e-01, -2.3288e-01],\n           [-4.7340e-01,  1.5728e+00, -2.8450e-01,  ...,  4.0949e-01,\n             2.0149e+00, -6.4164e-01],\n           [ 9.4767e-01, -2.0916e-01,  5.7323e-01,  ...,  4.2186e-01,\n            -2.8381e-01, -8.7815e-01]],\n\n          ...,\n\n          [[ 6.2200e-01,  2.7533e-01, -4.5706e-01,  ..., -7.0569e-01,\n             1.0372e+00,  7.6153e-01],\n           [ 1.0434e+00,  7.0877e-01, -7.0729e-01,  ...,  7.3727e-01,\n             3.6381e-01, -6.6611e-01],\n           [-6.7254e-01,  7.0178e-01, -1.4005e-01,  ...,  2.3939e-01,\n            -5.5016e-01, -2.6204e-01],\n           ...,\n           [-6.4143e-01, -5.9928e-01, -3.1488e-01,  ..., -7.4659e-01,\n             1.0031e+00,  1.3598e+00],\n           [-4.6038e-01, -6.0975e-01,  1.3310e+00,  ...,  1.4192e+00,\n            -1.9219e-02, -2.9515e-01],\n           [ 4.2309e-01,  8.0155e-01,  2.5212e-01,  ..., -2.5044e-01,\n            -8.0388e-01,  1.6119e+00]],\n\n          [[-9.7822e-02, -4.9012e-01, -1.0698e-01,  ..., -8.0140e-01,\n            -3.8817e-01, -6.2805e-01],\n           [-2.2171e-01,  8.5949e-01, -3.3437e-01,  ...,  5.9421e-01,\n             1.7385e+00, -7.5119e-01],\n           [ 3.3336e-01, -5.8266e-01, -6.2234e-01,  ..., -7.3449e-01,\n            -1.1865e-01,  1.6792e+00],\n           ...,\n           [ 4.1166e-01,  3.8141e-01,  1.1600e+00,  ...,  7.9642e-01,\n             3.3321e-01, -6.7358e-01],\n           [ 2.1900e-01,  1.0408e-01, -6.5817e-01,  ...,  1.2810e+00,\n             7.2393e-02, -2.0912e-01],\n           [-7.7600e-01,  7.2666e-01, -1.8383e-01,  ..., -5.0582e-01,\n            -5.7352e-01, -6.9994e-01]],\n\n          [[ 2.4838e-02,  3.2364e-01, -4.2494e-01,  ..., -6.6138e-01,\n            -5.4496e-01,  7.6157e-01],\n           [ 4.6417e-01, -6.8486e-01, -5.5417e-01,  ...,  1.3000e-01,\n             1.4419e+00,  1.2703e+00],\n           [ 2.7903e-01, -6.2038e-01,  1.2888e+00,  ...,  1.3547e+00,\n            -6.1609e-01,  2.1940e+00],\n           ...,\n           [ 4.2923e-01, -7.2773e-01,  1.1377e+00,  ..., -8.5353e-01,\n            -3.4645e-02, -3.4338e-02],\n           [-2.2423e-01, -6.2163e-01,  1.7862e+00,  ..., -5.3530e-01,\n            -4.4973e-01,  1.5207e+00],\n           [-3.2718e-01, -1.1434e-01, -5.2309e-01,  ...,  8.2919e-01,\n             4.0709e-01,  1.1286e+00]]]],\n\n\n\n        ...,\n\n\n\n        [[[[-1.7081e-01, -5.4114e-01, -5.5699e-02,  ...,  5.7536e-01,\n             8.8385e-01, -3.5138e-01],\n           [-4.4962e-01, -1.5463e-01, -3.5943e-01,  ...,  1.1996e+00,\n            -7.1564e-01,  4.5884e-01],\n           [-7.6022e-01, -5.8820e-01,  1.5429e+00,  ..., -4.3709e-01,\n             1.3738e+00, -1.5842e-02],\n           ...,\n           [ 6.3542e-01, -6.6579e-01,  8.3866e-01,  ...,  8.5495e-01,\n            -5.2445e-01,  7.4111e-01],\n           [-6.9229e-01,  2.2633e-01,  6.8930e-01,  ...,  1.6705e+00,\n            -8.0479e-01,  3.9194e-01],\n           [ 1.7217e+00,  2.7874e-02, -8.6919e-01,  ...,  1.0554e+00,\n             5.7825e-01,  1.1342e+00]],\n\n          [[ 1.1804e+00, -4.1547e-01,  9.5600e-01,  ...,  1.1858e+00,\n            -7.5668e-01,  2.3284e+00],\n           [ 1.6349e+00, -7.5665e-01,  6.4188e-01,  ...,  1.9612e-01,\n             2.8045e-01,  1.2654e+00],\n           [ 8.6205e-01, -2.1935e-01, -1.4168e-01,  ...,  1.8972e-01,\n            -3.5446e-01, -2.3797e-01],\n           ...,\n           [-7.1840e-01, -6.4319e-01,  6.5728e-01,  ...,  2.8958e-01,\n             3.7712e-02,  9.8284e-01],\n           [ 7.8340e-01,  1.0601e+00, -8.3722e-01,  ..., -3.8454e-01,\n            -4.5496e-01,  2.4410e+00],\n           [ 1.4416e+00,  3.0248e-01,  7.8213e-01,  ...,  1.0405e+00,\n            -2.9448e-01,  1.2757e+00]],\n\n          [[ 1.2921e+00, -6.5483e-01, -7.1162e-01,  ..., -5.5207e-01,\n             9.4001e-01, -4.6527e-01],\n           [ 6.3749e-01,  4.2668e-01, -2.1844e-01,  ...,  5.6891e-01,\n            -2.8832e-01, -8.3688e-01],\n           [-2.9864e-01,  6.6196e-01,  7.0280e-01,  ...,  2.3833e-01,\n            -5.9361e-01, -3.0551e-01],\n           ...,\n           [ 1.0996e+00, -6.3447e-01,  1.2049e+00,  ...,  1.5114e-01,\n             5.7297e-01, -8.3803e-02],\n           [ 5.5054e-01,  2.3743e+00,  2.5157e+00,  ...,  2.6866e+00,\n            -8.7210e-01,  1.3654e+00],\n           [-6.3779e-01,  1.0667e-01,  1.0464e+00,  ...,  1.0856e+00,\n            -7.1045e-01,  3.6985e-01]],\n\n          ...,\n\n          [[ 1.2868e-01,  1.2496e+00, -3.4045e-01,  ...,  4.5826e-01,\n             2.7414e-01,  3.0399e-02],\n           [ 8.5349e-01, -2.7962e-01, -4.1191e-01,  ..., -6.8022e-01,\n            -5.0728e-01,  2.0320e+00],\n           [ 1.0542e+00, -7.9894e-01, -5.8561e-01,  ..., -3.4170e-01,\n             3.7401e-01, -9.7211e-02],\n           ...,\n           [-3.4361e-02,  3.6296e-01, -1.0269e-01,  ...,  1.8840e+00,\n             2.0936e+00, -6.1652e-01],\n           [ 7.2672e-01, -5.7124e-01, -2.1362e-01,  ..., -3.5870e-01,\n             4.9849e-01, -8.4642e-01],\n           [-4.9173e-01,  6.0278e-01, -2.6812e-01,  ...,  1.3025e+00,\n             1.3335e+00, -8.1854e-01]],\n\n          [[-9.1140e-01,  7.0049e-01, -5.9926e-01,  ..., -6.8575e-01,\n            -4.4806e-01,  1.1286e-01],\n           [-8.6263e-01,  6.5749e-01, -1.8313e-01,  ...,  5.7200e-01,\n            -3.4596e-01, -7.1107e-01],\n           [ 4.4090e-01,  2.7763e-01, -5.7102e-01,  ...,  1.4073e-01,\n             3.6883e-01, -2.8789e-01],\n           ...,\n           [-3.5377e-01,  7.0625e-01, -4.6061e-01,  ..., -8.0911e-01,\n            -5.5231e-02, -1.3919e-01],\n           [ 2.6211e-01,  2.9611e-01, -1.0891e-01,  ...,  1.2114e+00,\n             4.5521e-01, -5.3671e-01],\n           [ 1.0105e+00,  3.0180e+00, -5.5347e-01,  ...,  2.0143e+00,\n            -2.0293e-01,  1.2293e+00]],\n\n          [[ 1.4059e-01,  4.9673e-01, -7.7865e-01,  ...,  1.1265e-02,\n            -2.6290e-01,  4.6541e-02],\n           [ 1.0963e+00,  4.7373e-01, -2.6624e-01,  ..., -4.8272e-01,\n            -3.0695e-02, -6.6518e-01],\n           [-5.8358e-01,  1.1886e+00,  1.1516e+00,  ...,  1.1494e-01,\n             1.1054e+00, -5.8549e-01],\n           ...,\n           [ 1.2305e+00,  2.2327e-01,  7.5316e-01,  ...,  5.0858e-02,\n            -7.1536e-01,  1.8966e+00],\n           [-5.6691e-01,  1.3657e+00, -7.1650e-01,  ..., -3.2689e-01,\n            -6.2316e-02, -4.7796e-01],\n           [-4.5912e-01,  7.2790e-01,  1.3181e+00,  ...,  4.7009e-01,\n            -7.9705e-01, -5.6841e-01]]]],\n\n\n\n        [[[[ 1.5793e+00, -7.3329e-01, -1.3813e-01,  ..., -7.0000e-01,\n            -7.6307e-01,  1.8319e-01],\n           [-1.9876e-01,  6.1038e-01,  1.1279e-01,  ..., -9.4890e-01,\n            -7.7499e-01,  3.5756e-01],\n           [-1.6331e-01, -2.1678e-01,  5.7578e-01,  ...,  3.7134e-01,\n            -1.4084e-01,  2.2101e+00],\n           ...,\n           [ 1.1798e+00, -7.0265e-01, -4.1259e-01,  ..., -4.4373e-02,\n            -4.0357e-01,  2.8520e-01],\n           [ 8.1310e-02,  1.8994e+00,  1.3484e-02,  ...,  3.4875e-01,\n             6.2423e-01,  1.9297e+00],\n           [-5.4397e-01, -8.9404e-01, -1.2655e-01,  ...,  5.1242e-01,\n            -2.3034e-01,  8.8064e-02]],\n\n          [[-1.1185e-01,  4.7787e-02, -9.7265e-02,  ...,  6.6181e-01,\n             1.8394e+00,  1.1580e+00],\n           [ 1.2117e+00, -4.7361e-01, -6.2550e-01,  ...,  1.1741e+00,\n             8.6403e-01,  7.2381e-01],\n           [ 2.2987e-01,  6.5192e-02,  3.5437e-01,  ..., -6.2905e-01,\n            -1.3763e-01, -3.8411e-01],\n           ...,\n           [-8.3669e-01,  2.0698e-01,  5.9187e-01,  ...,  2.4010e-03,\n             1.2505e-01,  1.8912e-01],\n           [ 1.3516e+00,  4.4567e-02,  5.9580e-01,  ...,  7.2956e-01,\n            -7.5627e-01,  1.1590e+00],\n           [-3.0913e-01,  3.5227e-01, -6.2928e-01,  ..., -4.6701e-01,\n            -7.0960e-01,  3.8611e-02]],\n\n          [[-8.3241e-01, -1.3304e-01, -6.1066e-01,  ...,  1.0368e+00,\n             2.1405e+00,  6.9320e-01],\n           [ 5.0733e-01, -3.2396e-01, -4.8888e-01,  ..., -4.2771e-01,\n             9.1492e-01, -6.5029e-01],\n           [-2.7156e-03, -7.2191e-01,  8.5930e-01,  ...,  1.1851e+00,\n             3.7607e-02, -1.8763e-01],\n           ...,\n           [-4.7225e-01,  4.9847e-01,  4.7795e-01,  ...,  7.1020e-01,\n             1.4642e+00, -5.0134e-01],\n           [ 4.3844e-01,  6.4312e-01, -6.4294e-02,  ...,  1.9732e-01,\n            -3.4072e-01, -5.8468e-01],\n           [ 1.5719e-01, -6.8643e-02,  9.2365e-01,  ..., -7.3681e-01,\n            -2.2049e-01, -4.7127e-01]],\n\n          ...,\n\n          [[-3.3546e-01, -9.3004e-01,  1.0513e+00,  ...,  1.0816e+00,\n            -1.2487e-02,  1.2895e+00],\n           [-7.5871e-01, -7.2009e-01, -3.7304e-01,  ...,  4.8142e-01,\n            -8.7054e-02,  4.9401e-02],\n           [-7.7520e-01, -4.5102e-01, -2.5700e-01,  ..., -7.6161e-01,\n             6.3875e-01, -1.5235e-01],\n           ...,\n           [ 5.3924e-01, -7.5349e-01, -7.2570e-01,  ...,  3.0719e-02,\n            -3.4596e-01,  7.0280e-01],\n           [-1.8133e-01,  2.6062e-02, -7.6967e-01,  ...,  7.6331e-01,\n             1.0393e+00, -7.8284e-01],\n           [-7.1075e-01, -6.1422e-01, -7.3423e-01,  ...,  3.1280e-01,\n            -4.7309e-01,  1.3933e+00]],\n\n          [[-5.5772e-01, -1.8233e-01, -2.2577e-01,  ...,  1.6240e+00,\n             1.1379e-01, -8.7187e-01],\n           [-7.3066e-01, -2.0591e-01,  8.6547e-01,  ...,  4.6433e-01,\n             1.7770e-01, -9.5248e-01],\n           [ 1.5966e+00,  6.5435e-01, -4.1800e-01,  ...,  8.9245e-01,\n            -5.0149e-01,  6.9280e-02],\n           ...,\n           [ 5.0496e-01, -6.3977e-01,  1.6487e+00,  ...,  1.1286e-01,\n             3.9435e-01, -4.5738e-01],\n           [-7.8366e-01, -1.8104e-01,  2.5517e-01,  ...,  1.4617e-01,\n             4.3761e-01, -7.0586e-02],\n           [ 1.3333e+00, -4.3561e-01,  2.7287e-01,  ...,  4.9054e-01,\n            -7.8653e-01,  1.1938e+00]],\n\n          [[ 7.4568e-01,  1.4940e-01,  8.7718e-01,  ...,  4.0205e-01,\n             7.8105e-01, -5.1752e-01],\n           [ 2.3411e-01, -5.2829e-02, -6.1330e-01,  ..., -6.5559e-01,\n            -4.7450e-01,  2.7348e-01],\n           [-2.8432e-02,  3.7858e-01,  1.0395e+00,  ..., -1.8942e-01,\n            -8.4422e-01, -4.4331e-01],\n           ...,\n           [-5.3184e-01, -5.5506e-01, -2.3025e-01,  ...,  5.4974e-01,\n             2.0615e-01,  1.4672e+00],\n           [ 2.4892e+00,  1.2966e+00, -5.8769e-01,  ...,  3.1703e-01,\n             8.3788e-01, -4.0753e-01],\n           [ 1.1749e-01, -7.9924e-01, -7.8779e-01,  ..., -9.6865e-01,\n            -9.9568e-02, -5.9008e-01]]]],\n\n\n\n        [[[[-6.1701e-01,  5.2799e-01, -2.0923e-01,  ...,  6.2081e-01,\n             6.3141e-01,  6.2332e-01],\n           [-1.1709e-01, -4.0503e-01, -2.4439e-02,  ...,  2.3376e-01,\n             1.7025e+00,  2.7744e+00],\n           [-4.0415e-01, -5.0448e-01,  2.0445e-01,  ..., -3.9775e-01,\n             1.0273e+00,  1.6498e+00],\n           ...,\n           [-6.0547e-02,  9.0274e-01,  1.9514e-01,  ...,  5.2388e-01,\n            -4.2762e-01,  5.2951e-01],\n           [ 2.2131e-01, -5.0886e-01, -3.5996e-01,  ...,  1.6568e+00,\n            -3.2772e-01,  1.2188e+00],\n           [-8.7775e-01, -7.0840e-01, -6.3469e-01,  ...,  1.9074e-01,\n             3.1423e-01,  1.0080e+00]],\n\n          [[-9.3460e-02, -8.0713e-01,  7.4299e-01,  ..., -8.3582e-01,\n            -6.4216e-01,  1.5195e+00],\n           [ 2.7593e-01,  1.0861e+00, -7.1333e-02,  ...,  3.3026e-01,\n            -8.4400e-01,  1.9745e-01],\n           [-2.4365e-01, -8.1582e-01, -8.2143e-01,  ...,  9.0528e-01,\n            -4.9428e-01,  9.4923e-01],\n           ...,\n           [ 8.4053e-01,  2.9632e-01,  9.9191e-01,  ...,  3.3274e-02,\n            -3.6377e-01,  6.8910e-01],\n           [-3.8667e-01,  1.0096e+00, -1.8882e-01,  ...,  2.4490e+00,\n            -5.7974e-01, -6.1686e-01],\n           [-5.0733e-01,  3.4192e-01, -6.8562e-01,  ..., -5.0044e-01,\n             6.9563e-01, -3.3343e-01]],\n\n          [[ 1.4447e+00, -5.9489e-01,  9.0743e-01,  ..., -8.2923e-01,\n             4.3790e-01, -8.1292e-01],\n           [ 4.6311e-01, -5.9260e-01, -6.5657e-01,  ...,  1.4688e+00,\n             2.1226e+00, -2.9967e-01],\n           [-6.1151e-01,  4.8307e-01, -5.8226e-01,  ..., -6.9422e-01,\n             6.8559e-01, -7.7110e-01],\n           ...,\n           [-7.5104e-01, -3.6250e-01,  5.2097e-01,  ...,  9.9361e-01,\n             6.3704e-01, -4.3693e-01],\n           [ 2.0453e+00,  1.8290e+00,  2.5513e-01,  ...,  5.7891e-01,\n            -1.2772e-01, -3.4353e-01],\n           [ 5.6526e-01, -5.3930e-01,  8.1470e-02,  ..., -8.9745e-01,\n            -6.4901e-01,  3.4925e-01]],\n\n          ...,\n\n          [[-5.5120e-01,  8.5239e-01,  1.6792e-02,  ..., -1.6991e-01,\n             5.4105e-01,  1.2628e+00],\n           [ 6.1038e-01,  9.1355e-02,  5.0462e-01,  ...,  8.3665e-01,\n            -7.5172e-01, -5.9006e-01],\n           [ 2.7005e-01, -1.6634e-01,  7.5779e-01,  ..., -3.2693e-01,\n            -1.6468e-01,  7.4063e-01],\n           ...,\n           [ 1.2969e+00, -3.9353e-01, -8.4081e-01,  ..., -2.4087e-01,\n            -6.4601e-01, -7.1554e-01],\n           [ 7.5800e-01, -3.1678e-01, -6.9077e-01,  ...,  1.1702e-01,\n             2.4013e+00, -2.6121e-01],\n           [ 2.2700e-01,  1.6384e+00, -6.7498e-01,  ...,  2.8219e-01,\n            -1.1246e-01,  4.0264e-01]],\n\n          [[-3.2559e-01, -7.2964e-01,  1.0938e+00,  ...,  8.7942e-01,\n             5.4661e-01, -7.0752e-01],\n           [ 1.0678e+00,  4.0121e-01,  1.5395e+00,  ...,  1.5862e-01,\n             2.0723e-01,  8.7337e-01],\n           [-1.2638e-01,  8.6274e-01, -4.1458e-01,  ...,  1.0311e+00,\n            -8.4887e-01, -3.8092e-01],\n           ...,\n           [-5.7672e-01,  2.8918e-01,  2.8818e+00,  ..., -7.5278e-02,\n            -2.6296e-01, -2.3520e-01],\n           [-4.1112e-01, -3.4420e-02,  2.6896e+00,  ...,  5.6652e-02,\n            -9.8094e-02,  1.6837e+00],\n           [-7.6235e-01,  7.9196e-01,  3.2137e-01,  ...,  1.7741e+00,\n            -7.3905e-01, -6.3292e-01]],\n\n          [[-5.0786e-01,  3.9715e-01,  1.2872e+00,  ..., -5.0748e-01,\n             1.5272e-01, -2.2838e-01],\n           [ 2.2695e+00, -3.1694e-01, -2.3546e-01,  ...,  1.4395e+00,\n             4.2132e-01,  7.4151e-01],\n           [-7.8914e-01,  6.1712e-02, -4.8448e-01,  ..., -1.0726e-01,\n             8.7095e-01, -5.5593e-03],\n           ...,\n           [ 1.0129e+00, -7.3273e-01, -3.2727e-01,  ...,  3.9735e-01,\n             1.5912e-01,  7.4717e-01],\n           [ 1.1977e+00, -5.4704e-01,  1.0113e+00,  ..., -5.4184e-01,\n            -1.3367e-01,  9.4100e-01],\n           [-2.7794e-01, -4.8678e-01,  2.0403e-01,  ...,  9.8159e-02,\n            -4.9092e-01, -8.2523e-02]]]]], grad_fn=<PermuteBackward0>)"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}
